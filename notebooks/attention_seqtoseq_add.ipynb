{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "from keras.models import Model, Sequential\n",
    "from keras.engine.training import slice_X\n",
    "from keras.layers import Lambda, Flatten, Permute, Reshape, Input\n",
    "from keras.layers import merge, Merge, Activation, TimeDistributed, Dense, RepeatVector, recurrent\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    '''\n",
    "    Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    '''\n",
    "    def __init__(self, chars, maxlen):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, self.char_indices[c]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset\n",
    "TRAINING_SIZE = 5000\n",
    "DIGITS = 2\n",
    "OPS = 2\n",
    "INVERT = True\n",
    "# Try replacing GRU, or SimpleRNN\n",
    "RNN = recurrent.LSTM\n",
    "HIDDEN_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "MAXLEN = OPS * DIGITS + OPS - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(training_size, num_digits, num_ops):\n",
    "    questions = []\n",
    "    expected = []\n",
    "    seen = set()\n",
    "    print('Generating data... ')\n",
    "    while len(questions) < training_size:\n",
    "#         f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, num_digits + 1))))\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(num_digits)))\n",
    "        ops = []\n",
    "        for i in range(num_ops):\n",
    "            ops.append(f())\n",
    "                    \n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that X+Y == Y+X (hence the sorting)\n",
    "        ops.sort()\n",
    "        key = tuple(ops)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN\n",
    "        q = ''.join([str(op) for op in ops])\n",
    "        query = q + ' ' * (MAXLEN - len(q))\n",
    "        ans = str(sum(ops))\n",
    "        # Answers can be of maximum size DIGITS + 1\n",
    "        if INVERT:\n",
    "            query = query[::-1]\n",
    "            ans = ans[::-1]\n",
    "        ans += ' ' * (num_digits + 1 - len(ans))\n",
    "        questions.append(query)\n",
    "        expected.append(ans)\n",
    "#         print(len(questions))\n",
    "    print('Total addition questions:', len(questions))\n",
    "    \n",
    "    return questions, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_valid(questions, expected, num_digits, num_ops):\n",
    "    print('Vectorization...')\n",
    "    X = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(questions), num_digits + 1, len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(questions):\n",
    "        X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "    for i, sentence in enumerate(expected):\n",
    "        y[i] = ctable.encode(sentence, maxlen=num_digits + 1)\n",
    "\n",
    "    # Shuffle (X, y) in unison as the later parts of X will almost all be larger digits\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Explicitly set apart 10% for validation data that we never train over\n",
    "    split_at = len(X) - len(X) / 10\n",
    "    (X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n",
    "    (y_train, y_val) = (y[:split_at], y[split_at:])\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(hidden_size, num_layers, num_digits, num_ops):\n",
    "    # Most simple seq2seq model using encoder-decoder framework\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "    # note: in a situation where your input sequences have a variable length,\n",
    "    # use input_shape=(None, nb_feature).\n",
    "    encoder = RNN(hidden_size, input_shape=(MAXLEN, len(chars)))\n",
    "    model.add(encoder)\n",
    "    # For the decoder's input, we repeat the encoded input for each time step\n",
    "    model.add(RepeatVector(num_digits + 1))\n",
    "    # The decoder RNN could be multiple layers stacked or a single layer\n",
    "    for _ in range(num_layers):\n",
    "        decoder = RNN(hidden_size, return_sequences=True)\n",
    "        model.add(decoder)\n",
    "\n",
    "    # For each of step of the output sequence, decide which character should be chosen\n",
    "    mapper = TimeDistributed(Dense(len(chars)))\n",
    "    model.add(mapper)\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    encoder_f = K.function(inputs, [encoder.output])\n",
    "    decoder_f = K.function(inputs, [decoder.output])\n",
    "    mapper_f = K.function(inputs, [mapper.output])\n",
    "    \n",
    "    return model, encoder_f, decoder_f, mapper_f, encoder, decoder, mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "def get_last_Y(X):\n",
    "    return X[:, -1, :]\n",
    "\n",
    "def get_Y(X, xmaxlen):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.T.batched_dot(Y, alpha)\n",
    "    return ans\n",
    "\n",
    "def get_R_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    outshape = (shape[0][0],shape[0][1])\n",
    "    return tuple(outshape)\n",
    "\n",
    "def stack_R(X):\n",
    "    r_1, r_2, r_3 = X[0], X[1], X[2]\n",
    "    ans = K.concatenate([r_1, r_2, r_3], axis=1)\n",
    "    return ans\n",
    "\n",
    "def get_stack_R_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    outshape = (shape[0][0],3,shape[0][2])\n",
    "    return tuple(outshape)\n",
    "\n",
    "def stack_decoder_input(X):\n",
    "    Y, r = X[0], X[1]\n",
    "    ans = K.concatenate([Y, r], axis=2)\n",
    "    return ans\n",
    "\n",
    "def stack_decoder_input_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    outshape = (shape[0][0], shape[0][1], shape[0][2]+shape[1][2])\n",
    "    return tuple(outshape)\n",
    "\n",
    "def build_att_seq2seq_model(hidden_size, num_layers, num_digits, num_ops, chars):    \n",
    "    main_input = Input(shape=(MAXLEN,len(chars)), name='main_input')\n",
    "    encoder = RNN((num_digits+1)*hidden_size, \n",
    "                  input_shape=(MAXLEN, len(chars)),\n",
    "                  return_sequences=True)(main_input)\n",
    "#     Y = Lambda(get_Y, arguments={\"xmaxlen\": MAXLEN}, name=\"Y\", output_shape=(MAXLEN, hidden_size))(encoder)\n",
    "    \n",
    "    last_Y = Lambda(get_last_Y, name=\"last_Y\", output_shape=((num_digits+1)*hidden_size,))(encoder)\n",
    "    Y = Reshape((num_digits+1, hidden_size))(last_Y)\n",
    "    \n",
    "    WY1 = TimeDistributed(Dense(len(chars)), name=\"WY1\")(main_input)\n",
    "    WY2 = TimeDistributed(Dense(len(chars)), name=\"WY2\")(main_input)\n",
    "    WY3 = TimeDistributed(Dense(len(chars)), name=\"WY3\")(main_input)\n",
    "\n",
    "#     WY2 = TimeDistributed(Dense(hidden_size), name=\"WY2\")(Y)\n",
    "#     WY3 = TimeDistributed(Dense(hidden_size), name=\"WY3\")(Y)\n",
    "    M1 = Activation('tanh', name=\"M1\")(WY1)\n",
    "    M2 = Activation('tanh', name=\"M2\")(WY2)\n",
    "    M3 = Activation('tanh', name=\"M3\")(WY3)\n",
    "#     M2 = Activation('tanh', name=\"M2\")(WY2)\n",
    "#     M3 = Activation('tanh', name=\"M3\")(WY3)\n",
    "\n",
    "    alpha_1 = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_1\")(M1)\n",
    "    alpha_2 = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_2\")(M2)\n",
    "    alpha_3 = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_3\")(M3)\n",
    "    flat_alpha1 = Flatten(name=\"flat_alpha1\")(alpha_1)\n",
    "    flat_alpha2 = Flatten(name=\"flat_alpha2\")(alpha_2)\n",
    "    flat_alpha3 = Flatten(name=\"flat_alpha3\")(alpha_3)\n",
    "    alpha1 = Dense(MAXLEN, activation='softmax', name=\"alpha1\")(flat_alpha1)\n",
    "    alpha2 = Dense(MAXLEN, activation='softmax', name=\"alpha2\")(flat_alpha2)\n",
    "    alpha3 = Dense(MAXLEN, activation='softmax', name=\"alpha3\")(flat_alpha3)\n",
    "\n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "    Input_trans = Permute((2, 1), name=\"input_trans\")(main_input)\n",
    "\n",
    "    r_1 = merge([Input_trans, alpha1], output_shape=get_R_shape, name=\"r_1\", mode=get_R)\n",
    "    r_2 = merge([Input_trans, alpha2], output_shape=get_R_shape, name=\"r_2\", mode=get_R)\n",
    "    r_3 = merge([Input_trans, alpha3], output_shape=get_R_shape, name=\"r_3\", mode=get_R)\n",
    "\n",
    "    r1 = Reshape((1,len(chars)))(r_1)\n",
    "    r2 = Reshape((1,len(chars)))(r_2)\n",
    "    r3 = Reshape((1,len(chars)))(r_3)\n",
    "    \n",
    "#     r_1_trans = Permute((2, 1))(r1)\n",
    "#     r_2_trans = Permute((2, 1))(r2)\n",
    "#     r_3_trans = Permute((2, 1))(r3)\n",
    "\n",
    "#     r = RepeatVector(num_digits+1)(r_1)\n",
    "#     r = T.stack([r_1, r_2, r_3])\n",
    "    r = merge([r1, r2, r3], mode=stack_R, output_shape=get_stack_R_shape)\n",
    "# #     decoder_input = Permute((2, 1))(r)\n",
    "    \n",
    "    decoder_input = merge([Y, r], mode=stack_decoder_input, output_shape=stack_decoder_input_shape)\n",
    "                \n",
    "                \n",
    "                \n",
    "    decoded_result = RNN(hidden_size, input_shape=(num_digits+1, hidden_size), return_sequences=True)(decoder_input)\n",
    "    mapping = TimeDistributed(Dense(len(chars)))(decoded_result)\n",
    "    out = Activation('softmax')(mapping)\n",
    "    \n",
    "    output = out\n",
    "    model = Model(input=[main_input], output=output)\n",
    "#     model = Model(input=[main_input], output=r)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    a1 = model.get_layer('alpha1')\n",
    "    a2 = model.get_layer('alpha2')\n",
    "    a3 = model.get_layer('alpha3')\n",
    "    alpha1_f = K.function(inputs, [a1.output])\n",
    "    alpha2_f = K.function(inputs, [a2.output])\n",
    "    alpha3_f = K.function(inputs, [a3.output])\n",
    "\n",
    "    return model, alpha1_f, alpha2_f, alpha3_f, a1, a2, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Reshape\n",
    "\n",
    "def build_seq_model(hidden_size, num_layers, num_digits, num_ops):\n",
    "    # Most simple seq2seq model using encoder-decoder framework\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "    # note: in a situation where your input sequences have a variable length,\n",
    "    # use input_shape=(None, nb_feature).\n",
    "    encoder = RNN(hidden_size*(num_digits+1), input_shape=(MAXLEN, len(chars)))\n",
    "    model.add(encoder)\n",
    "    # For the decoder's input, we repeat the encoded input for each time step\n",
    "    model.add(Reshape((num_digits+1, hidden_size)))\n",
    "    # The decoder RNN could be multiple layers stacked or a single layer\n",
    "    for _ in range(num_layers):\n",
    "        decoder = RNN(hidden_size, return_sequences=True)\n",
    "        model.add(decoder)\n",
    "\n",
    "    # For each of step of the output sequence, decide which character should be chosen\n",
    "    mapper = TimeDistributed(Dense(len(chars)))\n",
    "    model.add(mapper)\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    encoder_f = K.function(inputs, [encoder.output])\n",
    "    decoder_f = K.function(inputs, [decoder.output])\n",
    "    mapper_f = K.function(inputs, [mapper.output])\n",
    "    \n",
    "    return model, encoder_f, decoder_f, mapper_f, encoder, decoder, mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, merge, TimeDistributedDense\n",
    "from keras.models import Model\n",
    "\n",
    "def feedback_model(hidden_size, num_layers, num_digits, num_ops):\n",
    "    ## Alternative model architecture\n",
    "    encoder_input = Input(shape=(MAXLEN, len(chars)), name = 'encoder_input')\n",
    "    decoder_input = Input(shape=(num_digits + 1, len(chars)), name = 'decoder_input')\n",
    "\n",
    "    x = RNN(hidden_size)(encoder_input)\n",
    "    context_input = RepeatVector(num_digits + 1)(x)\n",
    "    x = merge([context_input, decoder_input], mode='concat')\n",
    "    x = RNN(hidden_size, return_sequences=True)(x)\n",
    "\n",
    "#     loss = TimeDistributed(Dense(len(chars), activation='softmax'))(x)\n",
    "    loss = TimeDistributedDense(len(chars), activation='softmax')(x)\n",
    "    \n",
    "    model_alt = Model(input=[encoder_input, decoder_input], output=[loss])\n",
    "    model_alt.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning(model, X_train, y_train, iterations, X_val, y_val):\n",
    "    y_true = []\n",
    "    for idx in range(y_val.shape[0]):\n",
    "        y_true.append(ctable.decode(y_val[idx]))\n",
    "\n",
    "#     val_acc_array = []\n",
    "#     # Train the model each generation and show predictions against the validation dataset\n",
    "#     for iteration in range(1, iterations):\n",
    "#         print()\n",
    "#         print('-' * 50)\n",
    "#         print('Iteration', iteration)\n",
    "#         training_obj = model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=1,\n",
    "#                   validation_data=(X_val, y_val))\n",
    "#         ###\n",
    "#         # Select 10 samples from the validation set at random so we can visualize errors\n",
    "#     #     for i in range(10):\n",
    "#     #         ind = np.random.randint(0, len(X_val))\n",
    "#     #         rowX, rowy = X_val[np.array([ind])], y_val[np.array([ind])]\n",
    "#     #         preds = model.predict_classes(rowX, verbose=0)\n",
    "#     #         q = ctable.decode(rowX[0])\n",
    "#     #         correct = ctable.decode(rowy[0])\n",
    "#     #         guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "#     #         print('Q', q[::-1] if INVERT else q)\n",
    "#     #         print('T', correct)\n",
    "#     #         print(colors.ok + '☑' + colors.close if correct == guess else colors.fail + '☒' + colors.close, guess)\n",
    "#     #         print('---')\n",
    "\n",
    "# #         preds = model.predict_classes(X_val, verbose=0)\n",
    "# #         y_preds = []\n",
    "# #         for idx in range(preds.shape[0]):\n",
    "# #             y_preds.append(ctable.decode(preds[idx], calc_argmax=False))\n",
    "\n",
    "# #         acc = accuracy_score(y_true, y_preds)\n",
    "#         acc = training_obj.history['val_acc']\n",
    "#         val_acc_array.append(acc)\n",
    "#         print('Current validation accuracy = ' + str(acc))\n",
    "    \n",
    "#     return val_acc_array\n",
    "    training_obj = model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=iterations,\n",
    "        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data... \n",
      "Total addition questions: 5000\n",
      "Vectorization...\n",
      "(4500, 4, 12)\n",
      "(4500, 3, 12)\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "4500/4500 [==============================] - 3s - loss: 2.2684 - acc: 0.2861 - val_loss: 1.9646 - val_acc: 0.3540\n",
      "Epoch 2/30\n",
      "4500/4500 [==============================] - 3s - loss: 1.7794 - acc: 0.3490 - val_loss: 1.6627 - val_acc: 0.3660\n",
      "Epoch 3/30\n",
      "4500/4500 [==============================] - 4s - loss: 1.6321 - acc: 0.3848 - val_loss: 1.5968 - val_acc: 0.3740\n",
      "Epoch 4/30\n",
      "4500/4500 [==============================] - 3s - loss: 1.5844 - acc: 0.3908 - val_loss: 1.5510 - val_acc: 0.3940\n",
      "Epoch 5/30\n",
      "4500/4500 [==============================] - 3s - loss: 1.5506 - acc: 0.4096 - val_loss: 1.5252 - val_acc: 0.4147\n",
      "Epoch 6/30\n",
      "4500/4500 [==============================] - 3s - loss: 1.5174 - acc: 0.4308 - val_loss: 1.4930 - val_acc: 0.4353\n",
      "Epoch 7/30\n",
      "4500/4500 [==============================] - 4s - loss: 1.4884 - acc: 0.4410 - val_loss: 1.4504 - val_acc: 0.4607\n",
      "Epoch 8/30\n",
      "4500/4500 [==============================] - 3s - loss: 1.4263 - acc: 0.4661 - val_loss: 1.3923 - val_acc: 0.4700\n",
      "Epoch 9/30\n",
      "4500/4500 [==============================] - 3s - loss: 1.3588 - acc: 0.4884 - val_loss: 1.3250 - val_acc: 0.4873\n",
      "Epoch 10/30\n",
      "4500/4500 [==============================] - 3s - loss: 1.2910 - acc: 0.5230 - val_loss: 1.2638 - val_acc: 0.5367\n",
      "Epoch 11/30\n",
      "4500/4500 [==============================] - 6s - loss: 1.2254 - acc: 0.5507 - val_loss: 1.2133 - val_acc: 0.5387\n",
      "Epoch 12/30\n",
      "4500/4500 [==============================] - 5s - loss: 1.1764 - acc: 0.5722 - val_loss: 1.1813 - val_acc: 0.5640\n",
      "Epoch 13/30\n",
      "4500/4500 [==============================] - 7s - loss: 1.1444 - acc: 0.5734 - val_loss: 1.1255 - val_acc: 0.5887\n",
      "Epoch 14/30\n",
      "4500/4500 [==============================] - 6s - loss: 1.0891 - acc: 0.6101 - val_loss: 1.0938 - val_acc: 0.5873\n",
      "Epoch 15/30\n",
      "4500/4500 [==============================] - 4s - loss: 1.0529 - acc: 0.6208 - val_loss: 1.0513 - val_acc: 0.6173\n",
      "Epoch 16/30\n",
      "4500/4500 [==============================] - 4s - loss: 1.0171 - acc: 0.6329 - val_loss: 1.0198 - val_acc: 0.6240\n",
      "Epoch 17/30\n",
      "4500/4500 [==============================] - 4s - loss: 0.9857 - acc: 0.6419 - val_loss: 0.9908 - val_acc: 0.6253\n",
      "Epoch 18/30\n",
      "4500/4500 [==============================] - 4s - loss: 0.9557 - acc: 0.6481 - val_loss: 0.9469 - val_acc: 0.6453\n",
      "Epoch 19/30\n",
      "4500/4500 [==============================] - 4s - loss: 0.9085 - acc: 0.6644 - val_loss: 0.9055 - val_acc: 0.6540\n",
      "Epoch 20/30\n",
      "4500/4500 [==============================] - 3s - loss: 0.8615 - acc: 0.6827 - val_loss: 0.8584 - val_acc: 0.6740\n",
      "Epoch 21/30\n",
      "4500/4500 [==============================] - 4s - loss: 0.8112 - acc: 0.7047 - val_loss: 0.8017 - val_acc: 0.6987\n",
      "Epoch 22/30\n",
      "4500/4500 [==============================] - 3s - loss: 0.7492 - acc: 0.7377 - val_loss: 0.7454 - val_acc: 0.7460\n",
      "Epoch 23/30\n",
      "4500/4500 [==============================] - 3s - loss: 0.6878 - acc: 0.7746 - val_loss: 0.6716 - val_acc: 0.7893\n",
      "Epoch 24/30\n",
      "4500/4500 [==============================] - 4s - loss: 0.6279 - acc: 0.8126 - val_loss: 0.6114 - val_acc: 0.8140\n",
      "Epoch 25/30\n",
      "4500/4500 [==============================] - 3s - loss: 0.5760 - acc: 0.8402 - val_loss: 0.5618 - val_acc: 0.8480\n",
      "Epoch 26/30\n",
      "4500/4500 [==============================] - 3s - loss: 0.5277 - acc: 0.8661 - val_loss: 0.5156 - val_acc: 0.8680\n",
      "Epoch 27/30\n",
      "4500/4500 [==============================] - 4s - loss: 0.4831 - acc: 0.8908 - val_loss: 0.4845 - val_acc: 0.8773\n",
      "Epoch 28/30\n",
      "4500/4500 [==============================] - 4s - loss: 0.4479 - acc: 0.9068 - val_loss: 0.4413 - val_acc: 0.9120\n",
      "Epoch 29/30\n",
      "4500/4500 [==============================] - 4s - loss: 0.4107 - acc: 0.9206 - val_loss: 0.4078 - val_acc: 0.9173\n",
      "Epoch 30/30\n",
      "4500/4500 [==============================] - 4s - loss: 0.3798 - acc: 0.9320 - val_loss: 0.3761 - val_acc: 0.9220\n"
     ]
    }
   ],
   "source": [
    "DIGITS = 2\n",
    "OPS = 2\n",
    "TRAINING_SIZE = 5000\n",
    "questions, expected = generate_data(TRAINING_SIZE, DIGITS, OPS)\n",
    "X_train, y_train, X_val, y_val = create_train_valid(questions, expected, DIGITS, OPS)\n",
    "# model, encoder_f_, decoder_f_, mapper_f_, encoder, decoder, mapper = build_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS)\n",
    "# model, encoder_f_, decoder_f_, mapper_f_, encoder, decoder, mapper = build_att_seq2seq_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS, chars)\n",
    "# val_acc_2_2 = learning(model, X_train, y_train, 50, X_val, y_val)\n",
    "model, alpha1_f, alpha2_f, alpha3_f, aplha1, alpha2, alpha3 = build_att_seq2seq_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS, chars)\n",
    "# val_acc_2_2 = learning(model, X_train, y_train, 100, X_val, y_val)\n",
    "learning(model, X_train, y_train, 30, X_val, y_val)\n",
    "\n",
    "# y_train_pre = np.zeros(y_train.shape)\n",
    "# for idx in range(len(y_train_pre)):\n",
    "#     y_train_pre[idx][1:] = y_train[idx][:-1]\n",
    "# y_val_pre = np.zeros(y_val.shape)\n",
    "# for idx in range(len(y_val_pre)):\n",
    "#     y_val_pre[idx][1:] = y_val[idx][:-1]\n",
    "    \n",
    "# fb_model = feedback_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS)\n",
    "# fb_val_acc_2_2 = learning(fb_model, [X_train, y_train_pre], y_train, 50, [X_val, y_val_pre], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def alpha1_f_(X):\n",
    "    # The [0] is to disable the training phase flag\n",
    "    return alpha1_f([0] + [X])\n",
    "\n",
    "def alpha2_f_(X):\n",
    "    # The [0] is to disable the training phase flag\n",
    "    return alpha2_f([0] + [X])\n",
    "\n",
    "def alpha3_f_(X):\n",
    "    # The [0] is to disable the training phase flag\n",
    "    return alpha3_f([0] + [X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'76 '"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_str = '1354'\n",
    "X_str = X_str[::-1]\n",
    "print(X_str)\n",
    "X = ctable.encode(X_str, maxlen=MAXLEN).reshape([1,4,12])\n",
    "preds = model.predict(X, verbose=0)\n",
    "y_hat = preds[0].argmax(axis=-1)\n",
    "''.join(ctable.indices_char[x] for x in y_hat)# ctable.indices_char[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1 = alpha1_f_(X)\n",
    "a2 = alpha2_f_(X)\n",
    "a3 = alpha3_f_(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npM4Y/JLUGYNfkjrzX+05iP1fOv1rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bd0f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alphas = np.concatenate((a1[0], a2[0], a3[0]))\n",
    "pl.imshow(np.transpose(alphas), interpolation='none', cmap=cm.binary, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00245195,  0.46754903,  0.07227816,  0.45772087]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.67041255e-06,   2.02052330e-07,   4.38094029e-06,\n",
       "          9.99992747e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.75605568e-10,   9.92902155e-01,   7.09717751e-03,\n",
       "          6.67429121e-07]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
