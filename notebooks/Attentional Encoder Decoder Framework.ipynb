{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.engine.training import slice_X\n",
    "from keras.layers import Lambda, Flatten, Permute, Reshape, Input\n",
    "from keras.layers import merge, Merge, recurrent\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    '''\n",
    "    Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    '''\n",
    "    def __init__(self, chars, maxlen):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, self.char_indices[c]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset\n",
    "TRAINING_SIZE = 100000\n",
    "DIGITS = 5\n",
    "OPS = 2\n",
    "INVERT = True\n",
    "# Try replacing GRU, or SimpleRNN\n",
    "RNN = recurrent.LSTM\n",
    "HIDDEN_SIZE = 16\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "MAXLEN = OPS * DIGITS + OPS - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(training_size, num_digits, num_ops):\n",
    "    questions = []\n",
    "    expected = []\n",
    "    seen = set()\n",
    "    print('Generating data... ')\n",
    "    while len(questions) < training_size:\n",
    "#         f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, num_digits + 1))))\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(num_digits)))\n",
    "        ops = []\n",
    "        for i in range(num_ops):\n",
    "            ops.append(f())\n",
    "                    \n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that X+Y == Y+X (hence the sorting)\n",
    "#         ops.sort()\n",
    "        key = tuple(ops)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN\n",
    "        ops_str = []\n",
    "        format_str = '{:>' + str(num_digits) + '}'\n",
    "        for op in ops:\n",
    "            op_str = format_str.format(str(op))\n",
    "            ops_str.append(op_str)\n",
    "        \n",
    "        q = '+'.join([str(op) for op in ops_str])\n",
    "        query = q + ' ' * (MAXLEN - len(q))\n",
    "        ans = str(sum(ops))\n",
    "        # Answers can be of maximum size DIGITS + 1\n",
    "        if INVERT:\n",
    "            query = query[::-1]\n",
    "            ans = ans[::-1]\n",
    "        ans += ' ' * (num_digits + 1 - len(ans))\n",
    "        questions.append(query)\n",
    "        expected.append(ans)\n",
    "#         print(len(questions))\n",
    "    print('Total addition questions:', len(questions))\n",
    "    \n",
    "    return questions, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_valid(questions, expected, num_digits, num_ops, percentage):\n",
    "    print('Vectorization...')\n",
    "    X = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(questions), num_digits + 1, len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(questions):\n",
    "        X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "    for i, sentence in enumerate(expected):\n",
    "        y[i] = ctable.encode(sentence, maxlen=num_digits + 1)\n",
    "\n",
    "    # Shuffle (X, y) in unison as the later parts of X will almost all be larger digits\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Explicitly set apart 10% for validation data that we never train over\n",
    "    split_at = len(X) - len(X)*percentage\n",
    "    (X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n",
    "    (y_train, y_val) = (y[:split_at], y[split_at:])\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data... \n",
      "Total addition questions: 100000\n",
      "Vectorization...\n",
      "(50000, 11, 12)\n",
      "(50000, 6, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "questions, expected = generate_data(TRAINING_SIZE, DIGITS, OPS)\n",
    "X_train, y_train, X_val, y_val = create_train_valid(questions, expected, DIGITS, OPS, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'27068+48550'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'81657 '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Standard Encoder-decoder Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_seq2seq_model(hidden_size, num_layers, num_digits, num_ops):\n",
    "    # Most simple seq2seq model using encoder-decoder framework\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "    # note: in a situation where your input sequences have a variable length,\n",
    "    # use input_shape=(None, nb_feature).\n",
    "    encoder = RNN(hidden_size, input_shape=(MAXLEN, len(chars)))\n",
    "    model.add(encoder)\n",
    "    # For the decoder's input, we repeat the encoded input for each time step\n",
    "    model.add(RepeatVector(num_digits + 1))\n",
    "    # The decoder RNN could be multiple layers stacked or a single layer\n",
    "    for _ in range(num_layers):\n",
    "        decoder = RNN(hidden_size, return_sequences=True)\n",
    "        model.add(decoder)\n",
    "\n",
    "    # For each of step of the output sequence, decide which character should be chosen\n",
    "    mapper = TimeDistributed(Dense(len(chars)))\n",
    "    model.add(mapper)\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    encoder_f = K.function(inputs, [encoder.output])\n",
    "    decoder_f = K.function(inputs, [decoder.output])\n",
    "    mapper_f = K.function(inputs, [mapper.output])\n",
    "    \n",
    "    return model, encoder_f, decoder_f, mapper_f, encoder, decoder, mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Attentional Encoder-decoder Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "def get_last_Y(X):\n",
    "    return X[:, -1, :]\n",
    "\n",
    "def get_Y(X, xmaxlen):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.T.batched_dot(Y, alpha)\n",
    "    return ans\n",
    "\n",
    "def get_R_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    outshape = (shape[0][0],shape[0][1])\n",
    "    return tuple(outshape)\n",
    "\n",
    "def stack_decoder_input(X):\n",
    "    ans = K.concatenate(X, axis=2)\n",
    "    return ans\n",
    "\n",
    "def stack_decoder_input_shape(input_shape):\n",
    "    shape = list(input_shape)        \n",
    "    outshape = (shape[0][0], len(shape), shape[0][2])\n",
    "    return tuple(outshape)\n",
    "\n",
    "def attentional_seq2seq_model(hidden_size, num_layers, num_digits, num_ops, chars):    \n",
    "    main_input = Input(shape=(MAXLEN,len(chars)), name='main_input')\n",
    "    \n",
    "    encoder = RNN(hidden_size, \n",
    "                  input_shape=(MAXLEN, len(chars)),\n",
    "                  return_sequences=True)(main_input)\n",
    "    \n",
    "    Y = Lambda(get_Y, arguments={\"xmaxlen\": MAXLEN}, name=\"Y\", output_shape=(MAXLEN, hidden_size))(encoder)    \n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "#     Input_trans = Permute((2, 1), name=\"input_trans\")(main_input)\n",
    "\n",
    "    r_array = []\n",
    "    for idx in range(num_digits+1):\n",
    "        WY = TimeDistributed(Dense(len(chars)), name=\"WY_\"+str(idx))(Y)\n",
    "\n",
    "        M = Activation('tanh', name=\"M_\"+str(idx))(WY)\n",
    "        alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\"+str(idx))(M)\n",
    "        flat_alpha = Flatten(name=\"flat_alpha_\"+str(idx))(alpha_)\n",
    "        alpha = Dense(MAXLEN, activation='softmax', name=\"alpha\"+str(idx))(flat_alpha)\n",
    "\n",
    "        r_ = merge([Y_trans, alpha], output_shape=get_R_shape, name=\"r_\"+str(idx), mode=get_R)\n",
    "        r = Reshape((1,hidden_size))(r_)\n",
    "        r_array.append(r)\n",
    "        \n",
    "    decoder_input = merge(r_array, mode=stack_decoder_input, output_shape=stack_decoder_input_shape)            \n",
    "    decoded_result = RNN(hidden_size, input_shape=(num_digits+1, hidden_size), return_sequences=True)(decoder_input)\n",
    "    mapping = TimeDistributed(Dense(len(chars)))(decoded_result)\n",
    "    out = Activation('softmax')(mapping)\n",
    "    \n",
    "    model = Model(input=[main_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    a1 = model.get_layer('alpha1')\n",
    "    a2 = model.get_layer('alpha2')\n",
    "    a3 = model.get_layer('alpha3')\n",
    "    alpha1_f = K.function(inputs, [a1.output])\n",
    "    alpha2_f = K.function(inputs, [a2.output])\n",
    "    alpha3_f = K.function(inputs, [a3.output])\n",
    "\n",
    "    return model, alpha1_f, alpha2_f, alpha3_f, a1, a2, a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning(model, X_train, y_train, iterations, X_val, y_val):\n",
    "    y_true = []\n",
    "    for idx in range(y_val.shape[0]):\n",
    "        y_true.append(ctable.decode(y_val[idx]))\n",
    "\n",
    "    training_obj = model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=iterations,\n",
    "        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "std_model, encoder_f, decoder_f, mapper_f, encoder, decoder, mapper = standard_seq2seq_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS)\n",
    "# val_acc_2_2 = learning(model, X_train, y_train, 100, X_val, y_val)\n",
    "learning(std_model, X_train, y_train, 200, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 15s - loss: 2.1826 - acc: 0.1842 - val_loss: 2.0421 - val_acc: 0.2266\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.9922 - acc: 0.2359 - val_loss: 1.9565 - val_acc: 0.2461\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 20s - loss: 1.9441 - acc: 0.2519 - val_loss: 1.9334 - val_acc: 0.2569\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.9275 - acc: 0.2568 - val_loss: 1.9174 - val_acc: 0.2618\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.9071 - acc: 0.2627 - val_loss: 1.8873 - val_acc: 0.2733\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 22s - loss: 1.8634 - acc: 0.2857 - val_loss: 1.8375 - val_acc: 0.2975\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.8186 - acc: 0.3061 - val_loss: 1.8049 - val_acc: 0.3152\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.7903 - acc: 0.3226 - val_loss: 1.7786 - val_acc: 0.3276\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.7706 - acc: 0.3344 - val_loss: 1.7770 - val_acc: 0.3343\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.7550 - acc: 0.3425 - val_loss: 1.7472 - val_acc: 0.3470\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 21s - loss: 1.7416 - acc: 0.3481 - val_loss: 1.7420 - val_acc: 0.3316\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.7316 - acc: 0.3513 - val_loss: 1.7299 - val_acc: 0.3420\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.7216 - acc: 0.3560 - val_loss: 1.7208 - val_acc: 0.3515\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 18s - loss: 1.7145 - acc: 0.3581 - val_loss: 1.7163 - val_acc: 0.3552\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.7076 - acc: 0.3605 - val_loss: 1.7030 - val_acc: 0.3650\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.7007 - acc: 0.3630 - val_loss: 1.7022 - val_acc: 0.3628\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.6960 - acc: 0.3635 - val_loss: 1.6941 - val_acc: 0.3692\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.6901 - acc: 0.3663 - val_loss: 1.6891 - val_acc: 0.3659\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.6859 - acc: 0.3674 - val_loss: 1.6841 - val_acc: 0.3663\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 13s - loss: 1.6810 - acc: 0.3687 - val_loss: 1.6803 - val_acc: 0.3642\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.6767 - acc: 0.3700 - val_loss: 1.6765 - val_acc: 0.3684\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.6726 - acc: 0.3713 - val_loss: 1.6692 - val_acc: 0.3756\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 13s - loss: 1.6687 - acc: 0.3724 - val_loss: 1.6733 - val_acc: 0.3650\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6656 - acc: 0.3720 - val_loss: 1.6664 - val_acc: 0.3711\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.6617 - acc: 0.3741 - val_loss: 1.6619 - val_acc: 0.3713\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.6585 - acc: 0.3749 - val_loss: 1.6725 - val_acc: 0.3614\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 18s - loss: 1.6563 - acc: 0.3748 - val_loss: 1.6538 - val_acc: 0.3795\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6524 - acc: 0.3760 - val_loss: 1.6560 - val_acc: 0.3757\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6505 - acc: 0.3761 - val_loss: 1.6474 - val_acc: 0.3796\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6466 - acc: 0.3779 - val_loss: 1.6440 - val_acc: 0.3813\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6451 - acc: 0.3777 - val_loss: 1.6418 - val_acc: 0.3798\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6423 - acc: 0.3782 - val_loss: 1.6452 - val_acc: 0.3771\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6383 - acc: 0.3803 - val_loss: 1.6369 - val_acc: 0.3809\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6369 - acc: 0.3798 - val_loss: 1.6441 - val_acc: 0.3729\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6344 - acc: 0.3804 - val_loss: 1.6326 - val_acc: 0.3813\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6323 - acc: 0.3808 - val_loss: 1.6291 - val_acc: 0.3834\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6288 - acc: 0.3825 - val_loss: 1.6269 - val_acc: 0.3829\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6261 - acc: 0.3829 - val_loss: 1.6351 - val_acc: 0.3737\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6233 - acc: 0.3839 - val_loss: 1.6241 - val_acc: 0.3810\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6224 - acc: 0.3832 - val_loss: 1.6198 - val_acc: 0.3849\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6193 - acc: 0.3849 - val_loss: 1.6188 - val_acc: 0.3838\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6176 - acc: 0.3858 - val_loss: 1.6169 - val_acc: 0.3832\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 18s - loss: 1.6155 - acc: 0.3865 - val_loss: 1.6145 - val_acc: 0.3862\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6136 - acc: 0.3864 - val_loss: 1.6124 - val_acc: 0.3853\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6135 - acc: 0.3868 - val_loss: 1.6119 - val_acc: 0.3871\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6097 - acc: 0.3886 - val_loss: 1.6079 - val_acc: 0.3900\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 21s - loss: 1.6091 - acc: 0.3877 - val_loss: 1.6083 - val_acc: 0.3845\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 20s - loss: 1.6062 - acc: 0.3891 - val_loss: 1.6060 - val_acc: 0.3894\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6048 - acc: 0.3895 - val_loss: 1.6047 - val_acc: 0.3896\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6038 - acc: 0.3902 - val_loss: 1.6025 - val_acc: 0.3899\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6023 - acc: 0.3898 - val_loss: 1.6005 - val_acc: 0.3911\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6021 - acc: 0.3890 - val_loss: 1.6056 - val_acc: 0.3903\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5991 - acc: 0.3917 - val_loss: 1.5990 - val_acc: 0.3918\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5977 - acc: 0.3911 - val_loss: 1.6039 - val_acc: 0.3844\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5974 - acc: 0.3910 - val_loss: 1.5961 - val_acc: 0.3930\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5959 - acc: 0.3915 - val_loss: 1.6135 - val_acc: 0.3802\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5946 - acc: 0.3926 - val_loss: 1.5944 - val_acc: 0.3922\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5938 - acc: 0.3924 - val_loss: 1.5923 - val_acc: 0.3948\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5924 - acc: 0.3930 - val_loss: 1.6022 - val_acc: 0.3839\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5926 - acc: 0.3918 - val_loss: 1.5907 - val_acc: 0.3940\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.5920 - acc: 0.3917 - val_loss: 1.5910 - val_acc: 0.3913\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5899 - acc: 0.3926 - val_loss: 1.5907 - val_acc: 0.3925\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 20s - loss: 1.5885 - acc: 0.3937 - val_loss: 1.5944 - val_acc: 0.3884\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.5885 - acc: 0.3930 - val_loss: 1.5918 - val_acc: 0.3904\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.5876 - acc: 0.3932 - val_loss: 1.5854 - val_acc: 0.3961\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.5854 - acc: 0.3948 - val_loss: 1.6019 - val_acc: 0.3811\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 15s - loss: 1.5859 - acc: 0.3938 - val_loss: 1.5884 - val_acc: 0.3905\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5835 - acc: 0.3943 - val_loss: 1.5825 - val_acc: 0.3965\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.5837 - acc: 0.3946 - val_loss: 1.5865 - val_acc: 0.3937\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.5828 - acc: 0.3943 - val_loss: 1.5803 - val_acc: 0.3965\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 15s - loss: 1.5814 - acc: 0.3946 - val_loss: 1.5814 - val_acc: 0.3956\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5809 - acc: 0.3943 - val_loss: 1.5828 - val_acc: 0.3940\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 15s - loss: 1.5803 - acc: 0.3944 - val_loss: 1.5845 - val_acc: 0.3914\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5784 - acc: 0.3954 - val_loss: 1.5809 - val_acc: 0.3941\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5772 - acc: 0.3953 - val_loss: 1.5745 - val_acc: 0.3962\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5741 - acc: 0.3957 - val_loss: 1.5729 - val_acc: 0.3958\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5714 - acc: 0.3953 - val_loss: 1.5707 - val_acc: 0.3940\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 15s - loss: 1.5679 - acc: 0.3964 - val_loss: 1.5644 - val_acc: 0.3992\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 15s - loss: 1.5658 - acc: 0.3968 - val_loss: 1.5633 - val_acc: 0.3979\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.5602 - acc: 0.3992 - val_loss: 1.5614 - val_acc: 0.3973\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 13s - loss: 1.5596 - acc: 0.3983 - val_loss: 1.5634 - val_acc: 0.3926\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 13s - loss: 1.5539 - acc: 0.3997 - val_loss: 1.5580 - val_acc: 0.3963\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 13s - loss: 1.5517 - acc: 0.3996 - val_loss: 1.5526 - val_acc: 0.3980\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 13s - loss: 1.5471 - acc: 0.4016 - val_loss: 1.5454 - val_acc: 0.4012\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.5438 - acc: 0.4009 - val_loss: 1.5446 - val_acc: 0.3997\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 15s - loss: 1.5385 - acc: 0.4025 - val_loss: 1.5375 - val_acc: 0.4030\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5327 - acc: 0.4039 - val_loss: 1.5300 - val_acc: 0.4025\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.5248 - acc: 0.4054 - val_loss: 1.5222 - val_acc: 0.4061\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5196 - acc: 0.4066 - val_loss: 1.5185 - val_acc: 0.4043\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5111 - acc: 0.4089 - val_loss: 1.5083 - val_acc: 0.4097\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 18s - loss: 1.5047 - acc: 0.4098 - val_loss: 1.5026 - val_acc: 0.4102\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.4975 - acc: 0.4127 - val_loss: 1.4940 - val_acc: 0.4131\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.4911 - acc: 0.4144 - val_loss: 1.4881 - val_acc: 0.4156\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.4852 - acc: 0.4169 - val_loss: 1.4826 - val_acc: 0.4178\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 20s - loss: 1.4807 - acc: 0.4178 - val_loss: 1.4814 - val_acc: 0.4184\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 18s - loss: 1.4767 - acc: 0.4196 - val_loss: 1.4764 - val_acc: 0.4207\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 18s - loss: 1.4711 - acc: 0.4221 - val_loss: 1.4698 - val_acc: 0.4239\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.4670 - acc: 0.4240 - val_loss: 1.4637 - val_acc: 0.4250\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 15s - loss: 1.4633 - acc: 0.4258 - val_loss: 1.4625 - val_acc: 0.4263\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.4600 - acc: 0.4266 - val_loss: 1.4582 - val_acc: 0.4285\n"
     ]
    }
   ],
   "source": [
    "att_model, alpha1_f, alpha2_f, alpha3_f, aplha1, alpha2, alpha3 = attentional_seq2seq_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS, chars)\n",
    "# val_acc_2_2 = learning(model, X_train, y_train, 100, X_val, y_val)\n",
    "learning(att_model, X_train, y_train, 100, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12+31\n",
      "43 \n"
     ]
    }
   ],
   "source": [
    "X_str = '13+21'\n",
    "X_str = X_str[::-1]\n",
    "print(X_str)\n",
    "X = ctable.encode(X_str, maxlen=MAXLEN).reshape([1,5,12])\n",
    "preds = std_model.predict(X, verbose=0)\n",
    "y_hat = preds[0].argmax(axis=-1)\n",
    "y_str = ''.join(ctable.indices_char[x] for x in y_hat)# ctable.indices_char[x]\n",
    "print(y_str)\n",
    "preds2 = att_model.predict(X, verbose=0)\n",
    "y_hat2 = preds2[0].argmax(axis=-1)\n",
    "y_str2 = ''.join(ctable.indices_char[x] for x in y_hat2)# ctable.indices_char[x]\n",
    "print(y_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
