{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.engine.training import slice_X\n",
    "from keras.layers import Lambda, Flatten, Permute, Reshape, Input\n",
    "from keras.layers import merge, Merge, recurrent\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    '''\n",
    "    Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    '''\n",
    "    def __init__(self, chars, maxlen):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, self.char_indices[c]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset\n",
    "TRAINING_SIZE = 100000\n",
    "DIGITS = 5\n",
    "OPS = 2\n",
    "INVERT = True\n",
    "# Try replacing GRU, or SimpleRNN\n",
    "RNN = recurrent.LSTM\n",
    "HIDDEN_SIZE = 16\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "MAXLEN = OPS * DIGITS + OPS - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(training_size, num_digits, num_ops):\n",
    "    questions = []\n",
    "    expected = []\n",
    "    seen = set()\n",
    "    print('Generating data... ')\n",
    "    while len(questions) < training_size:\n",
    "#         f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, num_digits + 1))))\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(num_digits)))\n",
    "        ops = []\n",
    "        for i in range(num_ops):\n",
    "            ops.append(f())\n",
    "                    \n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that X+Y == Y+X (hence the sorting)\n",
    "#         ops.sort()\n",
    "        key = tuple(ops)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN\n",
    "        ops_str = []\n",
    "        format_str = '{:>' + str(num_digits) + '}'\n",
    "        for op in ops:\n",
    "            op_str = format_str.format(str(op))\n",
    "            ops_str.append(op_str)\n",
    "        \n",
    "        q = '+'.join([str(op) for op in ops_str])\n",
    "        query = q + ' ' * (MAXLEN - len(q))\n",
    "        ans = str(sum(ops))\n",
    "        # Answers can be of maximum size DIGITS + 1\n",
    "        if INVERT:\n",
    "            query = query[::-1]\n",
    "            ans = ans[::-1]\n",
    "        ans += ' ' * (num_digits + 1 - len(ans))\n",
    "        questions.append(query)\n",
    "        expected.append(ans)\n",
    "#         print(len(questions))\n",
    "    print('Total addition questions:', len(questions))\n",
    "    \n",
    "    return questions, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_valid(questions, expected, num_digits, num_ops, percentage):\n",
    "    print('Vectorization...')\n",
    "    X = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(questions), num_digits + 1, len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(questions):\n",
    "        X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "    for i, sentence in enumerate(expected):\n",
    "        y[i] = ctable.encode(sentence, maxlen=num_digits + 1)\n",
    "\n",
    "    # Shuffle (X, y) in unison as the later parts of X will almost all be larger digits\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Explicitly set apart 10% for validation data that we never train over\n",
    "    split_at = len(X) - len(X)*percentage\n",
    "    (X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n",
    "    (y_train, y_val) = (y[:split_at], y[split_at:])\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data... \n",
      "Total addition questions: 100000\n",
      "Vectorization...\n",
      "(50000, 11, 12)\n",
      "(50000, 6, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "questions, expected = generate_data(TRAINING_SIZE, DIGITS, OPS)\n",
    "X_train, y_train, X_val, y_val = create_train_valid(questions, expected, DIGITS, OPS, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'81657 '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Standard Encoder-decoder Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_seq2seq_model(hidden_size, num_layers, num_digits, num_ops):\n",
    "    # Most simple seq2seq model using encoder-decoder framework\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "    # note: in a situation where your input sequences have a variable length,\n",
    "    # use input_shape=(None, nb_feature).\n",
    "    encoder = RNN(hidden_size, input_shape=(MAXLEN, len(chars)))\n",
    "    model.add(encoder)\n",
    "    # For the decoder's input, we repeat the encoded input for each time step\n",
    "    model.add(RepeatVector(num_digits + 1))\n",
    "    # The decoder RNN could be multiple layers stacked or a single layer\n",
    "    for _ in range(num_layers):\n",
    "        decoder = RNN(hidden_size, return_sequences=True)\n",
    "        model.add(decoder)\n",
    "\n",
    "    # For each of step of the output sequence, decide which character should be chosen\n",
    "    mapper = TimeDistributed(Dense(len(chars)))\n",
    "    model.add(mapper)\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    encoder_f = K.function(inputs, [encoder.output])\n",
    "    decoder_f = K.function(inputs, [decoder.output])\n",
    "    mapper_f = K.function(inputs, [mapper.output])\n",
    "    \n",
    "    return model, encoder_f, decoder_f, mapper_f, encoder, decoder, mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Attentional Encoder-decoder Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "def get_last_Y(X):\n",
    "    return X[:, -1, :]\n",
    "\n",
    "def get_Y(X, xmaxlen):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.T.batched_dot(Y, alpha)\n",
    "    return ans\n",
    "\n",
    "def get_R_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    outshape = (shape[0][0],shape[0][1])\n",
    "    return tuple(outshape)\n",
    "\n",
    "def stack_decoder_input(X):\n",
    "    ans = K.concatenate(X, axis=2)\n",
    "    return ans\n",
    "\n",
    "def stack_decoder_input_shape(input_shape):\n",
    "    shape = list(input_shape)        \n",
    "    outshape = (shape[0][0], len(shape), shape[0][2])\n",
    "    return tuple(outshape)\n",
    "\n",
    "def attentional_seq2seq_model(hidden_size, num_layers, num_digits, num_ops, chars):    \n",
    "    main_input = Input(shape=(MAXLEN,len(chars)), name='main_input')\n",
    "    \n",
    "    encoder = RNN(hidden_size, \n",
    "                  input_shape=(MAXLEN, len(chars)),\n",
    "                  return_sequences=True)(main_input)\n",
    "    \n",
    "    Y = Lambda(get_Y, arguments={\"xmaxlen\": MAXLEN}, name=\"Y\", output_shape=(MAXLEN, hidden_size))(encoder)    \n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "#     Input_trans = Permute((2, 1), name=\"input_trans\")(main_input)\n",
    "\n",
    "    r_array = []\n",
    "    for idx in range(num_digits+1):\n",
    "        WY = TimeDistributed(Dense(len(chars)), name=\"WY_\"+str(idx))(Y)\n",
    "\n",
    "        M = Activation('tanh', name=\"M_\"+str(idx))(WY)\n",
    "        alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\"+str(idx))(M)\n",
    "        flat_alpha = Flatten(name=\"flat_alpha_\"+str(idx))(alpha_)\n",
    "        alpha = Dense(MAXLEN, activation='softmax', name=\"alpha\"+str(idx))(flat_alpha)\n",
    "\n",
    "        r_ = merge([Y_trans, alpha], output_shape=get_R_shape, name=\"r_\"+str(idx), mode=get_R)\n",
    "        r = Reshape((1,hidden_size))(r_)\n",
    "        r_array.append(r)\n",
    "        \n",
    "    decoder_input = merge(r_array, mode=stack_decoder_input, output_shape=stack_decoder_input_shape)            \n",
    "    decoded_result = RNN(hidden_size, input_shape=(num_digits+1, hidden_size), return_sequences=True)(decoder_input)\n",
    "    mapping = TimeDistributed(Dense(len(chars)))(decoded_result)\n",
    "    out = Activation('softmax')(mapping)\n",
    "    \n",
    "    model = Model(input=[main_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    a1 = model.get_layer('alpha1')\n",
    "    a2 = model.get_layer('alpha2')\n",
    "    a3 = model.get_layer('alpha3')\n",
    "    alpha1_f = K.function(inputs, [a1.output])\n",
    "    alpha2_f = K.function(inputs, [a2.output])\n",
    "    alpha3_f = K.function(inputs, [a3.output])\n",
    "\n",
    "    return model, alpha1_f, alpha2_f, alpha3_f, a1, a2, a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning(model, X_train, y_train, iterations, X_val, y_val):\n",
    "    y_true = []\n",
    "    for idx in range(y_val.shape[0]):\n",
    "        y_true.append(ctable.decode(y_val[idx]))\n",
    "\n",
    "    training_obj = model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=iterations,\n",
    "        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 10s - loss: 2.1779 - acc: 0.1907 - val_loss: 2.0455 - val_acc: 0.2162\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 10s - loss: 2.0087 - acc: 0.2275 - val_loss: 1.9798 - val_acc: 0.2412\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.9624 - acc: 0.2460 - val_loss: 1.9459 - val_acc: 0.2496\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.9320 - acc: 0.2565 - val_loss: 1.9135 - val_acc: 0.2661\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.8935 - acc: 0.2743 - val_loss: 1.8688 - val_acc: 0.2891\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 12s - loss: 1.8460 - acc: 0.2939 - val_loss: 1.8282 - val_acc: 0.3061\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.8148 - acc: 0.3087 - val_loss: 1.8173 - val_acc: 0.3012\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 12s - loss: 1.7930 - acc: 0.3204 - val_loss: 1.7839 - val_acc: 0.3216\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 12s - loss: 1.7736 - acc: 0.3309 - val_loss: 1.7723 - val_acc: 0.3261\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 12s - loss: 1.7563 - acc: 0.3389 - val_loss: 1.7475 - val_acc: 0.3469\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.7417 - acc: 0.3470 - val_loss: 1.7326 - val_acc: 0.3536\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.7264 - acc: 0.3559 - val_loss: 1.7210 - val_acc: 0.3596\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.7158 - acc: 0.3576 - val_loss: 1.7241 - val_acc: 0.3397\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.7054 - acc: 0.3608 - val_loss: 1.7098 - val_acc: 0.3549\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.6969 - acc: 0.3635 - val_loss: 1.6902 - val_acc: 0.3726\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.6889 - acc: 0.3662 - val_loss: 1.6855 - val_acc: 0.3639\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.6823 - acc: 0.3676 - val_loss: 1.6805 - val_acc: 0.3684\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 13s - loss: 1.6775 - acc: 0.3671 - val_loss: 1.6737 - val_acc: 0.3688\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.6729 - acc: 0.3685 - val_loss: 1.6708 - val_acc: 0.3691\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.6673 - acc: 0.3716 - val_loss: 1.6625 - val_acc: 0.3756\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.6627 - acc: 0.3727 - val_loss: 1.6585 - val_acc: 0.3764\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.6576 - acc: 0.3749 - val_loss: 1.6546 - val_acc: 0.3775\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 8s - loss: 1.6562 - acc: 0.3735 - val_loss: 1.6594 - val_acc: 0.3698\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 8s - loss: 1.6500 - acc: 0.3775 - val_loss: 1.6499 - val_acc: 0.3769\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.6506 - acc: 0.3748 - val_loss: 1.6476 - val_acc: 0.3732\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 8s - loss: 1.6469 - acc: 0.3766 - val_loss: 1.6560 - val_acc: 0.3642\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.6434 - acc: 0.3777 - val_loss: 1.6400 - val_acc: 0.3819\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.6417 - acc: 0.3779 - val_loss: 1.6418 - val_acc: 0.3750\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 7s - loss: 1.6385 - acc: 0.3794 - val_loss: 1.6336 - val_acc: 0.3843\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 8s - loss: 1.6356 - acc: 0.3804 - val_loss: 1.6390 - val_acc: 0.3731\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.6334 - acc: 0.3805 - val_loss: 1.6446 - val_acc: 0.3721\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 12s - loss: 1.6315 - acc: 0.3806 - val_loss: 1.6320 - val_acc: 0.3799\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.6314 - acc: 0.3791 - val_loss: 1.6259 - val_acc: 0.3857\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.6261 - acc: 0.3826 - val_loss: 1.6234 - val_acc: 0.3854\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.6280 - acc: 0.3791 - val_loss: 1.6333 - val_acc: 0.3724\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 12s - loss: 1.6231 - acc: 0.3824 - val_loss: 1.6318 - val_acc: 0.3699\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.6230 - acc: 0.3813 - val_loss: 1.6182 - val_acc: 0.3857\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 12s - loss: 1.6200 - acc: 0.3827 - val_loss: 1.6150 - val_acc: 0.3871\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.6175 - acc: 0.3830 - val_loss: 1.6140 - val_acc: 0.3857\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.6169 - acc: 0.3821 - val_loss: 1.6156 - val_acc: 0.3818\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.6134 - acc: 0.3833 - val_loss: 1.6144 - val_acc: 0.3806\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.6129 - acc: 0.3834 - val_loss: 1.6176 - val_acc: 0.3776\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.6120 - acc: 0.3829 - val_loss: 1.6115 - val_acc: 0.3805\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.6103 - acc: 0.3830 - val_loss: 1.6108 - val_acc: 0.3830\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 14s - loss: 1.6080 - acc: 0.3847 - val_loss: 1.6197 - val_acc: 0.3713\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 13s - loss: 1.6075 - acc: 0.3846 - val_loss: 1.6146 - val_acc: 0.3778\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.6063 - acc: 0.3837 - val_loss: 1.6027 - val_acc: 0.3881\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.6049 - acc: 0.3845 - val_loss: 1.6012 - val_acc: 0.3881\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.6028 - acc: 0.3857 - val_loss: 1.6158 - val_acc: 0.3746\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 17s - loss: 1.6010 - acc: 0.3865 - val_loss: 1.6121 - val_acc: 0.3780\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 9s - loss: 1.6013 - acc: 0.3853 - val_loss: 1.6195 - val_acc: 0.3678\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.5998 - acc: 0.3853 - val_loss: 1.6029 - val_acc: 0.3817\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 15s - loss: 1.5994 - acc: 0.3852 - val_loss: 1.6019 - val_acc: 0.3817\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 11s - loss: 1.5990 - acc: 0.3846 - val_loss: 1.6028 - val_acc: 0.3819\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 8s - loss: 1.5970 - acc: 0.3858 - val_loss: 1.5971 - val_acc: 0.3882\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 14s - loss: 1.5961 - acc: 0.3864 - val_loss: 1.6076 - val_acc: 0.3765\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 8s - loss: 1.5945 - acc: 0.3866 - val_loss: 1.6009 - val_acc: 0.3817\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 8s - loss: 1.5942 - acc: 0.3864 - val_loss: 1.5944 - val_acc: 0.3856\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 8s - loss: 1.5932 - acc: 0.3865 - val_loss: 1.5922 - val_acc: 0.3876\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 10s - loss: 1.5917 - acc: 0.3878 - val_loss: 1.5922 - val_acc: 0.3867\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 12s - loss: 1.5921 - acc: 0.3864 - val_loss: 1.5904 - val_acc: 0.3887\n",
      "Epoch 62/200\n",
      "31488/50000 [=================>............] - ETA: 5s - loss: 1.5906 - acc: 0.3876"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-208e1c59657c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstd_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_encoder_decode_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIGITS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# val_acc_2_2 = learning(model, X_train, y_train, 100, X_val, y_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-db6bcb0164f5>\u001b[0m in \u001b[0;36mlearning\u001b[0;34m(model, X_train, y_train, iterations, X_val, y_val)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     training_obj = model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=iterations,\n\u001b[0;32m----> 7\u001b[0;31m         validation_data=(X_val, y_val))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    822\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "std_model, encoder_f, decoder_f, mapper_f, encoder, decoder, mapper = standard_seq2seq_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS)\n",
    "# val_acc_2_2 = learning(model, X_train, y_train, 100, X_val, y_val)\n",
    "learning(std_model, X_train, y_train, 200, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 15s - loss: 2.1826 - acc: 0.1842 - val_loss: 2.0421 - val_acc: 0.2266\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.9922 - acc: 0.2359 - val_loss: 1.9565 - val_acc: 0.2461\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 20s - loss: 1.9441 - acc: 0.2519 - val_loss: 1.9334 - val_acc: 0.2569\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.9275 - acc: 0.2568 - val_loss: 1.9174 - val_acc: 0.2618\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.9071 - acc: 0.2627 - val_loss: 1.8873 - val_acc: 0.2733\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 22s - loss: 1.8634 - acc: 0.2857 - val_loss: 1.8375 - val_acc: 0.2975\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.8186 - acc: 0.3061 - val_loss: 1.8049 - val_acc: 0.3152\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.7903 - acc: 0.3226 - val_loss: 1.7786 - val_acc: 0.3276\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.7706 - acc: 0.3344 - val_loss: 1.7770 - val_acc: 0.3343\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.7550 - acc: 0.3425 - val_loss: 1.7472 - val_acc: 0.3470\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 21s - loss: 1.7416 - acc: 0.3481 - val_loss: 1.7420 - val_acc: 0.3316\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.7316 - acc: 0.3513 - val_loss: 1.7299 - val_acc: 0.3420\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.7216 - acc: 0.3560 - val_loss: 1.7208 - val_acc: 0.3515\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 18s - loss: 1.7145 - acc: 0.3581 - val_loss: 1.7163 - val_acc: 0.3552\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.7076 - acc: 0.3605 - val_loss: 1.7030 - val_acc: 0.3650\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.7007 - acc: 0.3630 - val_loss: 1.7022 - val_acc: 0.3628\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.6960 - acc: 0.3635 - val_loss: 1.6941 - val_acc: 0.3692\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.6901 - acc: 0.3663 - val_loss: 1.6891 - val_acc: 0.3659\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.6859 - acc: 0.3674 - val_loss: 1.6841 - val_acc: 0.3663\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 13s - loss: 1.6810 - acc: 0.3687 - val_loss: 1.6803 - val_acc: 0.3642\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.6767 - acc: 0.3700 - val_loss: 1.6765 - val_acc: 0.3684\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 14s - loss: 1.6726 - acc: 0.3713 - val_loss: 1.6692 - val_acc: 0.3756\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 13s - loss: 1.6687 - acc: 0.3724 - val_loss: 1.6733 - val_acc: 0.3650\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6656 - acc: 0.3720 - val_loss: 1.6664 - val_acc: 0.3711\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.6617 - acc: 0.3741 - val_loss: 1.6619 - val_acc: 0.3713\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.6585 - acc: 0.3749 - val_loss: 1.6725 - val_acc: 0.3614\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 18s - loss: 1.6563 - acc: 0.3748 - val_loss: 1.6538 - val_acc: 0.3795\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6524 - acc: 0.3760 - val_loss: 1.6560 - val_acc: 0.3757\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6505 - acc: 0.3761 - val_loss: 1.6474 - val_acc: 0.3796\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6466 - acc: 0.3779 - val_loss: 1.6440 - val_acc: 0.3813\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6451 - acc: 0.3777 - val_loss: 1.6418 - val_acc: 0.3798\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6423 - acc: 0.3782 - val_loss: 1.6452 - val_acc: 0.3771\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6383 - acc: 0.3803 - val_loss: 1.6369 - val_acc: 0.3809\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6369 - acc: 0.3798 - val_loss: 1.6441 - val_acc: 0.3729\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6344 - acc: 0.3804 - val_loss: 1.6326 - val_acc: 0.3813\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6323 - acc: 0.3808 - val_loss: 1.6291 - val_acc: 0.3834\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6288 - acc: 0.3825 - val_loss: 1.6269 - val_acc: 0.3829\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6261 - acc: 0.3829 - val_loss: 1.6351 - val_acc: 0.3737\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6233 - acc: 0.3839 - val_loss: 1.6241 - val_acc: 0.3810\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.6224 - acc: 0.3832 - val_loss: 1.6198 - val_acc: 0.3849\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6193 - acc: 0.3849 - val_loss: 1.6188 - val_acc: 0.3838\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6176 - acc: 0.3858 - val_loss: 1.6169 - val_acc: 0.3832\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 18s - loss: 1.6155 - acc: 0.3865 - val_loss: 1.6145 - val_acc: 0.3862\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6136 - acc: 0.3864 - val_loss: 1.6124 - val_acc: 0.3853\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6135 - acc: 0.3868 - val_loss: 1.6119 - val_acc: 0.3871\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6097 - acc: 0.3886 - val_loss: 1.6079 - val_acc: 0.3900\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 21s - loss: 1.6091 - acc: 0.3877 - val_loss: 1.6083 - val_acc: 0.3845\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 20s - loss: 1.6062 - acc: 0.3891 - val_loss: 1.6060 - val_acc: 0.3894\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6048 - acc: 0.3895 - val_loss: 1.6047 - val_acc: 0.3896\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6038 - acc: 0.3902 - val_loss: 1.6025 - val_acc: 0.3899\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6023 - acc: 0.3898 - val_loss: 1.6005 - val_acc: 0.3911\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.6021 - acc: 0.3890 - val_loss: 1.6056 - val_acc: 0.3903\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5991 - acc: 0.3917 - val_loss: 1.5990 - val_acc: 0.3918\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5977 - acc: 0.3911 - val_loss: 1.6039 - val_acc: 0.3844\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5974 - acc: 0.3910 - val_loss: 1.5961 - val_acc: 0.3930\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5959 - acc: 0.3915 - val_loss: 1.6135 - val_acc: 0.3802\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5946 - acc: 0.3926 - val_loss: 1.5944 - val_acc: 0.3922\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5938 - acc: 0.3924 - val_loss: 1.5923 - val_acc: 0.3948\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5924 - acc: 0.3930 - val_loss: 1.6022 - val_acc: 0.3839\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 17s - loss: 1.5926 - acc: 0.3918 - val_loss: 1.5907 - val_acc: 0.3940\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 19s - loss: 1.5920 - acc: 0.3917 - val_loss: 1.5910 - val_acc: 0.3913\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 16s - loss: 1.5899 - acc: 0.3926 - val_loss: 1.5907 - val_acc: 0.3925\n",
      "Epoch 63/100\n",
      "45824/50000 [==========================>...] - ETA: 1s - loss: 1.5885 - acc: 0.3935"
     ]
    }
   ],
   "source": [
    "model, alpha1_f, alpha2_f, alpha3_f, aplha1, alpha2, alpha3 = attentional_seq2seq_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS, chars)\n",
    "# val_acc_2_2 = learning(model, X_train, y_train, 100, X_val, y_val)\n",
    "learning(model, X_train, y_train, 100, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12+31\n",
      "43 \n"
     ]
    }
   ],
   "source": [
    "X_str = '13+21'\n",
    "X_str = X_str[::-1]\n",
    "print(X_str)\n",
    "X = ctable.encode(X_str, maxlen=MAXLEN).reshape([1,5,12])\n",
    "# preds = basic_model.predict(X, verbose=0)\n",
    "# y_hat = preds[0].argmax(axis=-1)\n",
    "# y_str = ''.join(ctable.indices_char[x] for x in y_hat)# ctable.indices_char[x]\n",
    "# print(y_str)\n",
    "preds2 = std_model.predict(X, verbose=0)\n",
    "y_hat2 = preds2[0].argmax(axis=-1)\n",
    "y_str2 = ''.join(ctable.indices_char[x] for x in y_hat2)# ctable.indices_char[x]\n",
    "print(y_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
