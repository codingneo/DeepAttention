{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.engine.training import slice_X\n",
    "from keras.layers import Lambda, Flatten, Permute \n",
    "from keras.layers import merge, Merge, Activation, TimeDistributed, Dense, RepeatVector, recurrent\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    '''\n",
    "    Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    '''\n",
    "    def __init__(self, chars, maxlen):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, self.char_indices[c]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset\n",
    "TRAINING_SIZE = 5000\n",
    "DIGITS = 2\n",
    "OPS = 2\n",
    "INVERT = True\n",
    "# Try replacing GRU, or SimpleRNN\n",
    "RNN = recurrent.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "MAXLEN = OPS * DIGITS + OPS - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(training_size, num_digits, num_ops):\n",
    "    MAXLEN = num_ops * num_digits + num_ops - 1\n",
    "    questions = []\n",
    "    expected = []\n",
    "    seen = set()\n",
    "    print('Generating data... ')\n",
    "    while len(questions) < training_size:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, num_digits + 1))))\n",
    "\n",
    "        ops = []\n",
    "        for i in range(num_ops):\n",
    "            ops.append(f())\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that X+Y == Y+X (hence the sorting)\n",
    "        ops.sort()\n",
    "        key = tuple(ops)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN\n",
    "        q = '+'.join([str(op) for op in ops])\n",
    "        query = q + ' ' * (MAXLEN - len(q))\n",
    "        ans = str(sum(ops))\n",
    "        # Answers can be of maximum size DIGITS + 1\n",
    "        ans += ' ' * (num_digits + 1 - len(ans))\n",
    "        if INVERT:\n",
    "            query = query[::-1]\n",
    "            ans = ans[::-1]\n",
    "        questions.append(query)\n",
    "        expected.append(ans)\n",
    "    print('Total addition questions:', len(questions))\n",
    "    \n",
    "    return questions, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_valid(questions, expected, num_digits, num_ops):\n",
    "    MAXLEN = num_ops * num_digits + num_ops - 1\n",
    "    print('Vectorization...')\n",
    "    X = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(questions), num_digits + 1, len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(questions):\n",
    "        X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "    for i, sentence in enumerate(expected):\n",
    "        y[i] = ctable.encode(sentence, maxlen=num_digits + 1)\n",
    "\n",
    "    # Shuffle (X, y) in unison as the later parts of X will almost all be larger digits\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Explicitly set apart 10% for validation data that we never train over\n",
    "    split_at = len(X) - len(X) / 10\n",
    "    (X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n",
    "    (y_train, y_val) = (y[:split_at], y[split_at:])\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(hidden_size, num_layers, num_digits, num_ops):\n",
    "    MAXLEN = num_ops * num_digits + num_ops - 1\n",
    "    # Most simple seq2seq model using encoder-decoder framework\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "    # note: in a situation where your input sequences have a variable length,\n",
    "    # use input_shape=(None, nb_feature).\n",
    "    encoder = RNN(hidden_size, input_shape=(MAXLEN, len(chars)))\n",
    "    model.add(encoder)\n",
    "    # For the decoder's input, we repeat the encoded input for each time step\n",
    "    model.add(RepeatVector(num_digits + 1))\n",
    "    # The decoder RNN could be multiple layers stacked or a single layer\n",
    "    for _ in range(num_layers):\n",
    "        decoder = RNN(hidden_size, return_sequences=True)\n",
    "        model.add(decoder)\n",
    "\n",
    "    # For each of step of the output sequence, decide which character should be chosen\n",
    "    mapper = TimeDistributed(Dense(len(chars)))\n",
    "    model.add(mapper)\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    encoder_f = K.function(inputs, [encoder.output])\n",
    "    decoder_f = K.function(inputs, [decoder.output])\n",
    "    mapper_f = K.function(inputs, [mapper.output])\n",
    "    \n",
    "    return model, encoder_f, decoder_f, mapper_f, encoder, decoder, mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "def get_Y(X, xmaxlen):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.T.batched_dot(Y, alpha)\n",
    "    return ans\n",
    "\n",
    "def stack_R(X):\n",
    "    r_1, r_2, r_3 = X[0], X[1], X[2]\n",
    "    ans = K.concatenate([r_1, r_2, r_3], axis=1)\n",
    "    return ans\n",
    "\n",
    "def build_att_seq2seq_model(hidden_size, num_layers, num_digits, num_ops, chars):\n",
    "    SEQ_LEN = num_ops * num_digits + num_ops - 1\n",
    "    \n",
    "    main_input = Input(shape=(SEQ_LEN,len(chars)), name='main_input')\n",
    "    encoder = RNN(hidden_size, \n",
    "                  input_shape=(SEQ_LEN, len(chars)),\n",
    "                  return_sequences=True)(main_input)\n",
    "    Y = Lambda(get_Y, arguments={\"xmaxlen\": SEQ_LEN}, name=\"Y\", output_shape=(SEQ_LEN, hidden_size))(encoder)\n",
    "    WY1 = TimeDistributed(Dense(hidden_size), name=\"WY1\")(Y)\n",
    "#     WY2 = TimeDistributed(Dense(hidden_size), name=\"WY2\")(Y)\n",
    "#     WY3 = TimeDistributed(Dense(hidden_size), name=\"WY3\")(Y)\n",
    "    M1 = Activation('tanh', name=\"M1\")(WY1)\n",
    "#     M2 = Activation('tanh', name=\"M2\")(WY2)\n",
    "#     M3 = Activation('tanh', name=\"M3\")(WY3)\n",
    "\n",
    "    alpha_1 = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_1\")(M1)\n",
    "    alpha_2 = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_2\")(M1)\n",
    "    alpha_3 = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_3\")(M1)\n",
    "    flat_alpha1 = Flatten(name=\"flat_alpha1\")(alpha_1)\n",
    "    flat_alpha2 = Flatten(name=\"flat_alpha2\")(alpha_2)\n",
    "    flat_alpha3 = Flatten(name=\"flat_alpha3\")(alpha_3)\n",
    "    alpha1 = Dense(SEQ_LEN, activation='softmax', name=\"alpha1\")(flat_alpha1)\n",
    "    alpha2 = Dense(SEQ_LEN, activation='softmax', name=\"alpha2\")(flat_alpha2)\n",
    "    alpha3 = Dense(SEQ_LEN, activation='softmax', name=\"alpha3\")(flat_alpha3)\n",
    "\n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "\n",
    "    r_1 = merge([Y_trans, alpha1], output_shape=(hidden_size, 1), name=\"r_1\", mode=get_R)\n",
    "    r_2 = merge([Y_trans, alpha2], output_shape=(hidden_size, 1), name=\"r_2\", mode=get_R)\n",
    "    r_3 = merge([Y_trans, alpha3], output_shape=(hidden_size, 1), name=\"r_3\", mode=get_R)\n",
    "    \n",
    "    r1 = Reshape((1,hidden_size))(r_1)\n",
    "    r2 = Reshape((1,hidden_size))(r_2)\n",
    "    r3 = Reshape((1,hidden_size))(r_3)\n",
    "    \n",
    "#     r_1_trans = Permute((2, 1))(r1)\n",
    "#     r_2_trans = Permute((2, 1))(r2)\n",
    "#     r_3_trans = Permute((2, 1))(r3)\n",
    "\n",
    "#     r = RepeatVector(num_digits+1)(r_1)\n",
    "#     r = T.stack([r_1, r_2, r_3])\n",
    "    r = merge([r1, r2, r3], mode=stack_R, output_shape=(3, hidden_size))\n",
    "#     decoder_input = Permute((2, 1))(r)\n",
    "    decoded_result = RNN(hidden_size, input_shape=(num_digits+1, hidden_size), return_sequences=True)(r)\n",
    "    mapping = TimeDistributed(Dense(len(chars)))(decoded_result)\n",
    "    out = Activation('softmax')(mapping)\n",
    "    \n",
    "    output = out\n",
    "    model = Model(input=[main_input], output=output)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Reshape\n",
    "\n",
    "def build_seq_model(hidden_size, num_layers, num_digits, num_ops):\n",
    "    MAXLEN = num_ops * num_digits + num_ops - 1\n",
    "    # Most simple seq2seq model using encoder-decoder framework\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "    # note: in a situation where your input sequences have a variable length,\n",
    "    # use input_shape=(None, nb_feature).\n",
    "    encoder = RNN(hidden_size*(num_digits+1), input_shape=(MAXLEN, len(chars)))\n",
    "    model.add(encoder)\n",
    "    # For the decoder's input, we repeat the encoded input for each time step\n",
    "    model.add(Reshape((num_digits+1, hidden_size)))\n",
    "    # The decoder RNN could be multiple layers stacked or a single layer\n",
    "    for _ in range(num_layers):\n",
    "        decoder = RNN(hidden_size, return_sequences=True)\n",
    "        model.add(decoder)\n",
    "\n",
    "    # For each of step of the output sequence, decide which character should be chosen\n",
    "    mapper = TimeDistributed(Dense(len(chars)))\n",
    "    model.add(mapper)\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    encoder_f = K.function(inputs, [encoder.output])\n",
    "    decoder_f = K.function(inputs, [decoder.output])\n",
    "    mapper_f = K.function(inputs, [mapper.output])\n",
    "    \n",
    "    return model, encoder_f, decoder_f, mapper_f, encoder, decoder, mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, merge, TimeDistributedDense\n",
    "from keras.models import Model\n",
    "\n",
    "def feedback_model(hidden_size, num_layers, num_digits, num_ops):\n",
    "    MAXLEN = num_ops * num_digits + num_ops - 1\n",
    "    ## Alternative model architecture\n",
    "    encoder_input = Input(shape=(MAXLEN, len(chars)), name = 'encoder_input')\n",
    "    decoder_input = Input(shape=(num_digits + 1, len(chars)), name = 'decoder_input')\n",
    "\n",
    "    x = RNN(hidden_size)(encoder_input)\n",
    "    context_input = RepeatVector(num_digits + 1)(x)\n",
    "    x = merge([context_input, decoder_input], mode='concat')\n",
    "    x = RNN(hidden_size, return_sequences=True)(x)\n",
    "\n",
    "#     loss = TimeDistributed(Dense(len(chars), activation='softmax'))(x)\n",
    "    loss = TimeDistributedDense(len(chars), activation='softmax')(x)\n",
    "    \n",
    "    model_alt = Model(input=[encoder_input, decoder_input], output=[loss])\n",
    "    model_alt.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning(model, X_train, y_train, iterations, X_val, y_val):\n",
    "    y_true = []\n",
    "    for idx in range(y_val.shape[0]):\n",
    "        y_true.append(ctable.decode(y_val[idx]))\n",
    "\n",
    "    val_acc_array = []\n",
    "    # Train the model each generation and show predictions against the validation dataset\n",
    "    for iteration in range(1, iterations):\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print('Iteration', iteration)\n",
    "        training_obj = model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=1,\n",
    "                  validation_data=(X_val, y_val))\n",
    "        ###\n",
    "        # Select 10 samples from the validation set at random so we can visualize errors\n",
    "    #     for i in range(10):\n",
    "    #         ind = np.random.randint(0, len(X_val))\n",
    "    #         rowX, rowy = X_val[np.array([ind])], y_val[np.array([ind])]\n",
    "    #         preds = model.predict_classes(rowX, verbose=0)\n",
    "    #         q = ctable.decode(rowX[0])\n",
    "    #         correct = ctable.decode(rowy[0])\n",
    "    #         guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "    #         print('Q', q[::-1] if INVERT else q)\n",
    "    #         print('T', correct)\n",
    "    #         print(colors.ok + '☑' + colors.close if correct == guess else colors.fail + '☒' + colors.close, guess)\n",
    "    #         print('---')\n",
    "\n",
    "#         preds = model.predict_classes(X_val, verbose=0)\n",
    "#         y_preds = []\n",
    "#         for idx in range(preds.shape[0]):\n",
    "#             y_preds.append(ctable.decode(preds[idx], calc_argmax=False))\n",
    "\n",
    "#         acc = accuracy_score(y_true, y_preds)\n",
    "        acc = training_obj.history['val_acc']\n",
    "        val_acc_array.append(acc)\n",
    "        print('Current validation accuracy = ' + str(acc))\n",
    "    \n",
    "    return val_acc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data... \n",
      "Total addition questions: 5000\n",
      "Vectorization...\n",
      "(4500, 5, 12)\n",
      "(4500, 3, 12)\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 2.3033 - acc: 0.2388 - val_loss: 2.1453 - val_acc: 0.3120\n",
      "Current validation accuracy = [0.312]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.9556 - acc: 0.3758 - val_loss: 1.7939 - val_acc: 0.3853\n",
      "Current validation accuracy = [0.38533333333333336]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7723 - acc: 0.3912 - val_loss: 1.7279 - val_acc: 0.3847\n",
      "Current validation accuracy = [0.38466666666666671]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7054 - acc: 0.3983 - val_loss: 1.6274 - val_acc: 0.3940\n",
      "Current validation accuracy = [0.39400000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.6130 - acc: 0.4101 - val_loss: 1.5544 - val_acc: 0.4293\n",
      "Current validation accuracy = [0.42933333333333334]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.5584 - acc: 0.4217 - val_loss: 1.5323 - val_acc: 0.4200\n",
      "Current validation accuracy = [0.41999999999999998]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.5138 - acc: 0.4396 - val_loss: 1.4737 - val_acc: 0.4320\n",
      "Current validation accuracy = [0.432]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.4686 - acc: 0.4605 - val_loss: 1.4384 - val_acc: 0.4633\n",
      "Current validation accuracy = [0.46333333333333332]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 1.4302 - acc: 0.4743 - val_loss: 1.4049 - val_acc: 0.4793\n",
      "Current validation accuracy = [0.47933333333333339]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 1.3978 - acc: 0.4901 - val_loss: 1.3876 - val_acc: 0.4653\n",
      "Current validation accuracy = [0.46533333333333332]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.3648 - acc: 0.5064 - val_loss: 1.3425 - val_acc: 0.4960\n",
      "Current validation accuracy = [0.496]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 7s - loss: 1.3147 - acc: 0.5262 - val_loss: 1.2715 - val_acc: 0.5373\n",
      "Current validation accuracy = [0.53733333333333333]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 8s - loss: 1.2653 - acc: 0.5515 - val_loss: 1.2309 - val_acc: 0.5620\n",
      "Current validation accuracy = [0.56200000000000006]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 10s - loss: 1.2259 - acc: 0.5716 - val_loss: 1.2263 - val_acc: 0.5380\n",
      "Current validation accuracy = [0.53800000000000003]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 7s - loss: 1.2007 - acc: 0.5746 - val_loss: 1.1700 - val_acc: 0.5927\n",
      "Current validation accuracy = [0.59266666666666667]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 1.1574 - acc: 0.6122 - val_loss: 1.1470 - val_acc: 0.5767\n",
      "Current validation accuracy = [0.57666666666666666]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 1.1348 - acc: 0.6108 - val_loss: 1.1328 - val_acc: 0.5787\n",
      "Current validation accuracy = [0.57866666666666666]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 1.1143 - acc: 0.6116 - val_loss: 1.0950 - val_acc: 0.6113\n",
      "Current validation accuracy = [0.6113333333333334]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.0767 - acc: 0.6350 - val_loss: 1.0684 - val_acc: 0.6173\n",
      "Current validation accuracy = [0.6173333333333334]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 1.0391 - acc: 0.6567 - val_loss: 1.0244 - val_acc: 0.6413\n",
      "Current validation accuracy = [0.64133333333333342]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 1.0019 - acc: 0.6696 - val_loss: 0.9985 - val_acc: 0.6467\n",
      "Current validation accuracy = [0.64666666666666661]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 7s - loss: 0.9742 - acc: 0.6724 - val_loss: 0.9706 - val_acc: 0.6673\n",
      "Current validation accuracy = [0.66733333333333322]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 7s - loss: 0.9399 - acc: 0.6867 - val_loss: 0.9299 - val_acc: 0.6627\n",
      "Current validation accuracy = [0.66266666666666663]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 0.9099 - acc: 0.6921 - val_loss: 0.9099 - val_acc: 0.6727\n",
      "Current validation accuracy = [0.67266666666666675]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 7s - loss: 0.8740 - acc: 0.6998 - val_loss: 0.8674 - val_acc: 0.6907\n",
      "Current validation accuracy = [0.69066666666666676]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 0.8446 - acc: 0.7109 - val_loss: 0.8159 - val_acc: 0.7127\n",
      "Current validation accuracy = [0.71266666666666678]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.7977 - acc: 0.7364 - val_loss: 0.7931 - val_acc: 0.7273\n",
      "Current validation accuracy = [0.72733333333333339]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.7691 - acc: 0.7471 - val_loss: 0.7631 - val_acc: 0.7373\n",
      "Current validation accuracy = [0.73733333333333329]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 7s - loss: 0.7368 - acc: 0.7558 - val_loss: 0.7279 - val_acc: 0.7480\n",
      "Current validation accuracy = [0.748]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 0.7060 - acc: 0.7721 - val_loss: 0.7061 - val_acc: 0.7760\n",
      "Current validation accuracy = [0.77600000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 31\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 0.6729 - acc: 0.7904 - val_loss: 0.6643 - val_acc: 0.7913\n",
      "Current validation accuracy = [0.79133333333333333]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 32\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 0.6405 - acc: 0.8004 - val_loss: 0.6496 - val_acc: 0.8000\n",
      "Current validation accuracy = [0.80000000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 33\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 0.6181 - acc: 0.8156 - val_loss: 0.6248 - val_acc: 0.7940\n",
      "Current validation accuracy = [0.79400000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 34\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 0.5948 - acc: 0.8199 - val_loss: 0.6028 - val_acc: 0.8300\n",
      "Current validation accuracy = [0.82999999999999996]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 35\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.5611 - acc: 0.8452 - val_loss: 0.5738 - val_acc: 0.8267\n",
      "Current validation accuracy = [0.82666666666666655]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 36\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.5495 - acc: 0.8428 - val_loss: 0.5475 - val_acc: 0.8573\n",
      "Current validation accuracy = [0.85733333333333339]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 37\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.5186 - acc: 0.8594 - val_loss: 0.5448 - val_acc: 0.8533\n",
      "Current validation accuracy = [0.8533333333333335]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 38\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.5003 - acc: 0.8697 - val_loss: 0.5049 - val_acc: 0.8793\n",
      "Current validation accuracy = [0.87933333333333341]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 39\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.4810 - acc: 0.8776 - val_loss: 0.4947 - val_acc: 0.8680\n",
      "Current validation accuracy = [0.86799999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 40\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.4661 - acc: 0.8802 - val_loss: 0.4765 - val_acc: 0.8820\n",
      "Current validation accuracy = [0.88200000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 41\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.4525 - acc: 0.8841 - val_loss: 0.5094 - val_acc: 0.8527\n",
      "Current validation accuracy = [0.85266666666666668]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 42\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.4413 - acc: 0.8867 - val_loss: 0.4543 - val_acc: 0.8860\n",
      "Current validation accuracy = [0.88600000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 43\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.4150 - acc: 0.8983 - val_loss: 0.4321 - val_acc: 0.8940\n",
      "Current validation accuracy = [0.89400000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 44\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.4014 - acc: 0.9008 - val_loss: 0.4337 - val_acc: 0.8847\n",
      "Current validation accuracy = [0.8846666666666666]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 45\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.3905 - acc: 0.9047 - val_loss: 0.4155 - val_acc: 0.9040\n",
      "Current validation accuracy = [0.90400000000000003]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 46\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.3734 - acc: 0.9085 - val_loss: 0.3946 - val_acc: 0.9053\n",
      "Current validation accuracy = [0.90533333333333332]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 47\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.3570 - acc: 0.9134 - val_loss: 0.3775 - val_acc: 0.9080\n",
      "Current validation accuracy = [0.90800000000000003]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 48\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 0.3466 - acc: 0.9176 - val_loss: 0.3810 - val_acc: 0.8953\n",
      "Current validation accuracy = [0.89533333333333354]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 49\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 0.3358 - acc: 0.9199 - val_loss: 0.3618 - val_acc: 0.9040\n",
      "Current validation accuracy = [0.90400000000000003]\n"
     ]
    }
   ],
   "source": [
    "DIGITS = 2\n",
    "OPS = 2\n",
    "TRAINING_SIZE = 5000\n",
    "questions, expected = generate_data(TRAINING_SIZE, DIGITS, OPS)\n",
    "X_train, y_train, X_val, y_val = create_train_valid(questions, expected, DIGITS, OPS)\n",
    "# model, encoder_f_, decoder_f_, mapper_f_, encoder, decoder, mapper = build_att_seq2seq_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS, chars)\n",
    "# val_acc_2_2 = learning(model, X_train, y_train, 50, X_val, y_val)\n",
    "model = build_att_seq2seq_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS, chars)\n",
    "val_acc_2_2 = learning(model, X_train, y_train, 50, X_val, y_val)\n",
    "\n",
    "# y_train_pre = np.zeros(y_train.shape)\n",
    "# for idx in range(len(y_train_pre)):\n",
    "#     y_train_pre[idx][1:] = y_train[idx][:-1]\n",
    "# y_val_pre = np.zeros(y_val.shape)\n",
    "# for idx in range(len(y_val_pre)):\n",
    "#     y_val_pre[idx][1:] = y_val[idx][:-1]\n",
    "    \n",
    "# fb_model = feedback_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS)\n",
    "# fb_val_acc_2_2 = learning(fb_model, [X_train, y_train_pre], y_train, 50, [X_val, y_val_pre], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers[3].get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data... \n",
      "Total addition questions: 50000\n",
      "Vectorization...\n",
      "(45000, 11, 12)\n",
      "(45000, 4, 12)\n",
      "Build model...\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 39s - loss: 1.8136 - acc: 0.3585 - val_loss: 1.6993 - val_acc: 0.3807\n",
      "Current validation accuracy = [0.38069999999999998]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 1.5668 - acc: 0.4196 - val_loss: 1.4374 - val_acc: 0.4574\n",
      "Current validation accuracy = [0.45745000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 1.3562 - acc: 0.4888 - val_loss: 1.2984 - val_acc: 0.5108\n",
      "Current validation accuracy = [0.51075000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 39s - loss: 1.2600 - acc: 0.5225 - val_loss: 1.2244 - val_acc: 0.5350\n",
      "Current validation accuracy = [0.53495000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 1.1883 - acc: 0.5480 - val_loss: 1.1680 - val_acc: 0.5526\n",
      "Current validation accuracy = [0.55259999999999998]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 1.1375 - acc: 0.5684 - val_loss: 1.1199 - val_acc: 0.5722\n",
      "Current validation accuracy = [0.57220000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 39s - loss: 1.0935 - acc: 0.5847 - val_loss: 1.0921 - val_acc: 0.5855\n",
      "Current validation accuracy = [0.58545000000000003]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 1.0505 - acc: 0.6009 - val_loss: 1.0378 - val_acc: 0.6086\n",
      "Current validation accuracy = [0.60860000000000003]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 1.0103 - acc: 0.6168 - val_loss: 1.0080 - val_acc: 0.6122\n",
      "Current validation accuracy = [0.61219999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 39s - loss: 0.9795 - acc: 0.6284 - val_loss: 0.9703 - val_acc: 0.6262\n",
      "Current validation accuracy = [0.62619999999999998]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.9460 - acc: 0.6415 - val_loss: 0.9495 - val_acc: 0.6407\n",
      "Current validation accuracy = [0.64070000000000005]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.9218 - acc: 0.6506 - val_loss: 0.9201 - val_acc: 0.6519\n",
      "Current validation accuracy = [0.65185000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 39s - loss: 0.8969 - acc: 0.6602 - val_loss: 0.9054 - val_acc: 0.6532\n",
      "Current validation accuracy = [0.6532]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.8733 - acc: 0.6687 - val_loss: 0.8987 - val_acc: 0.6516\n",
      "Current validation accuracy = [0.65164999999999995]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.8489 - acc: 0.6795 - val_loss: 0.8587 - val_acc: 0.6714\n",
      "Current validation accuracy = [0.67135]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 39s - loss: 0.8327 - acc: 0.6841 - val_loss: 0.8446 - val_acc: 0.6747\n",
      "Current validation accuracy = [0.67474999999999996]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.8103 - acc: 0.6935 - val_loss: 0.8460 - val_acc: 0.6684\n",
      "Current validation accuracy = [0.66839999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.7914 - acc: 0.7003 - val_loss: 0.8073 - val_acc: 0.6871\n",
      "Current validation accuracy = [0.68710000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 43s - loss: 0.7751 - acc: 0.7057 - val_loss: 0.7897 - val_acc: 0.6976\n",
      "Current validation accuracy = [0.69764999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.7536 - acc: 0.7149 - val_loss: 0.7832 - val_acc: 0.6968\n",
      "Current validation accuracy = [0.69684999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.7386 - acc: 0.7211 - val_loss: 0.7735 - val_acc: 0.7023\n",
      "Current validation accuracy = [0.70230000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.7257 - acc: 0.7249 - val_loss: 0.7463 - val_acc: 0.7132\n",
      "Current validation accuracy = [0.71319999999999995]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.7063 - acc: 0.7336 - val_loss: 0.7552 - val_acc: 0.7087\n",
      "Current validation accuracy = [0.7087]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.6899 - acc: 0.7400 - val_loss: 0.7275 - val_acc: 0.7163\n",
      "Current validation accuracy = [0.71625000000000005]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.6759 - acc: 0.7459 - val_loss: 0.6959 - val_acc: 0.7334\n",
      "Current validation accuracy = [0.73340000000000005]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 39s - loss: 0.6562 - acc: 0.7541 - val_loss: 0.6892 - val_acc: 0.7326\n",
      "Current validation accuracy = [0.73255000000000003]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.6428 - acc: 0.7587 - val_loss: 0.6793 - val_acc: 0.7379\n",
      "Current validation accuracy = [0.7379]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.6269 - acc: 0.7644 - val_loss: 0.6440 - val_acc: 0.7532\n",
      "Current validation accuracy = [0.75319999999999998]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.6086 - acc: 0.7734 - val_loss: 0.6706 - val_acc: 0.7379\n",
      "Current validation accuracy = [0.73785000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.6008 - acc: 0.7752 - val_loss: 0.6293 - val_acc: 0.7575\n",
      "Current validation accuracy = [0.75749999999999995]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 31\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.5858 - acc: 0.7806 - val_loss: 0.6230 - val_acc: 0.7610\n",
      "Current validation accuracy = [0.76100000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 32\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.5736 - acc: 0.7861 - val_loss: 0.6184 - val_acc: 0.7634\n",
      "Current validation accuracy = [0.76339999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 33\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.5568 - acc: 0.7937 - val_loss: 0.5892 - val_acc: 0.7737\n",
      "Current validation accuracy = [0.77370000000000005]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 34\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.5520 - acc: 0.7943 - val_loss: 0.6027 - val_acc: 0.7698\n",
      "Current validation accuracy = [0.76975000000000005]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 35\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.5350 - acc: 0.8027 - val_loss: 0.5934 - val_acc: 0.7721\n",
      "Current validation accuracy = [0.77210000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 36\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.5257 - acc: 0.8067 - val_loss: 0.5725 - val_acc: 0.7810\n",
      "Current validation accuracy = [0.78095000000000003]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 37\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.5166 - acc: 0.8090 - val_loss: 0.5689 - val_acc: 0.7788\n",
      "Current validation accuracy = [0.77880000000000005]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 38\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.5113 - acc: 0.8103 - val_loss: 0.5573 - val_acc: 0.7914\n",
      "Current validation accuracy = [0.79144999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 39\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.4980 - acc: 0.8164 - val_loss: 0.5549 - val_acc: 0.7893\n",
      "Current validation accuracy = [0.78925000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 40\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.4933 - acc: 0.8183 - val_loss: 0.5476 - val_acc: 0.7935\n",
      "Current validation accuracy = [0.79349999999999998]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 41\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.4776 - acc: 0.8260 - val_loss: 0.5414 - val_acc: 0.7933\n",
      "Current validation accuracy = [0.79325000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 42\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.4761 - acc: 0.8252 - val_loss: 0.5271 - val_acc: 0.8006\n",
      "Current validation accuracy = [0.80064999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 43\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.4692 - acc: 0.8283 - val_loss: 0.5401 - val_acc: 0.7966\n",
      "Current validation accuracy = [0.79664999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 44\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.4632 - acc: 0.8293 - val_loss: 0.5250 - val_acc: 0.8020\n",
      "Current validation accuracy = [0.80195000000000005]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 45\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.4521 - acc: 0.8353 - val_loss: 0.5069 - val_acc: 0.8102\n",
      "Current validation accuracy = [0.81020000000000003]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 46\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.4451 - acc: 0.8377 - val_loss: 0.5033 - val_acc: 0.8118\n",
      "Current validation accuracy = [0.81184999999999996]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 47\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.4427 - acc: 0.8371 - val_loss: 0.5059 - val_acc: 0.8112\n",
      "Current validation accuracy = [0.81115000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 48\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.4324 - acc: 0.8421 - val_loss: 0.5008 - val_acc: 0.8102\n",
      "Current validation accuracy = [0.81015000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 49\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.4276 - acc: 0.8441 - val_loss: 0.4918 - val_acc: 0.8162\n",
      "Current validation accuracy = [0.81620000000000004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CNSHYQH/anaconda2/lib/python2.7/site-packages/Keras-1.0.3-py2.7.egg/keras/layers/core.py:1015: UserWarning: TimeDistributedDense is deprecated, please use TimeDistributed(Dense(...)) instead.\n",
      "  warnings.warn('TimeDistributedDense is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 1.7630 - acc: 0.3662 - val_loss: 1.6598 - val_acc: 0.3878\n",
      "Current validation accuracy = [0.38784999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 43s - loss: 1.5903 - acc: 0.4066 - val_loss: 1.4825 - val_acc: 0.4408\n",
      "Current validation accuracy = [0.44080000000000003]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 1.4016 - acc: 0.4626 - val_loss: 1.3256 - val_acc: 0.4864\n",
      "Current validation accuracy = [0.48644999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 1.2804 - acc: 0.5060 - val_loss: 1.2344 - val_acc: 0.5220\n",
      "Current validation accuracy = [0.52200000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 1.1913 - acc: 0.5424 - val_loss: 1.1569 - val_acc: 0.5553\n",
      "Current validation accuracy = [0.55525000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 43s - loss: 1.1209 - acc: 0.5720 - val_loss: 1.1009 - val_acc: 0.5764\n",
      "Current validation accuracy = [0.57640000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 1.0668 - acc: 0.5923 - val_loss: 1.0511 - val_acc: 0.5981\n",
      "Current validation accuracy = [0.59809999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 1.0121 - acc: 0.6147 - val_loss: 0.9941 - val_acc: 0.6204\n",
      "Current validation accuracy = [0.62039999999999995]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.9618 - acc: 0.6332 - val_loss: 0.9482 - val_acc: 0.6359\n",
      "Current validation accuracy = [0.63590000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.9186 - acc: 0.6485 - val_loss: 0.9204 - val_acc: 0.6379\n",
      "Current validation accuracy = [0.63790000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.8764 - acc: 0.6644 - val_loss: 0.8893 - val_acc: 0.6517\n",
      "Current validation accuracy = [0.65169999999999995]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.8360 - acc: 0.6799 - val_loss: 0.8365 - val_acc: 0.6763\n",
      "Current validation accuracy = [0.67625000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.8053 - acc: 0.6915 - val_loss: 0.8169 - val_acc: 0.6817\n",
      "Current validation accuracy = [0.68169999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.7792 - acc: 0.6999 - val_loss: 0.7885 - val_acc: 0.6936\n",
      "Current validation accuracy = [0.69364999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.7496 - acc: 0.7110 - val_loss: 0.7694 - val_acc: 0.6936\n",
      "Current validation accuracy = [0.69364999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.7280 - acc: 0.7185 - val_loss: 0.7433 - val_acc: 0.7078\n",
      "Current validation accuracy = [0.70784999999999998]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.7018 - acc: 0.7288 - val_loss: 0.7121 - val_acc: 0.7217\n",
      "Current validation accuracy = [0.72170000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.6823 - acc: 0.7362 - val_loss: 0.7087 - val_acc: 0.7188\n",
      "Current validation accuracy = [0.71884999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.6643 - acc: 0.7419 - val_loss: 0.6881 - val_acc: 0.7275\n",
      "Current validation accuracy = [0.72745000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.6404 - acc: 0.7522 - val_loss: 0.6692 - val_acc: 0.7312\n",
      "Current validation accuracy = [0.73119999999999996]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.6284 - acc: 0.7557 - val_loss: 0.6456 - val_acc: 0.7446\n",
      "Current validation accuracy = [0.74460000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 43s - loss: 0.6080 - acc: 0.7654 - val_loss: 0.6469 - val_acc: 0.7365\n",
      "Current validation accuracy = [0.73650000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.6037 - acc: 0.7627 - val_loss: 0.6218 - val_acc: 0.7543\n",
      "Current validation accuracy = [0.75429999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.5787 - acc: 0.7760 - val_loss: 0.6460 - val_acc: 0.7379\n",
      "Current validation accuracy = [0.73794999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.5645 - acc: 0.7812 - val_loss: 0.6045 - val_acc: 0.7590\n",
      "Current validation accuracy = [0.75895000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.5528 - acc: 0.7851 - val_loss: 0.5821 - val_acc: 0.7675\n",
      "Current validation accuracy = [0.76749999999999996]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.5439 - acc: 0.7875 - val_loss: 0.5878 - val_acc: 0.7644\n",
      "Current validation accuracy = [0.76439999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 40s - loss: 0.5321 - acc: 0.7916 - val_loss: 0.5718 - val_acc: 0.7690\n",
      "Current validation accuracy = [0.76895000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 43s - loss: 0.5217 - acc: 0.7968 - val_loss: 0.5707 - val_acc: 0.7680\n",
      "Current validation accuracy = [0.76800000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.5070 - acc: 0.8028 - val_loss: 0.5539 - val_acc: 0.7755\n",
      "Current validation accuracy = [0.77549999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 31\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.4988 - acc: 0.8057 - val_loss: 0.5483 - val_acc: 0.7782\n",
      "Current validation accuracy = [0.77825]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 32\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.4912 - acc: 0.8076 - val_loss: 0.5205 - val_acc: 0.7923\n",
      "Current validation accuracy = [0.79225000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 33\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 44s - loss: 0.4831 - acc: 0.8106 - val_loss: 0.5174 - val_acc: 0.7900\n",
      "Current validation accuracy = [0.79000000000000004]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 34\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.4709 - acc: 0.8158 - val_loss: 0.5207 - val_acc: 0.7872\n",
      "Current validation accuracy = [0.78720000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 35\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.4644 - acc: 0.8185 - val_loss: 0.5005 - val_acc: 0.7994\n",
      "Current validation accuracy = [0.79944999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 36\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 44s - loss: 0.4618 - acc: 0.8179 - val_loss: 0.4971 - val_acc: 0.8019\n",
      "Current validation accuracy = [0.80189999999999995]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 37\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 43s - loss: 0.4485 - acc: 0.8242 - val_loss: 0.5088 - val_acc: 0.7964\n",
      "Current validation accuracy = [0.7964]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 38\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.4443 - acc: 0.8257 - val_loss: 0.4846 - val_acc: 0.8038\n",
      "Current validation accuracy = [0.80384999999999995]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 39\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 44s - loss: 0.4396 - acc: 0.8271 - val_loss: 0.4818 - val_acc: 0.8046\n",
      "Current validation accuracy = [0.80464999999999998]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 40\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 43s - loss: 0.4282 - acc: 0.8324 - val_loss: 0.4976 - val_acc: 0.7943\n",
      "Current validation accuracy = [0.79425000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 41\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 43s - loss: 0.4218 - acc: 0.8345 - val_loss: 0.4928 - val_acc: 0.7995\n",
      "Current validation accuracy = [0.79949999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 42\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.4213 - acc: 0.8345 - val_loss: 0.4706 - val_acc: 0.8078\n",
      "Current validation accuracy = [0.80784999999999996]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 43\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.4120 - acc: 0.8377 - val_loss: 0.4542 - val_acc: 0.8166\n",
      "Current validation accuracy = [0.81664999999999999]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 44\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.4030 - acc: 0.8427 - val_loss: 0.4515 - val_acc: 0.8158\n",
      "Current validation accuracy = [0.81579999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 45\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.4036 - acc: 0.8409 - val_loss: 0.4745 - val_acc: 0.8074\n",
      "Current validation accuracy = [0.80735000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 46\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.3945 - acc: 0.8459 - val_loss: 0.4672 - val_acc: 0.8057\n",
      "Current validation accuracy = [0.80569999999999997]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 47\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.3880 - acc: 0.8483 - val_loss: 0.4528 - val_acc: 0.8164\n",
      "Current validation accuracy = [0.81640000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 48\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 42s - loss: 0.3896 - acc: 0.8459 - val_loss: 0.4415 - val_acc: 0.8195\n",
      "Current validation accuracy = [0.81950000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 49\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 41s - loss: 0.3858 - acc: 0.8468 - val_loss: 0.4454 - val_acc: 0.8196\n",
      "Current validation accuracy = [0.81955]\n"
     ]
    }
   ],
   "source": [
    "DIGITS = 3\n",
    "OPS = 3\n",
    "TRAINING_SIZE = 50000\n",
    "questions, expected = generate_data(TRAINING_SIZE, DIGITS, OPS)\n",
    "X_train, y_train, X_val, y_val = create_train_valid(questions, expected, DIGITS, OPS)\n",
    "model = build_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS)\n",
    "val_acc_3_3 = learning(model, X_train, y_train, 50, X_val, y_val)\n",
    "\n",
    "y_train_pre = np.zeros(y_train.shape)\n",
    "for idx in range(len(y_train_pre)):\n",
    "    y_train_pre[idx][1:] = y_train[idx][:-1]\n",
    "y_val_pre = np.zeros(y_val.shape)\n",
    "for idx in range(len(y_val_pre)):\n",
    "    y_val_pre[idx][1:] = y_val[idx][:-1]\n",
    "    \n",
    "fb_model = feedback_model(HIDDEN_SIZE, LAYERS, DIGITS, OPS)\n",
    "fb_val_acc_3_3 = learning(fb_model, [X_train, y_train_pre], y_train, 50, [X_val, y_val_pre], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "(4500, 7, 12)\n",
      "(4500, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = create_train_valid(questions, expected, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "model = build_model(HIDDEN_SIZE, LAYERS, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 2.1080 - acc: 0.2800 - val_loss: 1.9476 - val_acc: 0.3295\n",
      "Current validation accuracy = [0.32950000214576719]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.8172 - acc: 0.3652 - val_loss: 1.7454 - val_acc: 0.3705\n",
      "Current validation accuracy = [0.37049999833106995]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.6995 - acc: 0.3834 - val_loss: 1.7012 - val_acc: 0.3735\n",
      "Current validation accuracy = [0.37350000119209292]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.6591 - acc: 0.3922 - val_loss: 1.6597 - val_acc: 0.3845\n",
      "Current validation accuracy = [0.38450000047683713]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.6244 - acc: 0.4018 - val_loss: 1.6345 - val_acc: 0.3930\n",
      "Current validation accuracy = [0.39300000286102293]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.5838 - acc: 0.4188 - val_loss: 1.6107 - val_acc: 0.4240\n",
      "Current validation accuracy = [0.4239999985694885]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.5436 - acc: 0.4353 - val_loss: 1.5457 - val_acc: 0.4315\n",
      "Current validation accuracy = [0.43149999737739564]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.5042 - acc: 0.4491 - val_loss: 1.5555 - val_acc: 0.4455\n",
      "Current validation accuracy = [0.44550000286102293]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.4735 - acc: 0.4644 - val_loss: 1.4741 - val_acc: 0.4520\n",
      "Current validation accuracy = [0.45199999713897704]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.4188 - acc: 0.4804 - val_loss: 1.4458 - val_acc: 0.4605\n",
      "Current validation accuracy = [0.4604999988079071]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.3818 - acc: 0.4914 - val_loss: 1.4223 - val_acc: 0.4745\n",
      "Current validation accuracy = [0.47450000047683716]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.3426 - acc: 0.5058 - val_loss: 1.3815 - val_acc: 0.4900\n",
      "Current validation accuracy = [0.48999999904632568]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.3145 - acc: 0.5136 - val_loss: 1.3685 - val_acc: 0.4925\n",
      "Current validation accuracy = [0.49249999785423276]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.2798 - acc: 0.5268 - val_loss: 1.3274 - val_acc: 0.4985\n",
      "Current validation accuracy = [0.49849999952316282]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.2489 - acc: 0.5349 - val_loss: 1.3007 - val_acc: 0.5125\n",
      "Current validation accuracy = [0.5125000057220459]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.2259 - acc: 0.5440 - val_loss: 1.3098 - val_acc: 0.5120\n",
      "Current validation accuracy = [0.51200000119209288]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.2263 - acc: 0.5479 - val_loss: 1.2555 - val_acc: 0.5155\n",
      "Current validation accuracy = [0.51549999809265135]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.1825 - acc: 0.5594 - val_loss: 1.2366 - val_acc: 0.5270\n",
      "Current validation accuracy = [0.52700000143051151]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.1538 - acc: 0.5718 - val_loss: 1.2195 - val_acc: 0.5415\n",
      "Current validation accuracy = [0.54149999904632573]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.1356 - acc: 0.5778 - val_loss: 1.2036 - val_acc: 0.5425\n",
      "Current validation accuracy = [0.54249999332427978]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.1158 - acc: 0.5831 - val_loss: 1.1916 - val_acc: 0.5515\n",
      "Current validation accuracy = [0.55149999666213989]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0935 - acc: 0.5948 - val_loss: 1.1728 - val_acc: 0.5495\n",
      "Current validation accuracy = [0.54949999904632574]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0725 - acc: 0.6042 - val_loss: 1.1548 - val_acc: 0.5665\n",
      "Current validation accuracy = [0.56649999761581415]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0539 - acc: 0.6096 - val_loss: 1.1479 - val_acc: 0.5680\n",
      "Current validation accuracy = [0.5680000023841858]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0404 - acc: 0.6130 - val_loss: 1.1342 - val_acc: 0.5730\n",
      "Current validation accuracy = [0.57300000619888303]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0211 - acc: 0.6235 - val_loss: 1.1272 - val_acc: 0.5700\n",
      "Current validation accuracy = [0.56999999427795411]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9987 - acc: 0.6323 - val_loss: 1.1111 - val_acc: 0.5780\n",
      "Current validation accuracy = [0.57800000619888303]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9818 - acc: 0.6417 - val_loss: 1.0868 - val_acc: 0.5880\n",
      "Current validation accuracy = [0.58799999809265135]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9621 - acc: 0.6506 - val_loss: 1.0788 - val_acc: 0.5855\n",
      "Current validation accuracy = [0.58549999809265141]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9459 - acc: 0.6524 - val_loss: 1.0993 - val_acc: 0.5800\n",
      "Current validation accuracy = [0.57999999761581422]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 31\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9295 - acc: 0.6614 - val_loss: 1.0613 - val_acc: 0.5900\n",
      "Current validation accuracy = [0.58999999475479126]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 32\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9182 - acc: 0.6679 - val_loss: 1.0540 - val_acc: 0.5895\n",
      "Current validation accuracy = [0.58949999570846556]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 33\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8891 - acc: 0.6793 - val_loss: 1.0403 - val_acc: 0.5975\n",
      "Current validation accuracy = [0.59750000667572023]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 34\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 0.8708 - acc: 0.6853 - val_loss: 1.0442 - val_acc: 0.5975\n",
      "Current validation accuracy = [0.59750000190734864]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 35\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8509 - acc: 0.6923 - val_loss: 1.0143 - val_acc: 0.6080\n",
      "Current validation accuracy = [0.60800000476837157]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 36\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8382 - acc: 0.6978 - val_loss: 1.0259 - val_acc: 0.6060\n",
      "Current validation accuracy = [0.60600000667572018]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 37\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8196 - acc: 0.7062 - val_loss: 1.0023 - val_acc: 0.6120\n",
      "Current validation accuracy = [0.6119999985694885]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 38\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.7986 - acc: 0.7147 - val_loss: 0.9883 - val_acc: 0.6180\n",
      "Current validation accuracy = [0.61799999618530277]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 39\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 0.7867 - acc: 0.7184 - val_loss: 0.9890 - val_acc: 0.6170\n",
      "Current validation accuracy = [0.61699999904632563]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 40\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 0.7644 - acc: 0.7274 - val_loss: 0.9809 - val_acc: 0.6240\n",
      "Current validation accuracy = [0.6239999966621399]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 41\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.7465 - acc: 0.7366 - val_loss: 0.9539 - val_acc: 0.6275\n",
      "Current validation accuracy = [0.62749999332427975]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 42\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.7310 - acc: 0.7430 - val_loss: 0.9569 - val_acc: 0.6285\n",
      "Current validation accuracy = [0.62849999666213985]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 43\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.7112 - acc: 0.7470 - val_loss: 0.9401 - val_acc: 0.6395\n",
      "Current validation accuracy = [0.63949999475479125]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 44\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.6918 - acc: 0.7579 - val_loss: 0.9259 - val_acc: 0.6365\n",
      "Current validation accuracy = [0.63649999713897709]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 45\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.6741 - acc: 0.7684 - val_loss: 0.9227 - val_acc: 0.6395\n",
      "Current validation accuracy = [0.63949999475479125]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 46\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.6615 - acc: 0.7702 - val_loss: 0.9193 - val_acc: 0.6445\n",
      "Current validation accuracy = [0.6444999971389771]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 47\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.6454 - acc: 0.7759 - val_loss: 0.9099 - val_acc: 0.6470\n",
      "Current validation accuracy = [0.64700000286102299]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 48\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.6246 - acc: 0.7888 - val_loss: 0.8916 - val_acc: 0.6650\n",
      "Current validation accuracy = [0.66499999523162845]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 49\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.6071 - acc: 0.7960 - val_loss: 0.8935 - val_acc: 0.6515\n",
      "Current validation accuracy = [0.65150000619888304]\n"
     ]
    }
   ],
   "source": [
    "val_acc_3_2 = learning(model, X_train, y_train, 50, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data... \n",
      "Total addition questions: 5000\n",
      "Vectorization...\n",
      "(4500, 8, 12)\n",
      "(4500, 3, 12)\n",
      "Build model...\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 2.2378 - acc: 0.2673 - val_loss: 2.0186 - val_acc: 0.3880\n",
      "Current validation accuracy = [0.38799999690055847]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.8676 - acc: 0.3744 - val_loss: 1.7854 - val_acc: 0.3953\n",
      "Current validation accuracy = [0.39533333849906921]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.7551 - acc: 0.3833 - val_loss: 1.7257 - val_acc: 0.3833\n",
      "Current validation accuracy = [0.38333333277702331]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.7051 - acc: 0.3917 - val_loss: 1.7177 - val_acc: 0.3793\n",
      "Current validation accuracy = [0.37933333468437197]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.6529 - acc: 0.4064 - val_loss: 1.6609 - val_acc: 0.3987\n",
      "Current validation accuracy = [0.39866666674613954]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.6175 - acc: 0.4200 - val_loss: 1.6029 - val_acc: 0.4073\n",
      "Current validation accuracy = [0.40733333611488343]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.5722 - acc: 0.4313 - val_loss: 1.5746 - val_acc: 0.4347\n",
      "Current validation accuracy = [0.43466666889190675]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.5241 - acc: 0.4462 - val_loss: 1.5384 - val_acc: 0.4413\n",
      "Current validation accuracy = [0.44133333015441895]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.4845 - acc: 0.4579 - val_loss: 1.4944 - val_acc: 0.4660\n",
      "Current validation accuracy = [0.46599999833106992]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.4495 - acc: 0.4755 - val_loss: 1.4691 - val_acc: 0.4520\n",
      "Current validation accuracy = [0.45199999928474427]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.3949 - acc: 0.4919 - val_loss: 1.4071 - val_acc: 0.4880\n",
      "Current validation accuracy = [0.48800000047683717]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.3798 - acc: 0.4911 - val_loss: 1.3712 - val_acc: 0.4880\n",
      "Current validation accuracy = [0.48800000548362732]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.3211 - acc: 0.5133 - val_loss: 1.3414 - val_acc: 0.5013\n",
      "Current validation accuracy = [0.50133333063125607]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.2776 - acc: 0.5339 - val_loss: 1.3181 - val_acc: 0.5067\n",
      "Current validation accuracy = [0.50666667318344116]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.2488 - acc: 0.5405 - val_loss: 1.2812 - val_acc: 0.5313\n",
      "Current validation accuracy = [0.53133335351943967]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.2251 - acc: 0.5516 - val_loss: 1.2285 - val_acc: 0.5467\n",
      "Current validation accuracy = [0.54666666412353515]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.2113 - acc: 0.5479 - val_loss: 1.2512 - val_acc: 0.5240\n",
      "Current validation accuracy = [0.52399999618530269]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1743 - acc: 0.5653 - val_loss: 1.2097 - val_acc: 0.5427\n",
      "Current validation accuracy = [0.54266665935516356]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.1655 - acc: 0.5681 - val_loss: 1.1834 - val_acc: 0.5647\n",
      "Current validation accuracy = [0.56466665697097773]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.1664 - acc: 0.5627 - val_loss: 1.1724 - val_acc: 0.5560\n",
      "Current validation accuracy = [0.55599999237060549]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.1391 - acc: 0.5762 - val_loss: 1.1466 - val_acc: 0.5600\n",
      "Current validation accuracy = [0.55999999809265133]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.1015 - acc: 0.5896 - val_loss: 1.1232 - val_acc: 0.5847\n",
      "Current validation accuracy = [0.58466665697097775]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0843 - acc: 0.5971 - val_loss: 1.1654 - val_acc: 0.5360\n",
      "Current validation accuracy = [0.53600000190734864]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0651 - acc: 0.6021 - val_loss: 1.1375 - val_acc: 0.5620\n",
      "Current validation accuracy = [0.56199999570846559]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.0572 - acc: 0.6044 - val_loss: 1.1366 - val_acc: 0.5553\n",
      "Current validation accuracy = [0.55533333921432493]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0483 - acc: 0.6068 - val_loss: 1.0976 - val_acc: 0.5793\n",
      "Current validation accuracy = [0.57933333063125614]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0236 - acc: 0.6230 - val_loss: 1.0776 - val_acc: 0.5907\n",
      "Current validation accuracy = [0.59066666507720944]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 1.0254 - acc: 0.6161 - val_loss: 1.1125 - val_acc: 0.5800\n",
      "Current validation accuracy = [0.5800000038146973]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.0232 - acc: 0.6171 - val_loss: 1.0576 - val_acc: 0.6067\n",
      "Current validation accuracy = [0.60666665554046628]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 0.9876 - acc: 0.6335 - val_loss: 1.0549 - val_acc: 0.6020\n",
      "Current validation accuracy = [0.60199999904632573]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 31\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 0.9934 - acc: 0.6321 - val_loss: 1.0385 - val_acc: 0.6087\n",
      "Current validation accuracy = [0.60866665792465213]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 32\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9678 - acc: 0.6401 - val_loss: 1.0387 - val_acc: 0.6020\n",
      "Current validation accuracy = [0.60200000381469732]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 33\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9605 - acc: 0.6429 - val_loss: 1.0339 - val_acc: 0.6027\n",
      "Current validation accuracy = [0.60266666030883786]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 34\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 0.9491 - acc: 0.6515 - val_loss: 1.0153 - val_acc: 0.6107\n",
      "Current validation accuracy = [0.6106666541099548]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 35\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9490 - acc: 0.6499 - val_loss: 1.0178 - val_acc: 0.6240\n",
      "Current validation accuracy = [0.62399998950958246]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 36\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 0.9371 - acc: 0.6557 - val_loss: 1.0143 - val_acc: 0.6167\n",
      "Current validation accuracy = [0.61666667985916135]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 37\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9265 - acc: 0.6613 - val_loss: 1.0095 - val_acc: 0.6193\n",
      "Current validation accuracy = [0.61933333063125606]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 38\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9228 - acc: 0.6629 - val_loss: 1.0019 - val_acc: 0.6193\n",
      "Current validation accuracy = [0.61933332967758181]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 39\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9041 - acc: 0.6738 - val_loss: 1.0669 - val_acc: 0.5860\n",
      "Current validation accuracy = [0.58600002145767216]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 40\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9386 - acc: 0.6519 - val_loss: 0.9860 - val_acc: 0.6333\n",
      "Current validation accuracy = [0.63333333873748776]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 41\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9060 - acc: 0.6667 - val_loss: 1.0196 - val_acc: 0.6080\n",
      "Current validation accuracy = [0.60800000715255742]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 42\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8940 - acc: 0.6763 - val_loss: 0.9762 - val_acc: 0.6300\n",
      "Current validation accuracy = [0.62999999904632564]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 43\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.9031 - acc: 0.6678 - val_loss: 0.9903 - val_acc: 0.6153\n",
      "Current validation accuracy = [0.61533332729339596]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 44\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8774 - acc: 0.6806 - val_loss: 0.9847 - val_acc: 0.6320\n",
      "Current validation accuracy = [0.63199999523162842]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 45\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8758 - acc: 0.6831 - val_loss: 0.9621 - val_acc: 0.6407\n",
      "Current validation accuracy = [0.64066665887832641]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 46\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8733 - acc: 0.6803 - val_loss: 0.9744 - val_acc: 0.6280\n",
      "Current validation accuracy = [0.62799998617172237]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 47\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 2s - loss: 0.8606 - acc: 0.6879 - val_loss: 0.9517 - val_acc: 0.6373\n",
      "Current validation accuracy = [0.63733332586288449]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 48\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8455 - acc: 0.6956 - val_loss: 0.9483 - val_acc: 0.6453\n",
      "Current validation accuracy = [0.64533334112167362]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 49\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 0.8528 - acc: 0.6902 - val_loss: 0.9388 - val_acc: 0.6547\n",
      "Current validation accuracy = [0.65466666221618652]\n"
     ]
    }
   ],
   "source": [
    "questions, expected = generate_data(TRAINING_SIZE, 2, 3)\n",
    "X_train, y_train, X_val, y_val = create_train_valid(questions, expected, 2, 3)\n",
    "model = build_model(HIDDEN_SIZE, LAYERS, 2, 3)\n",
    "val_acc_2_3 = learning(model, X_train, y_train, 50, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data... \n",
      "Total addition questions: 5000\n",
      "Vectorization...\n",
      "(4500, 11, 12)\n",
      "(4500, 4, 12)\n",
      "Build model...\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 2.1491 - acc: 0.2719 - val_loss: 1.9375 - val_acc: 0.3605\n",
      "Current validation accuracy = [0.36049999785423281]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.8317 - acc: 0.3777 - val_loss: 1.8095 - val_acc: 0.3755\n",
      "Current validation accuracy = [0.37549999952316282]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7611 - acc: 0.3820 - val_loss: 1.7523 - val_acc: 0.3815\n",
      "Current validation accuracy = [0.38150000119209287]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7450 - acc: 0.3866 - val_loss: 1.7818 - val_acc: 0.3840\n",
      "Current validation accuracy = [0.38400000119209288]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7325 - acc: 0.3894 - val_loss: 1.7243 - val_acc: 0.3960\n",
      "Current validation accuracy = [0.39599999761581423]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7126 - acc: 0.3907 - val_loss: 1.7112 - val_acc: 0.3930\n",
      "Current validation accuracy = [0.39300000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.6986 - acc: 0.3945 - val_loss: 1.7356 - val_acc: 0.3865\n",
      "Current validation accuracy = [0.38650000333786011]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.6931 - acc: 0.3953 - val_loss: 1.6847 - val_acc: 0.3880\n",
      "Current validation accuracy = [0.38800000286102293]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.6699 - acc: 0.3984 - val_loss: 1.7331 - val_acc: 0.3985\n",
      "Current validation accuracy = [0.39849999713897705]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.6694 - acc: 0.4018 - val_loss: 1.6519 - val_acc: 0.4100\n",
      "Current validation accuracy = [0.41000000143051146]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.6414 - acc: 0.4029 - val_loss: 1.6289 - val_acc: 0.4170\n",
      "Current validation accuracy = [0.41700000143051147]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.6189 - acc: 0.4107 - val_loss: 1.6019 - val_acc: 0.4255\n",
      "Current validation accuracy = [0.42549999904632568]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.5977 - acc: 0.4187 - val_loss: 1.6026 - val_acc: 0.4230\n",
      "Current validation accuracy = [0.42300000190734866]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.5696 - acc: 0.4253 - val_loss: 1.5525 - val_acc: 0.4270\n",
      "Current validation accuracy = [0.42700000023841855]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.5477 - acc: 0.4307 - val_loss: 1.5471 - val_acc: 0.4355\n",
      "Current validation accuracy = [0.43550000190734861]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.5119 - acc: 0.4419 - val_loss: 1.5903 - val_acc: 0.4305\n",
      "Current validation accuracy = [0.43050000071525574]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.5039 - acc: 0.4462 - val_loss: 1.5131 - val_acc: 0.4545\n",
      "Current validation accuracy = [0.45449999713897704]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.4677 - acc: 0.4559 - val_loss: 1.4655 - val_acc: 0.4530\n",
      "Current validation accuracy = [0.45300000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.4438 - acc: 0.4671 - val_loss: 1.4515 - val_acc: 0.4710\n",
      "Current validation accuracy = [0.47100000047683716]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.4170 - acc: 0.4746 - val_loss: 1.4253 - val_acc: 0.4680\n",
      "Current validation accuracy = [0.46799999809265136]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.4030 - acc: 0.4834 - val_loss: 1.4145 - val_acc: 0.4820\n",
      "Current validation accuracy = [0.48199999761581419]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.3805 - acc: 0.4886 - val_loss: 1.4226 - val_acc: 0.4665\n",
      "Current validation accuracy = [0.46650000095367433]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.3605 - acc: 0.4968 - val_loss: 1.3972 - val_acc: 0.4770\n",
      "Current validation accuracy = [0.47699999928474424]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 3s - loss: 1.3538 - acc: 0.4943 - val_loss: 1.3861 - val_acc: 0.4840\n",
      "Current validation accuracy = [0.48400000309944152]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.3272 - acc: 0.5078 - val_loss: 1.3861 - val_acc: 0.4830\n",
      "Current validation accuracy = [0.4830000026226044]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.3079 - acc: 0.5138 - val_loss: 1.3540 - val_acc: 0.4860\n",
      "Current validation accuracy = [0.48599999952316286]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.2897 - acc: 0.5228 - val_loss: 1.3584 - val_acc: 0.4905\n",
      "Current validation accuracy = [0.49050000190734866]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.2835 - acc: 0.5237 - val_loss: 1.3536 - val_acc: 0.4970\n",
      "Current validation accuracy = [0.49700000286102297]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.2669 - acc: 0.5341 - val_loss: 1.3363 - val_acc: 0.5085\n",
      "Current validation accuracy = [0.50849999761581421]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2528 - acc: 0.5383 - val_loss: 1.3528 - val_acc: 0.5010\n",
      "Current validation accuracy = [0.50099999475479129]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 31\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.2503 - acc: 0.5392 - val_loss: 1.3543 - val_acc: 0.5035\n",
      "Current validation accuracy = [0.50350000119209293]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 32\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.2312 - acc: 0.5462 - val_loss: 1.3076 - val_acc: 0.5190\n",
      "Current validation accuracy = [0.51899999475479131]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 33\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.2117 - acc: 0.5510 - val_loss: 1.3136 - val_acc: 0.5050\n",
      "Current validation accuracy = [0.50500000119209287]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 34\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.2127 - acc: 0.5507 - val_loss: 1.3248 - val_acc: 0.5075\n",
      "Current validation accuracy = [0.50749999761581421]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 35\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1967 - acc: 0.5638 - val_loss: 1.3068 - val_acc: 0.5085\n",
      "Current validation accuracy = [0.50850000286102293]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 36\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1832 - acc: 0.5642 - val_loss: 1.2960 - val_acc: 0.5215\n",
      "Current validation accuracy = [0.52150000667572016]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 37\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1672 - acc: 0.5770 - val_loss: 1.2926 - val_acc: 0.5205\n",
      "Current validation accuracy = [0.52050000095367432]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 38\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1549 - acc: 0.5777 - val_loss: 1.2870 - val_acc: 0.5295\n",
      "Current validation accuracy = [0.52950000095367433]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 39\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1420 - acc: 0.5838 - val_loss: 1.2996 - val_acc: 0.5100\n",
      "Current validation accuracy = [0.5099999947547913]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 40\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1373 - acc: 0.5831 - val_loss: 1.2896 - val_acc: 0.5250\n",
      "Current validation accuracy = [0.52500000381469725]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 41\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1247 - acc: 0.5910 - val_loss: 1.2841 - val_acc: 0.5295\n",
      "Current validation accuracy = [0.52950000143051146]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 42\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1073 - acc: 0.6016 - val_loss: 1.2655 - val_acc: 0.5345\n",
      "Current validation accuracy = [0.53450000095367434]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 43\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.1060 - acc: 0.5986 - val_loss: 1.3013 - val_acc: 0.5150\n",
      "Current validation accuracy = [0.51500000047683714]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 44\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.0888 - acc: 0.6068 - val_loss: 1.2677 - val_acc: 0.5385\n",
      "Current validation accuracy = [0.53849999904632573]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 45\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.0854 - acc: 0.6093 - val_loss: 1.2675 - val_acc: 0.5380\n",
      "Current validation accuracy = [0.53799999332427983]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 46\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.0686 - acc: 0.6152 - val_loss: 1.2771 - val_acc: 0.5360\n",
      "Current validation accuracy = [0.53599999380111696]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 47\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.0619 - acc: 0.6198 - val_loss: 1.2921 - val_acc: 0.5270\n",
      "Current validation accuracy = [0.52699999809265141]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 48\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.0453 - acc: 0.6278 - val_loss: 1.2754 - val_acc: 0.5335\n",
      "Current validation accuracy = [0.53349999856948849]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 49\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.0375 - acc: 0.6270 - val_loss: 1.2800 - val_acc: 0.5215\n",
      "Current validation accuracy = [0.52149999809265135]\n"
     ]
    }
   ],
   "source": [
    "questions, expected = generate_data(TRAINING_SIZE, 3, 3)\n",
    "X_train, y_train, X_val, y_val = create_train_valid(questions, expected, 3, 3)\n",
    "model = build_model(HIDDEN_SIZE, LAYERS, 3, 3)\n",
    "val_acc_3_3 = learning(model, X_train, y_train, 50, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data... \n",
      "Total addition questions: 5000\n",
      "Vectorization...\n",
      "(4500, 15, 12)\n",
      "(4500, 4, 12)\n",
      "Build model...\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 2.1641 - acc: 0.2753 - val_loss: 1.9400 - val_acc: 0.3530\n",
      "Current validation accuracy = [0.35299999856948855]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.9000 - acc: 0.3496 - val_loss: 1.8691 - val_acc: 0.3585\n",
      "Current validation accuracy = [0.3585000026226044]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.8458 - acc: 0.3583 - val_loss: 1.8539 - val_acc: 0.3545\n",
      "Current validation accuracy = [0.35449999856948855]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.8302 - acc: 0.3650 - val_loss: 1.8399 - val_acc: 0.3540\n",
      "Current validation accuracy = [0.35399999785423281]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.8166 - acc: 0.3648 - val_loss: 1.8344 - val_acc: 0.3510\n",
      "Current validation accuracy = [0.3509999969005585]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.8098 - acc: 0.3616 - val_loss: 1.8257 - val_acc: 0.3535\n",
      "Current validation accuracy = [0.35349999737739562]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.8014 - acc: 0.3646 - val_loss: 1.8278 - val_acc: 0.3530\n",
      "Current validation accuracy = [0.35299999666213988]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.7848 - acc: 0.3658 - val_loss: 1.8008 - val_acc: 0.3610\n",
      "Current validation accuracy = [0.36099999904632568]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7666 - acc: 0.3690 - val_loss: 1.8429 - val_acc: 0.3410\n",
      "Current validation accuracy = [0.34100000333786012]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7559 - acc: 0.3688 - val_loss: 1.7724 - val_acc: 0.3585\n",
      "Current validation accuracy = [0.35849999785423281]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.7283 - acc: 0.3765 - val_loss: 1.7634 - val_acc: 0.3590\n",
      "Current validation accuracy = [0.35899999666213989]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7362 - acc: 0.3760 - val_loss: 1.7657 - val_acc: 0.3540\n",
      "Current validation accuracy = [0.35399999690055844]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.7125 - acc: 0.3811 - val_loss: 1.7013 - val_acc: 0.3730\n",
      "Current validation accuracy = [0.37299999713897702]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.6646 - acc: 0.3902 - val_loss: 1.6765 - val_acc: 0.3765\n",
      "Current validation accuracy = [0.37649999785423277]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.6397 - acc: 0.3967 - val_loss: 1.6472 - val_acc: 0.3820\n",
      "Current validation accuracy = [0.3820000023841858]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.6235 - acc: 0.3999 - val_loss: 1.6408 - val_acc: 0.3945\n",
      "Current validation accuracy = [0.39450000000000002]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.6018 - acc: 0.4021 - val_loss: 1.6262 - val_acc: 0.3930\n",
      "Current validation accuracy = [0.39299999713897704]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.6044 - acc: 0.4074 - val_loss: 1.6926 - val_acc: 0.3775\n",
      "Current validation accuracy = [0.3775]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.5871 - acc: 0.4133 - val_loss: 1.6209 - val_acc: 0.3925\n",
      "Current validation accuracy = [0.39250000214576719]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.5477 - acc: 0.4226 - val_loss: 1.5862 - val_acc: 0.4200\n",
      "Current validation accuracy = [0.42000000309944152]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.5223 - acc: 0.4312 - val_loss: 1.7211 - val_acc: 0.3820\n",
      "Current validation accuracy = [0.38200000333786011]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.5479 - acc: 0.4262 - val_loss: 1.5753 - val_acc: 0.4075\n",
      "Current validation accuracy = [0.40749999713897705]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 1.5042 - acc: 0.4403 - val_loss: 1.5841 - val_acc: 0.4045\n",
      "Current validation accuracy = [0.40449999761581423]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.4886 - acc: 0.4430 - val_loss: 1.5262 - val_acc: 0.4255\n",
      "Current validation accuracy = [0.42550000143051148]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.4717 - acc: 0.4505 - val_loss: 1.5498 - val_acc: 0.4145\n",
      "Current validation accuracy = [0.41449999856948855]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.4673 - acc: 0.4462 - val_loss: 1.4976 - val_acc: 0.4400\n",
      "Current validation accuracy = [0.44000000190734861]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.4476 - acc: 0.4561 - val_loss: 1.4727 - val_acc: 0.4450\n",
      "Current validation accuracy = [0.44499999666213991]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.4271 - acc: 0.4670 - val_loss: 1.4679 - val_acc: 0.4430\n",
      "Current validation accuracy = [0.44299999785423277]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.4193 - acc: 0.4679 - val_loss: 1.4574 - val_acc: 0.4475\n",
      "Current validation accuracy = [0.44750000000000001]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.3991 - acc: 0.4744 - val_loss: 1.4553 - val_acc: 0.4465\n",
      "Current validation accuracy = [0.4465000014305115]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 31\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.3895 - acc: 0.4789 - val_loss: 1.4599 - val_acc: 0.4455\n",
      "Current validation accuracy = [0.44550000071525575]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 32\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.4017 - acc: 0.4729 - val_loss: 1.4458 - val_acc: 0.4630\n",
      "Current validation accuracy = [0.46299999713897705]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 33\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.3912 - acc: 0.4805 - val_loss: 1.4496 - val_acc: 0.4395\n",
      "Current validation accuracy = [0.43950000309944154]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 34\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 6s - loss: 1.3620 - acc: 0.4946 - val_loss: 1.4733 - val_acc: 0.4345\n",
      "Current validation accuracy = [0.43449999737739564]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 35\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.3580 - acc: 0.4929 - val_loss: 1.4098 - val_acc: 0.4610\n",
      "Current validation accuracy = [0.46099999713897705]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 36\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.3419 - acc: 0.4993 - val_loss: 1.4028 - val_acc: 0.4785\n",
      "Current validation accuracy = [0.47850000214576721]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 37\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.3412 - acc: 0.4986 - val_loss: 1.4091 - val_acc: 0.4785\n",
      "Current validation accuracy = [0.47849999690055844]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 38\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.3278 - acc: 0.5023 - val_loss: 1.3839 - val_acc: 0.4850\n",
      "Current validation accuracy = [0.48500000143051147]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 39\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 4s - loss: 1.3184 - acc: 0.5079 - val_loss: 1.4641 - val_acc: 0.4350\n",
      "Current validation accuracy = [0.43499999904632569]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 40\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.3214 - acc: 0.5070 - val_loss: 1.3885 - val_acc: 0.4770\n",
      "Current validation accuracy = [0.47699999761581419]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 41\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2925 - acc: 0.5187 - val_loss: 1.3756 - val_acc: 0.4945\n",
      "Current validation accuracy = [0.49450000023841856]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 42\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2917 - acc: 0.5201 - val_loss: 1.4000 - val_acc: 0.4635\n",
      "Current validation accuracy = [0.46350000166893007]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 43\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2867 - acc: 0.5226 - val_loss: 1.3852 - val_acc: 0.4690\n",
      "Current validation accuracy = [0.46900000047683715]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 44\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2791 - acc: 0.5286 - val_loss: 1.3635 - val_acc: 0.4870\n",
      "Current validation accuracy = [0.48700000023841861]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 45\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2751 - acc: 0.5279 - val_loss: 1.3719 - val_acc: 0.4750\n",
      "Current validation accuracy = [0.47499999976158142]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 46\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2638 - acc: 0.5326 - val_loss: 1.3718 - val_acc: 0.4805\n",
      "Current validation accuracy = [0.48050000095367429]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 47\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2700 - acc: 0.5288 - val_loss: 1.3826 - val_acc: 0.4690\n",
      "Current validation accuracy = [0.4690000021457672]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 48\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2477 - acc: 0.5385 - val_loss: 1.3579 - val_acc: 0.4885\n",
      "Current validation accuracy = [0.48850000190734866]\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 49\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 5s - loss: 1.2344 - acc: 0.5434 - val_loss: 1.3476 - val_acc: 0.4900\n",
      "Current validation accuracy = [0.48999999737739564]\n"
     ]
    }
   ],
   "source": [
    "questions, expected = generate_data(TRAINING_SIZE, 3, 4)\n",
    "X_train, y_train, X_val, y_val = create_train_valid(questions, expected, 3, 4)\n",
    "model = build_model(HIDDEN_SIZE, LAYERS, 3, 4)\n",
    "val_acc_3_4 = learning(model, X_train, y_train, 50, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff1f8e7a410>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBSncRURckMJdRMQF/T97uN5VJOsPqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1f8fa8350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.figure()\n",
    "# plt.plot(fb_val_acc_2_2, 'y')\n",
    "# plt.plot(val_acc_2_2)\n",
    "# plt.plot(val_acc_3_2, 'r')\n",
    "# plt.plot(val_acc_2_3, 'g')\n",
    "plt.plot(val_acc_3_3, 'k')\n",
    "plt.plot(fb_val_acc_3_3, 'g')\n",
    "# plt.plot(val_acc_3_4, 'c')\n",
    "# plt.legend(['2 digits 2 ops', '3 digits 2 ops', '2 digits 3 ops', '3 digits 3 ops', '3 digits 4 ops'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analysis of the learned encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapper_f(X):\n",
    "    # The [0] is to disable the training phase flag\n",
    "    return mapper_f_([0] + [X])\n",
    "\n",
    "def decoder_f(X):\n",
    "    # The [0] is to disable the training phase flag\n",
    "    return decoder_f_([0] + [X])\n",
    "\n",
    "def encoder_f(X):\n",
    "    # The [0] is to disable the training phase flag\n",
    "    return encoder_f_([0] + [X])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72+18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 23'"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_str = '81+27'\n",
    "X_str = X_str[::-1]\n",
    "print(X_str)\n",
    "X = ctable.encode(X_str, maxlen=MAXLEN).reshape([1,5,12])\n",
    "# preds = model.predict_classes(X, verbose=0)\n",
    "preds = model.predict(X, verbose=0)\n",
    "# answer = ctable.decode(preds[0], calc_argmax=False)\n",
    "# print(answer[::-1])\n",
    "# X = ctable.encode(X_str).reshape([1,5,12])\n",
    "# W, b = mapper.get_weights()\n",
    "# mapper_f(X)\n",
    "# H = decoder_f(X)\n",
    "X = preds[0].argmax(axis=-1)\n",
    "''.join(ctable.indices_char[x] for x in X)# ctable.indices_char[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array(K.eval(model.layers[0].W_i)))\n",
    "a = np.tanh\n",
    "a(np.array([0.1, 0.2]))\n",
    "model.layers[0].output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMVisualizer:\n",
    "    # LSTM weights  \n",
    "    \n",
    "    def __sigmoid__(self, x):\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "        \n",
    "    def __init__(self, LSTM):\n",
    "        # input gate\n",
    "        self.W_i = np.array(K.eval(LSTM.W_i))\n",
    "        self.U_i = np.array(K.eval(LSTM.U_i))\n",
    "        self.b_i = np.transpose(np.array(K.eval(LSTM.b_i)))\n",
    "\n",
    "        # forget gate\n",
    "        self.W_f = np.array(K.eval(LSTM.W_f))\n",
    "        self.U_f = np.array(K.eval(LSTM.U_f))\n",
    "        self.b_f = np.transpose(np.array(K.eval(LSTM.b_f)))\n",
    "        \n",
    "        # cell\n",
    "        self.W_c = np.array(K.eval(LSTM.W_c))\n",
    "        self.U_c = np.array(K.eval(LSTM.U_c))\n",
    "        self.b_c = np.transpose(np.array(K.eval(LSTM.b_c)))\n",
    "        \n",
    "        # output gate\n",
    "        self.W_o = np.array(K.eval(LSTM.W_o))\n",
    "        self.U_o = np.array(K.eval(LSTM.U_o))\n",
    "        self.b_o = np.transpose(np.array(K.eval(LSTM.b_o)))\n",
    "        \n",
    "        # activation functions\n",
    "        self.activation = np.tanh # LSTM.activation\n",
    "        self.inner_activation = self.__sigmoid__ # LSTM.inner_activation\n",
    "        \n",
    "        self.dim = LSTM.output_dim\n",
    "\n",
    "    def compute(self, X):\n",
    "        # Initialize h\n",
    "        H = [np.zeros((self.dim,))]\n",
    "        C = [np.zeros((self.dim,))]\n",
    "        I = []\n",
    "        F = []\n",
    "        O = []\n",
    "        \n",
    "        for x in X:\n",
    "            # Get previous hidden state\n",
    "            print('x shape=', x.shape)\n",
    "            h_tml = H[-1]\n",
    "            print(h_tml.shape)\n",
    "            c_tml = C[-1]\n",
    "            \n",
    "            x_i = np.inner(np.transpose(self.W_i), x) + self.b_i\n",
    "            print('x_i.shape=', x_i.shape)\n",
    "            x_f = np.inner(np.transpose(self.W_f), x) + self.b_f\n",
    "            x_c = np.inner(np.transpose(self.W_c), x) + self.b_c\n",
    "            x_o = np.inner(np.transpose(self.W_o), x) + self.b_o\n",
    "\n",
    "            i = self.inner_activation(x_i + np.inner(np.transpose(self.U_i), H[-1]))\n",
    "            print('i.shape=', i.shape)\n",
    "            f = self.inner_activation(x_f + np.inner(np.transpose(self.U_f), H[-1]))\n",
    "            print(f.shape)\n",
    "            c_new = self.activation(x_c + np.inner(np.transpose(self.U_c), H[-1]))\n",
    "            c = np.multiply(f, c_tml) + np.multiply(i, c_new)\n",
    "            print(c.shape)\n",
    "            o = self.inner_activation(x_o + np.inner(np.transpose(self.U_o), H[-1]))\n",
    "            print(o.shape)\n",
    "            h = np.multiply(o, self.activation(c))\n",
    "            print(h.shape)\n",
    "            \n",
    "            I.append(i)\n",
    "            F.append(f)\n",
    "            C.append(c)\n",
    "            O.append(o)\n",
    "            H.append(h)\n",
    "        \n",
    "        return I, F, C, O, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(W[:,4])\n",
    "np.argmax(W[:,4]), np.max(W[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34+59\n",
      "78 \n",
      "37+56\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "X1_str = '95+43'\n",
    "X1_str = X1_str[::-1]\n",
    "print(X1_str)\n",
    "X1 = ctable.encode(X1_str, maxlen=MAXLEN).reshape([1,5,12])\n",
    "preds1 = model.predict_classes(X1, verbose=0)\n",
    "answer1 = ctable.decode(preds1[0], calc_argmax=False)\n",
    "print(answer1[::-1])\n",
    "\n",
    "X2_str = '65+73'\n",
    "X2_str = X2_str[::-1]\n",
    "print(X2_str)\n",
    "X2 = ctable.encode(X2_str, maxlen=MAXLEN).reshape([1,5,12])\n",
    "preds2 = model.predict_classes(X2, verbose=0)\n",
    "answer2 = ctable.decode(preds2[0], calc_argmax=False)\n",
    "print(answer2[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "x shape= (12,)\n",
      "(384,)\n",
      "x_i.shape= (384,)\n",
      "i.shape= (384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "viz = LSTMVisualizer(model.layers[0])\n",
    "np.transpose(viz.W_i).shape\n",
    "I1, F1, C1, O1, H1 = viz.compute(X1[0])\n",
    "I2, F2, C2, O2, H2 = viz.compute(X2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H11 = encoder_f(X1)\n",
    "H22 = encoder_f(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02621288, -0.41299691,  0.01349065,  0.17890247, -0.28438289,\n",
       "        0.00558295, -0.06101805, -0.04258914,  0.05178099,  0.20801063])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H11[0][0][:10]-H22[0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01842767, -0.46913334,  0.01314212,  0.14688788, -0.30514536,\n",
       "       -0.01535442, -0.07694036, -0.05140045,  0.075449  ,  0.24241088])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1[5][:10]-H2[5][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.79272263407823362, -0.37972572291801598)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H11[0][0][1], H22[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTM' object has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-65745f86e367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTM' object has no attribute 'inputs'"
     ]
    }
   ],
   "source": [
    "decoder_f = K.function(decoder.inputs, [decoder.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 0.75900405282036232)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = decoder_f(X)\n",
    "H[0].shape\n",
    "np.argmax(H[0][0,0,:]), np.max(H[0][0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789+ \n",
      "[-4.3513234791601132, -4.9431112475557448, -4.6511908840946461, -8.9641301699517157, -4.4348598222546585, -1.963126190168381, 2.9753856653092701, 8.7274458271487507, 11.555584048872351, 7.987633832776071, 2.0469878540829969, -3.7234760182555617]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'66 '"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chars)\n",
    "print([ np.inner(H[0][0,0,:], W[:,idx]) for idx in range(12) ])\n",
    "preds = model.predict_classes(X, verbose=0)\n",
    "ctable.decode(preds[0], calc_argmax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06709065,  0.40383556,  0.16411367, -0.74047542, -0.35855955,\n",
       "       -0.59028941, -0.26894078, -0.66406655, -0.37315661,  0.09656011,\n",
       "       -0.22731853, -0.56127489, -0.49666747,  0.3873997 ,  0.32721272,\n",
       "        0.74144036,  0.75600386,  0.50949365,  0.44447297, -0.56334132,\n",
       "        0.67948729,  0.22404099, -0.18213749,  0.27260011, -0.35370538,\n",
       "       -0.6508016 ,  0.5779109 , -0.21924061,  0.05315667, -0.39903998,\n",
       "        0.39056793, -0.        ,  0.15501858, -0.62290728, -0.67472887,\n",
       "        0.21198085,  0.12123933,  0.52974063, -0.65263051,  0.6788249 ,\n",
       "        0.47473016, -0.53419101, -0.0291235 ,  0.74110025,  0.        ,\n",
       "        0.75623643, -0.64309251,  0.39502251,  0.61232328, -0.62432057,\n",
       "       -0.11690293, -0.01695574,  0.50563914, -0.21612525, -0.32952598,\n",
       "        0.5745647 ,  0.18968037,  0.7037279 , -0.69483364, -0.62477803,\n",
       "       -0.57743329, -0.42290473,  0.61531985,  0.26257876, -0.5781849 ,\n",
       "        0.68098319, -0.73864007, -0.27567244, -0.7042762 ,  0.72349125,\n",
       "        0.61089551, -0.03887191, -0.74912137, -0.67729241, -0.7189979 ,\n",
       "        0.45572764, -0.47757691, -0.43260944,  0.63883197,  0.00961242,\n",
       "       -0.35464063,  0.32595715, -0.75157613,  0.07013302, -0.1147771 ,\n",
       "       -0.01545928, -0.06468385,  0.48737547, -0.49748945, -0.71463346,\n",
       "       -0.75395423,  0.29033199,  0.58166301, -0.40639684,  0.47529167,\n",
       "       -0.6166876 , -0.35756159,  0.29596832, -0.68779182,  0.08947203,\n",
       "       -0.42900568,  0.66031361, -0.39926156,  0.45977357, -0.60209894,\n",
       "        0.18464854, -0.19791   ,  0.75690162, -0.5247376 , -0.50292712,\n",
       "       -0.24802937,  0.35192591,  0.66844153,  0.41236135,  0.72213209,\n",
       "        0.03156203, -0.42111576,  0.30703384,  0.38017774,  0.75751692,\n",
       "       -0.26518655, -0.13845894, -0.74450171,  0.29961479,  0.47395283,\n",
       "       -0.30834025, -0.02306511, -0.30869997], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H[0][0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06709065,  0.40383556,  0.16411367, -0.74047542, -0.35855955,\n",
       "        -0.59028941, -0.26894078, -0.66406655, -0.37315661,  0.09656011,\n",
       "        -0.22731853, -0.56127489, -0.49666747,  0.3873997 ,  0.32721272,\n",
       "         0.74144036,  0.75600386,  0.50949365,  0.44447297, -0.56334132,\n",
       "         0.67948729,  0.22404099, -0.18213749,  0.27260011, -0.35370538,\n",
       "        -0.6508016 ,  0.5779109 , -0.21924061,  0.05315667, -0.39903998,\n",
       "         0.39056793, -0.        ,  0.15501858, -0.62290728, -0.67472887,\n",
       "         0.21198085,  0.12123933,  0.52974063, -0.65263051,  0.6788249 ,\n",
       "         0.47473016, -0.53419101, -0.0291235 ,  0.74110025,  0.        ,\n",
       "         0.75623643, -0.64309251,  0.39502251,  0.61232328, -0.62432057,\n",
       "        -0.11690293, -0.01695574,  0.50563914, -0.21612525, -0.32952598,\n",
       "         0.5745647 ,  0.18968037,  0.7037279 , -0.69483364, -0.62477803,\n",
       "        -0.57743329, -0.42290473,  0.61531985,  0.26257876, -0.5781849 ,\n",
       "         0.68098319, -0.73864007, -0.27567244, -0.7042762 ,  0.72349125,\n",
       "         0.61089551, -0.03887191, -0.74912137, -0.67729241, -0.7189979 ,\n",
       "         0.45572764, -0.47757691, -0.43260944,  0.63883197,  0.00961242,\n",
       "        -0.35464063,  0.32595715, -0.75157613,  0.07013302, -0.1147771 ,\n",
       "        -0.01545928, -0.06468385,  0.48737547, -0.49748945, -0.71463346,\n",
       "        -0.75395423,  0.29033199,  0.58166301, -0.40639684,  0.47529167,\n",
       "        -0.6166876 , -0.35756159,  0.29596832, -0.68779182,  0.08947203,\n",
       "        -0.42900568,  0.66031361, -0.39926156,  0.45977357, -0.60209894,\n",
       "         0.18464854, -0.19791   ,  0.75690162, -0.5247376 , -0.50292712,\n",
       "        -0.24802937,  0.35192591,  0.66844153,  0.41236135,  0.72213209,\n",
       "         0.03156203, -0.42111576,  0.30703384,  0.38017774,  0.75751692,\n",
       "        -0.26518655, -0.13845894, -0.74450171,  0.29961479,  0.47395283,\n",
       "        -0.30834025, -0.02306511, -0.30869997],\n",
       "       [-0.69994533,  0.76321411,  0.15535766, -0.65344214, -0.88647878,\n",
       "         0.25560793, -0.81919712, -0.45533341,  0.27077356,  0.29419547,\n",
       "        -0.02144295, -0.84962237, -0.91721439,  0.44998246,  0.        ,\n",
       "         0.        ,  0.82862532,  0.64688653,  0.73954397, -0.91859907,\n",
       "         0.87968051,  0.29914984, -0.07507572,  0.77487564, -0.62631953,\n",
       "        -0.81438786, -0.02533447, -0.08969957,  0.77608752, -0.73654878,\n",
       "         0.2907638 , -0.67405748,  0.44373825, -0.88843524,  0.03123279,\n",
       "        -0.52790016, -0.6611138 ,  0.80778039, -0.75994819,  0.91814965,\n",
       "         0.76106578, -0.51561689,  0.15538892,  0.93557227, -0.51685983,\n",
       "         0.90239698, -0.94287002,  0.36629105,  0.53293186, -0.94712871,\n",
       "         0.73345929,  0.0893545 ,  0.26360336, -0.76727957,  0.3423062 ,\n",
       "         0.1201067 ,  0.798612  ,  0.45061913, -0.41951275, -0.53485352,\n",
       "         0.36591163,  0.05451221,  0.93636763, -0.65462619, -0.68913627,\n",
       "         0.66622269, -0.95830816, -0.65435702, -0.10083887,  0.89494902,\n",
       "         0.93562615,  0.6598193 , -0.92536646,  0.11554859, -0.23388234,\n",
       "         0.67674857, -0.41253474, -0.70742184,  0.87375456, -0.4631013 ,\n",
       "        -0.08554054,  0.00632936, -0.88636178,  0.44608563, -0.34360796,\n",
       "         0.67244905,  0.74465764,  0.41988492, -0.80310714, -0.96309108,\n",
       "        -0.70842063,  0.17630763,  0.49206769, -0.77916503,  0.60389054,\n",
       "        -0.93163276, -0.711384  , -0.08215633, -0.35217124,  0.554766  ,\n",
       "        -0.42379117,  0.9456225 , -0.55936402,  0.86239368, -0.86113065,\n",
       "        -0.20461668, -0.21869616,  0.91854304, -0.25319299, -0.2395523 ,\n",
       "         0.59244412,  0.80410171,  0.76140451,  0.86761189,  0.78436691,\n",
       "        -0.18879384, -0.34042352,  0.31742704,  0.04000176,  0.85967487,\n",
       "        -0.42730716, -0.43447372, -0.96116805,  0.83471322,  0.94954866,\n",
       "        -0.10928034, -0.7506845 , -0.69407344]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H[0][0,:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD/CAYAAADytG0IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noQO+iEiJ+H/EPAjBHe72tQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f637ed5c2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "pl.imshow(np.transpose(H[0][0,:3,:]), interpolation='nearest', cmap=cm.binary, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD/CAYAAADhYy38AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKeH/AzGoO2LPe7+wAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6396bc4090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.imshow(W, interpolation='nearest', cmap=cm.binary, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 0.56187469)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(H[0][0,0,:] * W[:,4]), np.max(H[0][0,0,:] * W[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 0.62769294)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(H[0][0,1,:] * W[:,4]), np.max(H[0][0,1,:] * W[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.67472887,  0.03123279,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H[0][0,:3,34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1932454 , -0.17046466, -0.33151487, -0.28610313, -0.10930867,\n",
       "       -0.22562845, -0.23864335, -0.23633261,  0.64464188,  0.514768  ,\n",
       "        0.32287183,  0.23024265], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[34,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 0.65876389)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(H[0][0,2,:] * W[:,4]), np.max(H[0][0,2,:] * W[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, -0.45415828)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmin(H[0][0,2,:] * W[:,4]), np.min(H[0][0,2,:] * W[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, -0.42374974)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmin(H[0][0,1,:] * W[:,4]), np.min(H[0][0,1,:] * W[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45415828, -0.32855698, -0.29704189, -0.2528812 , -0.24726333,\n",
       "       -0.22769974, -0.22538853, -0.22083418, -0.20432733, -0.19023129,\n",
       "       -0.17320378, -0.1695088 , -0.16207343, -0.15740235, -0.14236188,\n",
       "       -0.14213942, -0.13477033, -0.12927851, -0.12831168, -0.11970972,\n",
       "       -0.10363259, -0.1032828 , -0.09262865, -0.08448901, -0.07727309,\n",
       "       -0.07494878, -0.0717946 , -0.06999763, -0.06153421, -0.05854693,\n",
       "       -0.04606926, -0.04022109, -0.03889963, -0.03877258, -0.03622344,\n",
       "       -0.03408409, -0.03324113, -0.0290855 , -0.02835351, -0.02560154,\n",
       "       -0.02457534, -0.02231443, -0.01837168, -0.01154075, -0.00861657,\n",
       "       -0.00798268, -0.00578552, -0.00291207, -0.00072087, -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.0006604 ,  0.00235866,  0.00646312,  0.01712832,\n",
       "        0.02227856,  0.02414711,  0.02534644,  0.02723772,  0.02817344,\n",
       "        0.02962844,  0.04378298,  0.04479284,  0.04778089,  0.05259071,\n",
       "        0.05498044,  0.06114025,  0.06737013,  0.07038768,  0.07419734,\n",
       "        0.07490229,  0.07688582,  0.07917839,  0.08645156,  0.08703602,\n",
       "        0.08862983,  0.09577078,  0.0999976 ,  0.10052644,  0.12584311,\n",
       "        0.12872617,  0.13169807,  0.13340101,  0.13913572,  0.15334426,\n",
       "        0.15537268,  0.15956056,  0.16541971,  0.16779615,  0.17974967,\n",
       "        0.1923687 ,  0.19505765,  0.2036147 ,  0.21206295,  0.22455968,\n",
       "        0.22504662,  0.23316552,  0.24413237,  0.25332928,  0.2596457 ,\n",
       "        0.28483191,  0.29851249,  0.30852795,  0.31414506,  0.31818533,\n",
       "        0.34294248,  0.34718034,  0.34857944,  0.35602358,  0.37379876,\n",
       "        0.37608159,  0.38508138,  0.38974813,  0.39278048,  0.43129802,\n",
       "        0.44780952,  0.46707478,  0.48255435,  0.49487916,  0.56500721,\n",
       "        0.57615417,  0.61665791,  0.65876389], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(H[0][0,2,:]*W[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.07066858e-01,  -2.36525223e-01,  -1.86790660e-01,\n",
       "        -1.79153278e-01,  -9.95083377e-02,  -8.52999687e-02,\n",
       "        -7.92786181e-02,  -5.84232956e-02,  -5.36007360e-02,\n",
       "        -3.76082137e-02,  -3.74200717e-02,  -2.97356788e-02,\n",
       "        -2.87593957e-02,  -2.67301910e-02,  -2.66843513e-02,\n",
       "        -1.54926321e-02,  -1.34472745e-02,  -1.23247402e-02,\n",
       "        -1.15500055e-02,  -1.06033040e-02,  -9.94192995e-03,\n",
       "        -9.92942695e-03,  -5.97132975e-03,  -5.30619640e-03,\n",
       "        -2.57752021e-03,  -1.43820746e-03,  -0.00000000e+00,\n",
       "        -0.00000000e+00,  -0.00000000e+00,  -0.00000000e+00,\n",
       "         0.00000000e+00,  -0.00000000e+00,   0.00000000e+00,\n",
       "         3.32088035e-04,   1.00557832e-03,   1.51619432e-03,\n",
       "         4.01117187e-03,   5.09049511e-03,   6.57067867e-03,\n",
       "         8.09591822e-03,   8.31112824e-03,   1.21207628e-02,\n",
       "         1.25999600e-02,   1.91498622e-02,   2.13483125e-02,\n",
       "         2.26804074e-02,   2.47206800e-02,   2.87241414e-02,\n",
       "         3.08485162e-02,   3.17067653e-02,   3.38971913e-02,\n",
       "         3.80584821e-02,   4.31777164e-02,   4.38277684e-02,\n",
       "         4.49320376e-02,   4.56416681e-02,   4.57284153e-02,\n",
       "         4.57441099e-02,   4.62947823e-02,   4.70370874e-02,\n",
       "         4.82262224e-02,   4.92709652e-02,   5.43154478e-02,\n",
       "         5.52334860e-02,   5.61399423e-02,   6.85058013e-02,\n",
       "         6.92265183e-02,   7.22152144e-02,   7.59950355e-02,\n",
       "         8.21665227e-02,   9.25646722e-02,   1.00123212e-01,\n",
       "         1.06100351e-01,   1.14388764e-01,   1.14692368e-01,\n",
       "         1.15460992e-01,   1.16938144e-01,   1.19890802e-01,\n",
       "         1.21548578e-01,   1.31272495e-01,   1.31615043e-01,\n",
       "         1.34969592e-01,   1.35133177e-01,   1.36581033e-01,\n",
       "         1.36625081e-01,   1.43077791e-01,   1.50115177e-01,\n",
       "         1.53605819e-01,   1.54143378e-01,   1.57359287e-01,\n",
       "         1.62541181e-01,   1.65562198e-01,   1.81113452e-01,\n",
       "         1.89826876e-01,   2.07039639e-01,   2.11493328e-01,\n",
       "         2.27072164e-01,   2.29275346e-01,   2.31581777e-01,\n",
       "         2.50185251e-01,   2.56177783e-01,   2.58738548e-01,\n",
       "         2.61075437e-01,   2.69964963e-01,   2.90376782e-01,\n",
       "         2.93340057e-01,   3.02351445e-01,   3.05366158e-01,\n",
       "         3.07923168e-01,   3.09660345e-01,   3.12246263e-01,\n",
       "         3.22186142e-01,   3.26824278e-01,   3.34542394e-01,\n",
       "         3.48860413e-01,   3.52766722e-01,   3.59418362e-01,\n",
       "         3.67470324e-01,   3.87315333e-01,   3.89949232e-01,\n",
       "         4.02509570e-01,   4.03534800e-01,   4.04645622e-01,\n",
       "         4.28490102e-01,   4.46417272e-01,   4.53108191e-01,\n",
       "         4.60138053e-01,   5.74842691e-01], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(H[0][0,2,:]*W[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5335024 , -0.51823604, -0.50100958, -0.48577577, -0.47957054,\n",
       "       -0.44241169, -0.37946013, -0.36722139, -0.35580841, -0.33394715,\n",
       "       -0.23222774, -0.22796875, -0.2009044 , -0.17646785, -0.17416285,\n",
       "       -0.17232595, -0.16369027, -0.16364257, -0.10973285, -0.10573573,\n",
       "       -0.1032261 , -0.08143459, -0.0812537 , -0.07862101, -0.07631801,\n",
       "       -0.07576364, -0.06466574, -0.06453831, -0.06009501, -0.05024419,\n",
       "       -0.04648986, -0.04348138, -0.04183068, -0.03520668, -0.03504439,\n",
       "       -0.02873553, -0.02783925, -0.02733363, -0.02595378, -0.01380991,\n",
       "       -0.01290411, -0.01269832, -0.01256789, -0.00633566, -0.00606702,\n",
       "       -0.00603559, -0.00495656, -0.00493447, -0.0049248 , -0.00277691,\n",
       "        0.        , -0.        ,  0.00060533,  0.00067053,  0.00182039,\n",
       "        0.00437887,  0.00475972,  0.01304994,  0.01382295,  0.01396575,\n",
       "        0.01528975,  0.01548589,  0.01594696,  0.0162168 ,  0.01740837,\n",
       "        0.01888322,  0.01985673,  0.02293642,  0.02334398,  0.029086  ,\n",
       "        0.029867  ,  0.03536496,  0.03579383,  0.03616775,  0.03943265,\n",
       "        0.0406722 ,  0.04438293,  0.04600561,  0.04622176,  0.04791395,\n",
       "        0.04925878,  0.04982607,  0.05837914,  0.06185332,  0.06425659,\n",
       "        0.06440832,  0.06462984,  0.06534637,  0.07266825,  0.07418796,\n",
       "        0.07787652,  0.0859278 ,  0.086803  ,  0.09313124,  0.09348384,\n",
       "        0.09621865,  0.10355424,  0.10773021,  0.11408503,  0.11869032,\n",
       "        0.12508096,  0.12934865,  0.13050248,  0.13083667,  0.13169053,\n",
       "        0.14707059,  0.15002634,  0.15020089,  0.15054029,  0.1561247 ,\n",
       "        0.16261391,  0.16920441,  0.16961174,  0.17333853,  0.18566015,\n",
       "        0.18759561,  0.19882663,  0.20133199,  0.21040641,  0.21242753,\n",
       "        0.22286218,  0.23713812,  0.23777719,  0.24145626,  0.2661728 ,\n",
       "        0.27632549,  0.28099507,  0.39949182], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(H[0][0,1,:]*W[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45415828, -0.32855698, -0.29704189, -0.2528812 , -0.24726333,\n",
       "       -0.22769974, -0.22538853, -0.22083418, -0.20432733, -0.19023129,\n",
       "       -0.17320378, -0.1695088 , -0.16207343, -0.15740235, -0.14236188,\n",
       "       -0.14213942, -0.13477033, -0.12927851, -0.12831168, -0.11970972,\n",
       "       -0.10363259, -0.1032828 , -0.09262865, -0.08448901, -0.07727309,\n",
       "       -0.07494878, -0.0717946 , -0.06999763, -0.06153421, -0.05854693,\n",
       "       -0.04606926, -0.04022109, -0.03889963, -0.03877258, -0.03622344,\n",
       "       -0.03408409, -0.03324113, -0.0290855 , -0.02835351, -0.02560154,\n",
       "       -0.02457534, -0.02231443, -0.01837168, -0.01154075, -0.00861657,\n",
       "       -0.00798268, -0.00578552, -0.00291207, -0.00072087, -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.0006604 ,  0.00235866,  0.00646312,  0.01712832,\n",
       "        0.02227856,  0.02414711,  0.02534644,  0.02723772,  0.02817344,\n",
       "        0.02962844,  0.04378298,  0.04479284,  0.04778089,  0.05259071,\n",
       "        0.05498044,  0.06114025,  0.06737013,  0.07038768,  0.07419734,\n",
       "        0.07490229,  0.07688582,  0.07917839,  0.08645156,  0.08703602,\n",
       "        0.08862983,  0.09577078,  0.0999976 ,  0.10052644,  0.12584311,\n",
       "        0.12872617,  0.13169807,  0.13340101,  0.13913572,  0.15334426,\n",
       "        0.15537268,  0.15956056,  0.16541971,  0.16779615,  0.17974967,\n",
       "        0.1923687 ,  0.19505765,  0.2036147 ,  0.21206295,  0.22455968,\n",
       "        0.22504662,  0.23316552,  0.24413237,  0.25332928,  0.2596457 ,\n",
       "        0.28483191,  0.29851249,  0.30852795,  0.31414506,  0.31818533,\n",
       "        0.34294248,  0.34718034,  0.34857944,  0.35602358,  0.37379876,\n",
       "        0.37608159,  0.38508138,  0.38974813,  0.39278048,  0.43129802,\n",
       "        0.44780952,  0.46707478,  0.48255435,  0.49487916,  0.56500721,\n",
       "        0.57615417,  0.61665791,  0.65876389], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(H[0][0,2,:]*W[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 65,  15, 121,  56, 110, 123,  44,  57,  22,  63, 102,  18,  28,\n",
       "        67, 120,  27, 113,  42, 109,  66, 125,  92, 101,  91,  95,  31,\n",
       "        45, 103,  87,  83, 127,  26,  94,  11,  99, 117, 106,  82,  76,\n",
       "        54, 116,  61,  20,  46, 108,  36,  55,   0,  14,  21, 104,  93,\n",
       "        19,  86, 114,  84,  70,  40, 105, 107,  62, 124,  53,  51,  47,\n",
       "        29,  72,  50,  74, 115,   7,  35,   2,  81,   9,  23,  80, 122,\n",
       "        97,  34,  43, 100,  85,  68,  96,  59,  48,   8,  41,  69,  38,\n",
       "        77, 112, 111,  90,  12,   3,  64, 126,   5,  60,  32,   1,  79,\n",
       "         6,  24, 118,  25,  13,  17,   4,  89,  16,  71,  52,  88,  75,\n",
       "        10,  49,  58,  30,  37,  98,  33, 119,  39,  73,  78])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(H[0][0,0,:]*W[:,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 65,  63, 101,  22,  42,  15,  25,  56,   6,  57, 123,  69,  28,\n",
       "        39, 125,   1,  44,  92,  19,  26,  66,  45, 114,  83, 106,  91,\n",
       "        87, 120,  78,  12,  54,  79, 103,  61,  11,  70, 102,  99,  88,\n",
       "        30, 126,  98,  51,  76,  82,  67,  97,  94, 108,  20, 116,  46,\n",
       "        93,  55, 127,  27,  21,  36, 105,  84,  40, 118,  62,  72, 109,\n",
       "        74,  86, 107,  48, 124, 110, 115,  53,  35,   7,   5,  81,   3,\n",
       "         8,  37, 121, 122, 117,  80,  43,  68,  23,  47,  34, 104, 100,\n",
       "        29,  18,   9,  60,  50,  77,   0,  59,  41,  38,  32,  90, 111,\n",
       "        85,  95,  96,   4,  89,  64,   2,  75, 113,  31,  24, 112,  16,\n",
       "        14,  17,  13,  52,  10,  33,  58,  49, 119,  71,  73])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(H[0][0,1,:]*W[:,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 73,  39,  30,  98,  17, 123,   4, 112,   6,  24,  65, 125, 106,\n",
       "       110,  45,  14, 121,  15, 119,   5,  89, 126,  63,  35,  91,  56,\n",
       "        67,  25, 120,  13,  49,  66,  88, 102,  32,  18,  23, 111,  44,\n",
       "        57,   9, 109,  53,  27,  19,  38, 115,  28,  86,  74,  40,  36,\n",
       "       122,  59,  43,  83,  93, 127,  87, 103,  46, 108,  81,  21,  54,\n",
       "        94,  33,  99,  82,  11,  85,  76,  26,  20,  61,  55, 116,  72,\n",
       "       105,  84,  92, 117,  51, 101,  69,  62,  47,  70, 124,  12,  31,\n",
       "       107,  10,  22,  42, 114,  97,   7,  34,   1, 100,  41,  71,  52,\n",
       "         8,  58,   3,  60,  95,  80,  48,  50,   0,  29,  37,  68,   2,\n",
       "        79,  75, 104,  77, 113, 118,  96,  16,  64,  90,  78])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(H[0][0,1,:]*W[:,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dweights = decoder.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dweights[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context = encoder_f(X)\n",
    "context = context[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.71680015, -0.71960968, -0.67189914,  0.35741776, -0.74951118,\n",
       "       -0.35821265,  0.11314015, -0.44468644, -0.32356325,  0.47177044,\n",
       "        0.8712455 , -0.49983853,  0.2256813 ,  0.30286032, -0.7024222 ,\n",
       "       -0.3009218 , -0.2085263 ,  0.48517787, -0.50062698, -0.63945681,\n",
       "        0.41134304, -0.12710692,  0.35288697, -0.32511729,  0.7333858 ,\n",
       "        0.63669533, -0.79113978, -0.47415727, -0.70502675, -0.45729491,\n",
       "        0.94472587, -0.69170535, -0.71485847,  0.53148812, -0.42479914,\n",
       "        0.79443866, -0.76739752, -0.45804244,  0.63051635, -0.32278904,\n",
       "       -0.18736556, -0.5895865 , -0.07616945, -0.70557344,  0.09437619,\n",
       "        0.4516294 , -0.59635127, -0.35728732, -0.60755306, -0.51159179,\n",
       "        0.83871293, -0.85435426, -0.62628043,  0.24694353,  0.37690449,\n",
       "       -0.56536186, -0.29809138, -0.42888081, -0.69122845, -0.32909194,\n",
       "       -0.3422389 ,  0.23144025,  0.79829955, -0.03181538, -0.54385889,\n",
       "        0.8051672 ,  0.22158965,  0.40830263,  0.48554987, -0.18048501,\n",
       "        0.25863943,  0.41948989,  0.60638374,  0.63353252,  0.02183209,\n",
       "        0.48267326, -0.83726984, -0.99353832,  0.05776672,  0.81352484,\n",
       "       -0.48400256,  0.0042755 ,  0.96776056, -0.48480931,  0.70168614,\n",
       "        0.70132828, -0.40280709,  0.51580983, -0.5750528 , -0.38270435,\n",
       "       -0.0639038 , -0.61799997, -0.63614011,  0.81532007, -0.68753004,\n",
       "        0.84851801, -0.53855199, -0.48810041,  0.82378352, -0.72564769,\n",
       "       -0.7464807 , -0.83057159,  0.31444475,  0.91995108, -0.52658582,\n",
       "       -0.60736173,  0.22396296, -0.22781688, -0.68356621,  0.97621125,\n",
       "       -0.81962818,  0.78978717,  0.45830211,  0.52139574, -0.02888704,\n",
       "        0.64774376,  0.40182143,  0.80477822,  0.65618575, -0.49759692,\n",
       "       -0.11176096, -0.27976739, -0.6482625 , -0.33725443, -0.75477457,\n",
       "        0.64396036, -0.4464213 ,  0.4779253 ], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dweights[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  1+0',\n",
       " ' 09+1',\n",
       " '  9+0',\n",
       " ' 27+8',\n",
       " '19+34',\n",
       " '35+92',\n",
       " '  4+1',\n",
       " ' 75+6',\n",
       " '  8+4',\n",
       " '  8+0',\n",
       " '  6+5',\n",
       " '  6+2',\n",
       " ' 55+6',\n",
       " '  5+3',\n",
       " '53+92',\n",
       " '  3+2',\n",
       " ' 38+1',\n",
       " ' 54+2',\n",
       " '38+93',\n",
       " ' 97+6',\n",
       " ' 47+1',\n",
       " '  7+3',\n",
       " '  5+4',\n",
       " ' 06+9',\n",
       " ' 67+8',\n",
       " ' 92+5',\n",
       " '  6+4',\n",
       " ' 22+3',\n",
       " '57+36',\n",
       " '89+03',\n",
       " '67+05',\n",
       " '57+45',\n",
       " '94+43',\n",
       " ' 52+9',\n",
       " ' 22+5',\n",
       " ' 81+2',\n",
       " '66+42',\n",
       " ' 81+6',\n",
       " '69+03',\n",
       " '  2+1',\n",
       " '78+51',\n",
       " ' 65+1',\n",
       " ' 94+8',\n",
       " '09+57',\n",
       " '  5+2',\n",
       " ' 57+2',\n",
       " ' 53+3',\n",
       " ' 74+9',\n",
       " ' 05+0',\n",
       " '69+12',\n",
       " ' 05+9',\n",
       " '98+31',\n",
       " ' 47+6',\n",
       " '86+25',\n",
       " '46+82',\n",
       " ' 26+0',\n",
       " ' 68+6',\n",
       " ' 01+5',\n",
       " '96+62',\n",
       " '  7+5',\n",
       " ' 76+7',\n",
       " ' 72+5',\n",
       " '39+12',\n",
       " '  9+8',\n",
       " '16+41',\n",
       " ' 35+0',\n",
       " '08+97',\n",
       " ' 84+7',\n",
       " ' 23+1',\n",
       " ' 59+2',\n",
       " ' 11+1',\n",
       " ' 21+0',\n",
       " '27+46',\n",
       " '  6+0',\n",
       " '35+44',\n",
       " ' 81+8',\n",
       " ' 95+4',\n",
       " ' 27+5',\n",
       " '47+05',\n",
       " '96+54',\n",
       " ' 72+7',\n",
       " '39+27',\n",
       " '  7+2',\n",
       " ' 58+7',\n",
       " ' 26+2',\n",
       " ' 64+7',\n",
       " '35+52',\n",
       " '  7+4',\n",
       " ' 77+1',\n",
       " '05+91',\n",
       " '76+43',\n",
       " ' 66+7',\n",
       " '36+93',\n",
       " ' 69+4',\n",
       " '45+53',\n",
       " ' 91+8',\n",
       " '  9+4',\n",
       " '49+67',\n",
       " '81+01',\n",
       " ' 08+2',\n",
       " ' 87+1',\n",
       " ' 83+0',\n",
       " '  9+6',\n",
       " '48+96',\n",
       " ' 13+5',\n",
       " ' 68+3',\n",
       " ' 78+4',\n",
       " '19+03',\n",
       " '53+51',\n",
       " ' 54+6',\n",
       " ' 49+6',\n",
       " '44+82',\n",
       " '  9+1',\n",
       " ' 19+0',\n",
       " ' 83+8',\n",
       " ' 12+4',\n",
       " '  7+6',\n",
       " '92+42',\n",
       " '18+82',\n",
       " ' 95+6',\n",
       " ' 66+9',\n",
       " '97+36',\n",
       " ' 61+2',\n",
       " ' 15+7',\n",
       " ' 23+7',\n",
       " '  9+9',\n",
       " ' 28+2',\n",
       " ' 84+5',\n",
       " ' 31+4',\n",
       " '38+45',\n",
       " ' 79+1',\n",
       " ' 39+8',\n",
       " ' 65+6',\n",
       " ' 59+9',\n",
       " ' 03+5',\n",
       " ' 65+2',\n",
       " '  2+0',\n",
       " ' 28+6',\n",
       " ' 75+2',\n",
       " ' 74+6',\n",
       " ' 31+0',\n",
       " '58+51',\n",
       " '  8+2',\n",
       " ' 67+5',\n",
       " '68+44',\n",
       " ' 92+6',\n",
       " '76+36',\n",
       " '  9+2',\n",
       " ' 71+6',\n",
       " '  5+0',\n",
       " ' 81+1',\n",
       " ' 04+4',\n",
       " ' 15+3',\n",
       " '  6+6',\n",
       " '  9+5',\n",
       " ' 68+2',\n",
       " ' 71+2',\n",
       " ' 99+9',\n",
       " ' 12+5',\n",
       " ' 27+9',\n",
       " '  4+2',\n",
       " ' 46+5',\n",
       " ' 16+5',\n",
       " ' 41+6',\n",
       " ' 35+7',\n",
       " '28+17',\n",
       " ' 65+7',\n",
       " ' 72+9',\n",
       " ' 69+0',\n",
       " ' 62+9',\n",
       " '47+53',\n",
       " ' 04+1',\n",
       " '  8+7',\n",
       " '09+55',\n",
       " '  6+1',\n",
       " ' 28+9',\n",
       " ' 25+1',\n",
       " '86+66',\n",
       " ' 21+2',\n",
       " ' 97+7',\n",
       " ' 99+6',\n",
       " ' 63+8',\n",
       " ' 68+4',\n",
       " '39+53',\n",
       " ' 94+6',\n",
       " '66+22',\n",
       " ' 38+5',\n",
       " '28+04',\n",
       " '  3+3',\n",
       " ' 18+8',\n",
       " '56+05',\n",
       " '  4+4',\n",
       " '55+22',\n",
       " ' 83+4',\n",
       " ' 35+9',\n",
       " ' 97+8',\n",
       " ' 52+4',\n",
       " '  9+3',\n",
       " '99+71',\n",
       " ' 34+3',\n",
       " ' 91+0',\n",
       " ' 59+8',\n",
       " '85+74',\n",
       " ' 99+1',\n",
       " ' 09+3',\n",
       " '18+53',\n",
       " '47+01',\n",
       " ' 58+5',\n",
       " ' 11+6',\n",
       " '  1+1',\n",
       " '27+04',\n",
       " ' 08+3',\n",
       " ' 09+6',\n",
       " '29+36',\n",
       " '89+72',\n",
       " '76+71',\n",
       " ' 07+1',\n",
       " ' 63+5',\n",
       " '  8+3',\n",
       " ' 41+0',\n",
       " ' 36+5',\n",
       " '69+98',\n",
       " '17+65',\n",
       " '48+71',\n",
       " ' 46+1',\n",
       " ' 28+7',\n",
       " ' 05+4',\n",
       " ' 61+8',\n",
       " ' 38+4',\n",
       " ' 95+7',\n",
       " ' 55+1',\n",
       " ' 86+7',\n",
       " ' 94+7',\n",
       " '88+97',\n",
       " ' 64+4',\n",
       " ' 96+9',\n",
       " '63+12',\n",
       " ' 73+3',\n",
       " '68+16',\n",
       " ' 21+4',\n",
       " ' 56+1',\n",
       " ' 88+0',\n",
       " '  2+2',\n",
       " '56+46',\n",
       " ' 29+9',\n",
       " ' 16+3',\n",
       " '  4+0',\n",
       " ' 44+5',\n",
       " ' 51+5',\n",
       " ' 15+8',\n",
       " ' 13+3',\n",
       " ' 56+5',\n",
       " '99+08',\n",
       " ' 56+0',\n",
       " '97+72',\n",
       " ' 29+6',\n",
       " '96+92',\n",
       " ' 13+1',\n",
       " ' 13+0',\n",
       " ' 26+1',\n",
       " ' 51+2',\n",
       " '17+62',\n",
       " ' 09+9',\n",
       " ' 35+1',\n",
       " ' 19+4',\n",
       " ' 31+6',\n",
       " ' 41+7',\n",
       " '  3+0',\n",
       " '73+22',\n",
       " ' 19+3',\n",
       " ' 98+5',\n",
       " '38+72',\n",
       " ' 63+3',\n",
       " ' 01+6',\n",
       " ' 86+2',\n",
       " ' 64+3',\n",
       " ' 37+3',\n",
       " '  7+0',\n",
       " ' 91+4',\n",
       " '  7+1',\n",
       " ' 87+0',\n",
       " '  5+5',\n",
       " ' 86+3',\n",
       " ' 41+3',\n",
       " ' 19+7',\n",
       " ' 44+7',\n",
       " ' 06+0',\n",
       " '55+52',\n",
       " '  9+7',\n",
       " ' 81+0',\n",
       " '  3+1',\n",
       " ' 46+8',\n",
       " ' 76+2',\n",
       " ' 74+2',\n",
       " '45+81',\n",
       " ' 49+2',\n",
       " '56+74',\n",
       " ' 92+9',\n",
       " ' 05+2',\n",
       " ' 12+3',\n",
       " ' 61+1',\n",
       " ' 05+1',\n",
       " '74+12',\n",
       " '64+22',\n",
       " '28+25',\n",
       " ' 94+1',\n",
       " '  7+7',\n",
       " '75+74',\n",
       " '59+67',\n",
       " '  4+3',\n",
       " ' 67+6',\n",
       " '27+83',\n",
       " ' 99+7',\n",
       " ' 71+9',\n",
       " '16+72',\n",
       " ' 32+1',\n",
       " '59+52',\n",
       " '89+48',\n",
       " ' 17+5',\n",
       " ' 07+3',\n",
       " ' 45+1',\n",
       " ' 86+5',\n",
       " '25+92',\n",
       " ' 88+9',\n",
       " '  8+1',\n",
       " '08+42',\n",
       " '37+52',\n",
       " ' 98+0',\n",
       " ' 83+9',\n",
       " ' 23+0',\n",
       " ' 55+8',\n",
       " ' 37+9',\n",
       " '  8+5',\n",
       " ' 54+7',\n",
       " ' 98+2',\n",
       " ' 01+8',\n",
       " '28+95',\n",
       " ' 53+2',\n",
       " '78+82',\n",
       " ' 89+3',\n",
       " ' 94+0',\n",
       " '17+75',\n",
       " ' 12+0',\n",
       " '  8+6',\n",
       " ' 62+6',\n",
       " '56+34',\n",
       " ' 21+9',\n",
       " '07+81',\n",
       " ' 83+1',\n",
       " '78+03',\n",
       " '  6+3',\n",
       " '79+51',\n",
       " ' 73+0',\n",
       " '09+06',\n",
       " '28+33',\n",
       " '39+88',\n",
       " ' 68+1',\n",
       " ' 55+5',\n",
       " ' 26+3',\n",
       " ' 48+6',\n",
       " ' 79+9',\n",
       " ' 02+4',\n",
       " ' 74+5',\n",
       " ' 58+6',\n",
       " ' 33+0',\n",
       " ' 88+7',\n",
       " '  0+0',\n",
       " ' 19+1',\n",
       " '38+52',\n",
       " ' 32+5',\n",
       " '49+46',\n",
       " '29+71',\n",
       " ' 15+4',\n",
       " ' 87+4',\n",
       " '  5+1',\n",
       " ' 62+7',\n",
       " ' 89+2',\n",
       " ' 12+2',\n",
       " ' 99+5',\n",
       " ' 02+8',\n",
       " ' 11+7',\n",
       " ' 16+9',\n",
       " ' 54+4',\n",
       " ' 73+7',\n",
       " ' 46+0',\n",
       " ' 18+0',\n",
       " ' 57+7',\n",
       " ' 22+7',\n",
       " ' 76+0',\n",
       " '98+32',\n",
       " '07+85',\n",
       " ' 78+5',\n",
       " '29+46',\n",
       " ' 11+3',\n",
       " ' 72+8',\n",
       " '85+01',\n",
       " '87+14',\n",
       " ' 69+5',\n",
       " '73+53',\n",
       " ' 83+6',\n",
       " '74+62',\n",
       " ' 85+8',\n",
       " ' 38+9',\n",
       " ' 13+9',\n",
       " ' 72+1',\n",
       " ' 22+2',\n",
       " '77+32',\n",
       " '42+51',\n",
       " ' 28+3',\n",
       " ' 84+6',\n",
       " '53+41',\n",
       " '96+13',\n",
       " ' 04+5',\n",
       " '21+21',\n",
       " ' 77+7',\n",
       " '95+53',\n",
       " ' 43+8',\n",
       " '48+43',\n",
       " '78+31',\n",
       " ' 13+2',\n",
       " '72+51',\n",
       " '07+92',\n",
       " ' 45+5',\n",
       " '94+82',\n",
       " '69+22',\n",
       " ' 11+5',\n",
       " ' 36+9',\n",
       " '34+14',\n",
       " ' 03+7',\n",
       " ' 78+2',\n",
       " ' 69+3',\n",
       " ' 66+0',\n",
       " ' 65+0',\n",
       " ' 44+1',\n",
       " ' 49+7',\n",
       " '69+75',\n",
       " ' 34+4',\n",
       " ' 05+8',\n",
       " '68+76',\n",
       " '37+03',\n",
       " ' 33+2',\n",
       " ' 95+9',\n",
       " ' 98+6',\n",
       " '47+92',\n",
       " '88+66',\n",
       " '97+44',\n",
       " ' 54+0',\n",
       " '87+92',\n",
       " ' 48+0',\n",
       " '73+02',\n",
       " '14+02',\n",
       " ' 89+5',\n",
       " ' 07+7',\n",
       " '63+61',\n",
       " ' 26+5',\n",
       " ' 88+5',\n",
       " ' 09+0',\n",
       " ' 13+8',\n",
       " '73+13',\n",
       " '79+61',\n",
       " ' 93+6',\n",
       " '57+63',\n",
       " ' 22+8',\n",
       " ' 82+6',\n",
       " ' 26+7',\n",
       " ' 57+8',\n",
       " ' 44+8',\n",
       " '39+28',\n",
       " '05+94',\n",
       " '87+46',\n",
       " ' 98+8',\n",
       " ' 61+0',\n",
       " '78+71',\n",
       " '69+47',\n",
       " ' 53+7',\n",
       " ' 58+8',\n",
       " ' 22+0',\n",
       " ' 45+0',\n",
       " ' 34+5',\n",
       " '12+51',\n",
       " ' 28+1',\n",
       " '34+51',\n",
       " '69+77',\n",
       " ' 85+3',\n",
       " ' 89+1',\n",
       " ' 45+3',\n",
       " ' 17+9',\n",
       " '79+66',\n",
       " ' 71+5',\n",
       " ' 27+4',\n",
       " ' 95+3',\n",
       " '35+91',\n",
       " ' 07+8',\n",
       " ' 87+5',\n",
       " '67+96',\n",
       " ' 86+8',\n",
       " '68+13',\n",
       " ' 29+5',\n",
       " ' 75+1',\n",
       " ' 76+3',\n",
       " '15+44',\n",
       " '67+14',\n",
       " '16+52',\n",
       " ' 86+4',\n",
       " ' 68+5',\n",
       " '59+44',\n",
       " '78+83',\n",
       " ' 84+2',\n",
       " ' 25+4',\n",
       " '85+33',\n",
       " ' 47+9',\n",
       " '39+77',\n",
       " '88+25',\n",
       " ' 27+3',\n",
       " '48+06',\n",
       " ' 88+8',\n",
       " '68+75',\n",
       " ' 96+0',\n",
       " ' 61+3',\n",
       " ' 18+1',\n",
       " '34+02',\n",
       " '43+71',\n",
       " ' 16+7',\n",
       " ' 33+7',\n",
       " ' 57+0',\n",
       " ' 52+1',\n",
       " '63+72',\n",
       " ' 08+4',\n",
       " ' 66+5',\n",
       " ' 14+6',\n",
       " ' 41+5',\n",
       " ' 19+5',\n",
       " ' 59+1',\n",
       " '74+71',\n",
       " '73+21',\n",
       " ' 85+0',\n",
       " '17+92',\n",
       " ' 67+1',\n",
       " '85+05',\n",
       " ' 78+6',\n",
       " ' 91+5',\n",
       " ' 56+3',\n",
       " ' 46+9',\n",
       " ' 73+9',\n",
       " '34+33',\n",
       " '27+65',\n",
       " '56+15',\n",
       " ' 31+9',\n",
       " '19+47',\n",
       " ' 45+9',\n",
       " ' 05+5',\n",
       " ' 58+4',\n",
       " ' 91+6',\n",
       " ' 11+2',\n",
       " ' 19+6',\n",
       " ' 39+1',\n",
       " ' 39+7',\n",
       " ' 07+4',\n",
       " ' 62+2',\n",
       " '55+14',\n",
       " '15+33',\n",
       " ' 48+3',\n",
       " ' 57+6',\n",
       " ' 47+4',\n",
       " ' 38+7',\n",
       " ' 17+7',\n",
       " ' 61+6',\n",
       " ' 35+6',\n",
       " '79+94',\n",
       " ' 14+7',\n",
       " '76+44',\n",
       " ' 15+1',\n",
       " '97+05',\n",
       " ' 29+7',\n",
       " ' 23+4',\n",
       " '88+67',\n",
       " ' 62+8',\n",
       " '83+63',\n",
       " ' 93+3',\n",
       " '25+34',\n",
       " ' 02+1',\n",
       " '78+87',\n",
       " '69+66',\n",
       " '26+31',\n",
       " ' 68+0',\n",
       " ' 95+1',\n",
       " ' 94+5',\n",
       " ' 41+4',\n",
       " '53+03',\n",
       " ' 36+0',\n",
       " '49+18',\n",
       " ' 61+5',\n",
       " '04+02',\n",
       " ' 92+3',\n",
       " ' 03+3',\n",
       " ' 32+2',\n",
       " '69+57',\n",
       " '16+85',\n",
       " ' 41+8',\n",
       " ' 93+4',\n",
       " '79+91',\n",
       " ' 21+1',\n",
       " ' 78+1',\n",
       " ' 53+1',\n",
       " '76+04',\n",
       " '72+01',\n",
       " '49+64',\n",
       " '67+52',\n",
       " ' 02+5',\n",
       " ' 14+3',\n",
       " ' 87+6',\n",
       " ' 42+9',\n",
       " '79+35',\n",
       " '87+32',\n",
       " '24+21',\n",
       " '19+98',\n",
       " '52+81',\n",
       " '48+17',\n",
       " '85+63',\n",
       " ' 22+4',\n",
       " '59+57',\n",
       " ' 52+3',\n",
       " '86+34',\n",
       " '33+62',\n",
       " ' 41+2',\n",
       " '75+23',\n",
       " ' 71+7',\n",
       " ' 21+5',\n",
       " '17+03',\n",
       " ' 63+2',\n",
       " '39+13',\n",
       " '98+34',\n",
       " '18+45',\n",
       " ' 53+0',\n",
       " ' 56+6',\n",
       " '72+02',\n",
       " ' 91+2',\n",
       " ' 77+4',\n",
       " ' 07+6',\n",
       " '64+14',\n",
       " ' 85+6',\n",
       " '35+25',\n",
       " ' 49+8',\n",
       " ' 43+7',\n",
       " '56+53',\n",
       " ' 44+0',\n",
       " '76+53',\n",
       " '28+11',\n",
       " '39+04',\n",
       " '16+83',\n",
       " '29+85',\n",
       " '23+92',\n",
       " '  8+8',\n",
       " '59+96',\n",
       " ' 73+2',\n",
       " '57+74',\n",
       " ' 47+8',\n",
       " '97+35',\n",
       " ' 22+6',\n",
       " ' 17+1',\n",
       " ' 24+4',\n",
       " '05+74',\n",
       " '16+61',\n",
       " '77+77',\n",
       " '38+41',\n",
       " ' 21+6',\n",
       " ' 16+8',\n",
       " '16+01',\n",
       " ' 63+6',\n",
       " '23+52',\n",
       " ' 76+5',\n",
       " ' 34+1',\n",
       " '92+61',\n",
       " ' 39+2',\n",
       " ' 16+1',\n",
       " '76+33',\n",
       " ' 58+0',\n",
       " ' 86+1',\n",
       " ' 62+4',\n",
       " ' 31+5',\n",
       " ' 02+2',\n",
       " '82+62',\n",
       " '18+16',\n",
       " ' 64+2',\n",
       " ' 88+3',\n",
       " '04+81',\n",
       " '29+13',\n",
       " ' 21+3',\n",
       " '05+92',\n",
       " '07+04',\n",
       " '79+34',\n",
       " ' 04+3',\n",
       " ' 14+5',\n",
       " ' 42+5',\n",
       " '88+85',\n",
       " '94+22',\n",
       " ' 45+8',\n",
       " ' 94+4',\n",
       " '87+42',\n",
       " ' 68+8',\n",
       " ' 29+3',\n",
       " ' 04+0',\n",
       " '54+32',\n",
       " '94+51',\n",
       " '88+22',\n",
       " '79+05',\n",
       " ' 17+4',\n",
       " '57+64',\n",
       " '36+45',\n",
       " ' 98+4',\n",
       " ' 81+7',\n",
       " '79+42',\n",
       " '96+74',\n",
       " ' 09+2',\n",
       " ' 91+9',\n",
       " ' 29+1',\n",
       " '36+14',\n",
       " '17+16',\n",
       " ' 01+1',\n",
       " '68+34',\n",
       " ' 04+8',\n",
       " '55+81',\n",
       " ' 39+0',\n",
       " ' 96+1',\n",
       " ' 54+9',\n",
       " '55+61',\n",
       " ' 22+9',\n",
       " ' 58+2',\n",
       " ' 47+7',\n",
       " ' 42+1',\n",
       " '89+79',\n",
       " '13+13',\n",
       " ' 71+8',\n",
       " ' 81+3',\n",
       " '25+74',\n",
       " ' 89+8',\n",
       " ' 88+2',\n",
       " ' 89+9',\n",
       " '05+84',\n",
       " '35+71',\n",
       " '78+57',\n",
       " ' 75+9',\n",
       " ' 01+3',\n",
       " ' 45+4',\n",
       " ' 66+6',\n",
       " ' 49+9',\n",
       " ' 38+3',\n",
       " ' 09+5',\n",
       " ' 41+1',\n",
       " ' 64+5',\n",
       " '26+04',\n",
       " ' 67+9',\n",
       " ' 36+8',\n",
       " '79+08',\n",
       " ' 65+4',\n",
       " '57+83',\n",
       " ' 81+4',\n",
       " ' 55+3',\n",
       " ' 66+8',\n",
       " ' 79+0',\n",
       " ' 55+2',\n",
       " '98+56',\n",
       " '05+73',\n",
       " ' 43+6',\n",
       " '49+23',\n",
       " ' 35+3',\n",
       " '76+05',\n",
       " '36+64',\n",
       " ' 24+1',\n",
       " ' 34+7',\n",
       " ' 55+7',\n",
       " ' 69+9',\n",
       " ' 44+3',\n",
       " ' 12+8',\n",
       " ' 96+5',\n",
       " ' 23+6',\n",
       " ' 63+1',\n",
       " ' 65+5',\n",
       " ' 85+4',\n",
       " ' 62+0',\n",
       " '68+58',\n",
       " '88+33',\n",
       " '78+08',\n",
       " '74+31',\n",
       " ' 38+8',\n",
       " ' 78+3',\n",
       " ' 43+4',\n",
       " ' 37+8',\n",
       " '55+41',\n",
       " ' 85+7',\n",
       " ' 37+1',\n",
       " '97+67',\n",
       " ' 73+4',\n",
       " ' 37+2',\n",
       " ' 75+3',\n",
       " ' 79+4',\n",
       " '94+42',\n",
       " '47+62',\n",
       " ' 52+7',\n",
       " ' 37+0',\n",
       " '06+01',\n",
       " ' 25+7',\n",
       " ' 14+4',\n",
       " '76+26',\n",
       " '94+81',\n",
       " '17+25',\n",
       " ' 13+4',\n",
       " ' 87+7',\n",
       " '86+76',\n",
       " ' 59+4',\n",
       " ' 33+6',\n",
       " ' 21+7',\n",
       " ' 92+1',\n",
       " ' 18+2',\n",
       " ' 27+7',\n",
       " ' 06+3',\n",
       " ' 43+9',\n",
       " ' 51+1',\n",
       " ' 73+1',\n",
       " '56+91',\n",
       " '78+34',\n",
       " ' 67+0',\n",
       " ' 69+2',\n",
       " ' 06+2',\n",
       " '29+14',\n",
       " '67+62',\n",
       " '38+32',\n",
       " ' 58+1',\n",
       " ' 38+6',\n",
       " '95+95',\n",
       " ' 84+1',\n",
       " ' 54+8',\n",
       " ' 84+8',\n",
       " '95+51',\n",
       " ' 23+5',\n",
       " '47+95',\n",
       " '12+31',\n",
       " ' 43+2',\n",
       " '27+01',\n",
       " '36+94',\n",
       " ' 73+8',\n",
       " ' 08+7',\n",
       " ' 99+4',\n",
       " ' 97+5',\n",
       " ' 79+6',\n",
       " '56+94',\n",
       " ' 61+7',\n",
       " ' 93+7',\n",
       " '08+74',\n",
       " ' 04+9',\n",
       " '59+14',\n",
       " '58+96',\n",
       " ' 33+3',\n",
       " '67+94',\n",
       " '68+36',\n",
       " '49+77',\n",
       " ' 47+5',\n",
       " ' 96+3',\n",
       " ' 93+8',\n",
       " ' 12+1',\n",
       " ' 53+9',\n",
       " ' 48+9',\n",
       " ' 03+6',\n",
       " ' 87+3',\n",
       " '13+51',\n",
       " '28+92',\n",
       " ' 26+8',\n",
       " '96+84',\n",
       " ' 36+7',\n",
       " ' 66+2',\n",
       " ' 97+9',\n",
       " '56+84',\n",
       " '89+34',\n",
       " ' 83+7',\n",
       " '87+55',\n",
       " '95+41',\n",
       " ' 93+1',\n",
       " ' 38+2',\n",
       " ' 67+3',\n",
       " '05+01',\n",
       " '16+42',\n",
       " '94+92',\n",
       " '37+63',\n",
       " ' 44+6',\n",
       " ' 93+0',\n",
       " '15+71',\n",
       " ' 02+7',\n",
       " '19+53',\n",
       " '84+43',\n",
       " ' 17+3',\n",
       " ' 18+5',\n",
       " '06+03',\n",
       " '28+73',\n",
       " '28+53',\n",
       " '67+24',\n",
       " ' 37+6',\n",
       " '56+65',\n",
       " '28+71',\n",
       " '75+22',\n",
       " ' 04+7',\n",
       " ' 78+0',\n",
       " '88+77',\n",
       " '74+33',\n",
       " ' 48+1',\n",
       " ' 83+5',\n",
       " ' 21+8',\n",
       " ' 64+1',\n",
       " '97+11',\n",
       " '36+55',\n",
       " '28+97',\n",
       " '99+86',\n",
       " '03+42',\n",
       " '19+95',\n",
       " ' 92+2',\n",
       " '96+96',\n",
       " '17+33',\n",
       " ' 06+6',\n",
       " ' 42+6',\n",
       " '39+48',\n",
       " '05+34',\n",
       " '75+82',\n",
       " ' 51+3',\n",
       " '18+24',\n",
       " ' 79+8',\n",
       " '27+55',\n",
       " '86+12',\n",
       " ' 34+9',\n",
       " ' 79+5',\n",
       " ' 23+9',\n",
       " ' 49+4',\n",
       " '74+54',\n",
       " ' 82+3',\n",
       " '23+81',\n",
       " ' 67+2',\n",
       " '27+12',\n",
       " ' 29+4',\n",
       " '59+48',\n",
       " ' 99+0',\n",
       " ' 63+9',\n",
       " ' 95+0',\n",
       " ' 74+0',\n",
       " ' 17+6',\n",
       " '17+07',\n",
       " '42+32',\n",
       " '69+24',\n",
       " ' 82+8',\n",
       " ' 02+0',\n",
       " '95+01',\n",
       " '44+52',\n",
       " '89+42',\n",
       " '33+11',\n",
       " ' 64+6',\n",
       " ' 74+8',\n",
       " '06+65',\n",
       " ' 04+6',\n",
       " ' 95+5',\n",
       " ' 06+7',\n",
       " '24+24',\n",
       " ' 58+9',\n",
       " '57+01',\n",
       " ' 59+7',\n",
       " '23+82',\n",
       " ' 71+1',\n",
       " '36+82',\n",
       " '33+22',\n",
       " ' 14+9',\n",
       " '87+03',\n",
       " ' 72+0',\n",
       " ' 99+3',\n",
       " '96+45',\n",
       " ' 52+0',\n",
       " '99+92',\n",
       " ' 97+4',\n",
       " '47+43',\n",
       " ' 17+8',\n",
       " '48+64',\n",
       " ' 85+2',\n",
       " ' 36+6',\n",
       " ' 95+2',\n",
       " '68+06',\n",
       " ' 33+1',\n",
       " '09+22',\n",
       " ' 84+4',\n",
       " '96+52',\n",
       " ' 27+2',\n",
       " '09+98',\n",
       " '48+31',\n",
       " '67+41',\n",
       " ' 65+3',\n",
       " ' 66+3',\n",
       " ' 44+4',\n",
       " '67+91',\n",
       " '09+84',\n",
       " '79+71',\n",
       " ' 63+7',\n",
       " '69+48',\n",
       " ' 49+1',\n",
       " ' 18+7',\n",
       " ' 56+8',\n",
       " ' 25+5',\n",
       " ...]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 54+2',\n",
       " '66+42',\n",
       " '46+82',\n",
       " '27+46',\n",
       " '44+82',\n",
       " ' 12+4',\n",
       " '92+42',\n",
       " '  4+2',\n",
       " '28+04',\n",
       " ' 52+4',\n",
       " '27+04',\n",
       " ' 21+4',\n",
       " ' 74+2',\n",
       " ' 49+2',\n",
       " '74+12',\n",
       " '64+22',\n",
       " '08+42',\n",
       " ' 02+4',\n",
       " '29+46',\n",
       " '74+62',\n",
       " '42+51',\n",
       " '94+82',\n",
       " '47+92',\n",
       " '14+02',\n",
       " ' 27+4',\n",
       " ' 84+2',\n",
       " ' 25+4',\n",
       " '34+02',\n",
       " ' 23+4',\n",
       " '25+34',\n",
       " '04+02',\n",
       " ' 42+9',\n",
       " '24+21',\n",
       " ' 22+4',\n",
       " ' 41+2',\n",
       " ' 24+4',\n",
       " ' 62+4',\n",
       " ' 64+2',\n",
       " ' 42+5',\n",
       " '94+22',\n",
       " '87+42',\n",
       " '54+32',\n",
       " '79+42',\n",
       " ' 42+1',\n",
       " '25+74',\n",
       " '26+04',\n",
       " '49+23',\n",
       " ' 24+1',\n",
       " '94+42',\n",
       " '47+62',\n",
       " '29+14',\n",
       " ' 43+2',\n",
       " '16+42',\n",
       " '94+92',\n",
       " '67+24',\n",
       " '03+42',\n",
       " ' 42+6',\n",
       " '18+24',\n",
       " ' 29+4',\n",
       " '42+32',\n",
       " '69+24',\n",
       " '44+52',\n",
       " '89+42',\n",
       " '24+24',\n",
       " '24+81',\n",
       " ' 42+0',\n",
       " '48+32',\n",
       " '36+24',\n",
       " '46+62',\n",
       " '49+52',\n",
       " ' 82+4',\n",
       " ' 24+3',\n",
       " '48+92',\n",
       " '84+52',\n",
       " '46+72',\n",
       " '64+42',\n",
       " '92+41',\n",
       " '48+12',\n",
       " '38+24',\n",
       " ' 24+8',\n",
       " '29+54',\n",
       " ' 24+9',\n",
       " '09+24',\n",
       " ' 14+2',\n",
       " ' 28+4',\n",
       " '65+42',\n",
       " '24+71',\n",
       " '14+12',\n",
       " '94+72',\n",
       " '64+92',\n",
       " '48+42',\n",
       " ' 34+2',\n",
       " ' 24+0',\n",
       " ' 48+2',\n",
       " '23+41',\n",
       " '26+94',\n",
       " '43+21',\n",
       " '24+53',\n",
       " ' 24+7',\n",
       " '47+27',\n",
       " '24+22',\n",
       " '94+52',\n",
       " '59+24',\n",
       " ' 47+2',\n",
       " '34+52',\n",
       " ' 32+4',\n",
       " '44+72',\n",
       " '84+82',\n",
       " '04+72',\n",
       " '47+82',\n",
       " '64+52',\n",
       " ' 24+6',\n",
       " ' 42+8',\n",
       " '34+42',\n",
       " ' 26+4',\n",
       " ' 42+7',\n",
       " '64+02',\n",
       " '27+74',\n",
       " '49+32',\n",
       " '24+13',\n",
       " ' 92+4',\n",
       " '24+61',\n",
       " ' 46+2',\n",
       " '24+72',\n",
       " '28+34',\n",
       " '26+24',\n",
       " ' 94+2',\n",
       " '44+23',\n",
       " ' 42+4',\n",
       " '39+42',\n",
       " '45+25',\n",
       " '29+44',\n",
       " '63+42',\n",
       " ' 45+2',\n",
       " '29+42',\n",
       " '33+42',\n",
       " ' 42+2',\n",
       " '24+01',\n",
       " '43+72',\n",
       " '06+42',\n",
       " '84+23',\n",
       " '49+28',\n",
       " '24+12',\n",
       " ' 44+2',\n",
       " '26+41',\n",
       " '24+42',\n",
       " '48+26',\n",
       " ' 42+3',\n",
       " '29+45',\n",
       " '49+24',\n",
       " '44+32',\n",
       " '49+12',\n",
       " '24+51',\n",
       " '34+32',\n",
       " '34+72',\n",
       " '14+52',\n",
       " '28+47',\n",
       " '64+12',\n",
       " ' 24+5',\n",
       " '46+22',\n",
       " '15+42',\n",
       " '88+24',\n",
       " '24+03',\n",
       " '45+23',\n",
       " '87+24',\n",
       " '74+22',\n",
       " '28+14',\n",
       " '64+62',\n",
       " '48+52',\n",
       " '94+24',\n",
       " '74+32',\n",
       " '42+91',\n",
       " '48+82',\n",
       " '16+24',\n",
       " '74+52',\n",
       " ' 72+4',\n",
       " '54+24',\n",
       " '75+24',\n",
       " '42+42',\n",
       " '43+42',\n",
       " '49+25',\n",
       " ' 04+2',\n",
       " '93+42',\n",
       " '24+32',\n",
       " '47+42',\n",
       " '55+24',\n",
       " '43+92',\n",
       " '27+34',\n",
       " '46+32',\n",
       " '46+25',\n",
       " '43+32',\n",
       " '96+42',\n",
       " '46+92',\n",
       " '24+33',\n",
       " '54+22',\n",
       " '74+23',\n",
       " '24+93',\n",
       " ' 24+2',\n",
       " '14+32',\n",
       " '34+92',\n",
       " '22+41',\n",
       " '04+92',\n",
       " '49+22',\n",
       " '02+41',\n",
       " '95+42',\n",
       " '28+94',\n",
       " '24+83',\n",
       " '19+42',\n",
       " '43+22',\n",
       " '27+41',\n",
       " '04+22',\n",
       " '25+44',\n",
       " '29+41',\n",
       " '36+42',\n",
       " '76+42',\n",
       " '44+02',\n",
       " '48+24',\n",
       " '14+72',\n",
       " '46+12',\n",
       " '26+42',\n",
       " '84+24',\n",
       " '07+24',\n",
       " '45+02',\n",
       " '24+62',\n",
       " '79+24',\n",
       " '25+54',\n",
       " '34+22',\n",
       " '54+21',\n",
       " '55+42',\n",
       " '62+41',\n",
       " '26+54',\n",
       " '42+21',\n",
       " '54+23',\n",
       " '49+92',\n",
       " '15+24',\n",
       " '57+42',\n",
       " '45+42',\n",
       " '94+62',\n",
       " '47+26',\n",
       " '54+82',\n",
       " '46+26',\n",
       " '42+11',\n",
       " '59+42',\n",
       " '64+24',\n",
       " '04+82',\n",
       " '25+94',\n",
       " '14+82',\n",
       " '54+62',\n",
       " '47+72',\n",
       " '45+12',\n",
       " '94+32',\n",
       " '94+02',\n",
       " '45+72',\n",
       " '54+52',\n",
       " '74+24',\n",
       " '34+12',\n",
       " '28+54',\n",
       " '84+02',\n",
       " '04+32',\n",
       " '96+24',\n",
       " '99+24',\n",
       " '77+24',\n",
       " '28+42',\n",
       " '06+24',\n",
       " '42+31',\n",
       " '14+62',\n",
       " '42+12',\n",
       " '29+74',\n",
       " '49+02',\n",
       " '32+41',\n",
       " '97+42',\n",
       " '26+45',\n",
       " '46+23',\n",
       " '85+24',\n",
       " '37+42',\n",
       " '69+42',\n",
       " '64+23',\n",
       " '84+32',\n",
       " '94+23',\n",
       " '27+94',\n",
       " '45+62',\n",
       " '47+25',\n",
       " '07+42',\n",
       " '28+44',\n",
       " '53+42',\n",
       " '49+27',\n",
       " '75+42',\n",
       " '88+42',\n",
       " '49+82',\n",
       " '68+42',\n",
       " '66+24',\n",
       " '14+23',\n",
       " '47+22',\n",
       " '24+14',\n",
       " '84+21',\n",
       " '08+24',\n",
       " '45+52',\n",
       " '23+42',\n",
       " '42+71',\n",
       " '49+42',\n",
       " '73+42',\n",
       " '37+24',\n",
       " '24+41',\n",
       " '24+11',\n",
       " '46+21',\n",
       " '28+45',\n",
       " '54+12',\n",
       " '44+24',\n",
       " '13+42',\n",
       " '25+04',\n",
       " '74+92',\n",
       " '62+42',\n",
       " '24+63',\n",
       " '05+42',\n",
       " '46+02',\n",
       " '29+47',\n",
       " '98+24',\n",
       " '42+22',\n",
       " '27+84',\n",
       " '14+92',\n",
       " '26+64',\n",
       " '14+22',\n",
       " '19+24',\n",
       " '47+21',\n",
       " '77+42',\n",
       " '34+23',\n",
       " '97+24',\n",
       " '24+92',\n",
       " '94+12',\n",
       " '49+21',\n",
       " '17+24',\n",
       " '44+92',\n",
       " '56+42',\n",
       " '26+44',\n",
       " '54+42',\n",
       " '44+42',\n",
       " '74+42',\n",
       " '24+23',\n",
       " '26+43',\n",
       " '34+82',\n",
       " '04+62',\n",
       " '25+64',\n",
       " '09+42',\n",
       " '44+62',\n",
       " '35+24',\n",
       " '58+24',\n",
       " '52+42',\n",
       " '39+24',\n",
       " '86+24',\n",
       " '64+21',\n",
       " '42+41',\n",
       " '64+72',\n",
       " '42+81',\n",
       " '48+27',\n",
       " '45+82',\n",
       " '99+42',\n",
       " '43+52',\n",
       " '38+42',\n",
       " '68+24',\n",
       " '89+24',\n",
       " '49+72',\n",
       " '48+62',\n",
       " '94+21',\n",
       " '54+72',\n",
       " '28+84',\n",
       " '52+41',\n",
       " '48+25',\n",
       " '24+31',\n",
       " '86+42',\n",
       " '44+22',\n",
       " '05+24',\n",
       " '25+43',\n",
       " '57+24',\n",
       " '64+32',\n",
       " '76+24',\n",
       " '41+21',\n",
       " '42+01',\n",
       " '24+04',\n",
       " '74+82',\n",
       " '49+26',\n",
       " '14+42',\n",
       " '04+21',\n",
       " '24+82',\n",
       " '28+64',\n",
       " '45+92',\n",
       " '54+92',\n",
       " '24+02',\n",
       " '49+62',\n",
       " '28+74',\n",
       " '26+14',\n",
       " '35+42',\n",
       " '29+48',\n",
       " '26+34',\n",
       " '34+62',\n",
       " '25+24',\n",
       " '25+14',\n",
       " '24+73',\n",
       " '78+42',\n",
       " '56+24',\n",
       " '72+42',\n",
       " '27+42',\n",
       " '46+42',\n",
       " '43+23',\n",
       " '48+21',\n",
       " '48+02',\n",
       " '43+82',\n",
       " '42+02',\n",
       " '18+42',\n",
       " '82+42',\n",
       " '47+12',\n",
       " '47+32',\n",
       " '04+52',\n",
       " '48+22',\n",
       " '34+21',\n",
       " '27+44',\n",
       " '42+61',\n",
       " '43+02',\n",
       " '46+52',\n",
       " '28+43',\n",
       " '27+54',\n",
       " '45+22',\n",
       " '12+41',\n",
       " '26+74',\n",
       " '84+42',\n",
       " '58+42',\n",
       " '84+92',\n",
       " '29+84',\n",
       " '27+43',\n",
       " '74+72',\n",
       " '26+84',\n",
       " '64+82',\n",
       " '04+12',\n",
       " '25+42',\n",
       " '44+12',\n",
       " '85+42',\n",
       " '82+41',\n",
       " '27+24',\n",
       " '46+24',\n",
       " '24+43',\n",
       " '45+32',\n",
       " '54+02',\n",
       " '27+45',\n",
       " '48+23',\n",
       " '49+29',\n",
       " '34+24',\n",
       " '28+46',\n",
       " '65+24',\n",
       " '04+42',\n",
       " '98+42',\n",
       " '28+41',\n",
       " '25+84',\n",
       " '29+24',\n",
       " '29+34',\n",
       " '48+28',\n",
       " '84+12',\n",
       " '29+04',\n",
       " '24+91',\n",
       " '29+64',\n",
       " '47+52',\n",
       " '67+42',\n",
       " '84+22',\n",
       " '43+12',\n",
       " '48+72',\n",
       " '25+41',\n",
       " '45+21',\n",
       " '27+64',\n",
       " '44+21',\n",
       " '29+94',\n",
       " '47+02',\n",
       " '27+14',\n",
       " '84+72',\n",
       " '43+62',\n",
       " '28+24',\n",
       " '78+24',\n",
       " '47+23',\n",
       " '14+21',\n",
       " '24+52',\n",
       " '17+42',\n",
       " '29+43',\n",
       " '74+21',\n",
       " '04+23',\n",
       " '74+02',\n",
       " '45+24']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in questions if '2' in s and '4' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
